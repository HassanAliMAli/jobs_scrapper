# **ðŸ“‹ PRODUCT REQUIREMENTS DOCUMENT (PRD)**

## **Pakistan Job Aggregator Platform**
### **Version 1.0 | October 14, 2025**

---

## **Document Control**

| Attribute | Details |
|-----------|---------|
| **Project Name** | PakJobs Aggregator (Working Title) |
| **Document Version** | 1.0.0 |
| **Author** | Technical Architecture Team |
| **Date Created** | October 14, 2025 |
| **Last Updated** | October 14, 2025 |
| **Status** | Draft - Ready for Development |
| **Budget** | PKR 0 (Zero-Cost Architecture) |
| **Target Launch** | December 2025 (10-12 weeks) |
| **Classification** | Internal - Development Specifications |

***

# **TABLE OF CONTENTS**

1. [Executive Summary](#1-executive-summary)
2. [Project Vision & Goals](#2-project-vision--goals)
3. [Target Audience & User Personas](#3-target-audience--user-personas)
4. [Market Analysis & Competitive Landscape](#4-market-analysis--competitive-landscape)
5. [Product Scope & Features](#5-product-scope--features)
6. [Technical Architecture](#6-technical-architecture)
7. [Technology Stack Specifications](#7-technology-stack-specifications)
8. [System Components Design](#8-system-components-design)
9. [Database Architecture](#9-database-architecture)
10. [Scraper Specifications (9 Sites)](#10-scraper-specifications-9-sites)
11. [Incremental Scraping Logic](#11-incremental-scraping-logic)
12. [Data Pipeline & Integration](#12-data-pipeline--integration)
13. [Web Dashboard Specifications](#13-web-dashboard-specifications)
14. [API Specifications](#14-api-specifications)
15. [Security & Privacy](#15-security--privacy)
16. [Deployment Strategy](#16-deployment-strategy)
17. [Monitoring & Observability](#17-monitoring--observability)
18. [Error Handling & Recovery](#18-error-handling--recovery)
19. [Maintenance & Updates](#19-maintenance--updates)
20. [Legal & Ethical Compliance](#20-legal--ethical-compliance)
21. [Risk Analysis & Mitigation](#21-risk-analysis--mitigation)
22. [Performance Requirements](#22-performance-requirements)
23. [Testing Strategy](#23-testing-strategy)
24. [Timeline & Milestones](#24-timeline--milestones)
25. [Success Metrics & KPIs](#25-success-metrics--kpis)
26. [Future Enhancements (Phase 2)](#26-future-enhancements-phase-2)
27. [Appendices](#27-appendices)

***

# **1. EXECUTIVE SUMMARY**

## **1.1 Project Overview**

**PakJobs Aggregator** is a zero-cost, commercial job aggregation platform designed to scrape and consolidate job listings from Pakistan's top 9 job portals into a single, searchable database with advanced analytics capabilities. The platform provides job seekers with unique insights unavailable on traditional job boards while maintaining ethical scraping practices and full legal compliance.[1][2][3][4]

## **1.2 Business Problem**

Pakistani job seekers currently face:
- **Fragmentation**: Must visit 10+ job sites daily to find opportunities[2][1]
- **Duplication**: Same jobs posted across multiple platforms
- **Limited Insights**: No salary trends, demand analysis, or market intelligence
- **Time Waste**: Average 2-3 hours/day browsing multiple sites
- **Missed Opportunities**: Jobs expire before discovery

## **1.3 Proposed Solution**

A centralized platform that:
1. **Aggregates** 5,000-10,000+ daily job listings from 9 major Pakistani job portals[1]
2. **Deduplicates** identical postings using ML-based matching
3. **Enriches** data with salary trends, skill demand analytics, and location heatmaps
4. **Delivers** via web dashboard with advanced filtering and export capabilities
5. **Integrates** with external databases and applications via REST API[5][6]
6. **Operates** on 100% free infrastructure (Render.com, GitHub, Gmail SMTP)[7][8][9]

## **1.4 Key Differentiators**

| Feature | Traditional Job Sites | PakJobs Aggregator |
|---------|----------------------|-------------------|
| **Coverage** | Single platform | 9 platforms aggregated[1] |
| **Insights** | None | Salary trends, demand analysis, skill mapping |
| **Updates** | Manual refresh | Automated daily scraping at 3 AM PKT[10][9] |
| **Data Export** | Limited | CSV, JSON, API, database push[11] |
| **Cost** | Free browsing | 100% free (no ads initially) |
| **Speed** | Page loads 3-5s | Search results <500ms (indexed DB)[8] |

## **1.5 Technical Approach**

**Zero-Budget Architecture**:[9][7]
- **Hosting**: Render.com free tier (512MB RAM, auto-deploy)[12][13][9]
- **Database**: Render PostgreSQL 16 (1GB storage, 90-day retention)[8]
- **Scraping**: Scrapy + Playwright (industry-standard stack)[14][15][16]
- **Scheduling**: APScheduler + Render Cron (daily 3 AM automation)[5][9]
- **Notifications**: Gmail SMTP (500 emails/day free)[17][18]
- **Version Control**: Git + GitHub with Actions CI/CD[7]

## **1.6 Success Criteria**

**Phase 1 (Months 1-3)**:
- âœ… 9 functional scrapers with 95%+ uptime
- âœ… 50,000+ total job listings in database
- âœ… Dashboard with search, filter, export features
- âœ… <2% scraping error rate
- âœ… Zero infrastructure costs

**Phase 2 (Months 4-6)**:
- âœ… 1,000+ monthly active users
- âœ… 10+ external database integrations
- âœ… Advanced analytics dashboard
- âœ… Mobile-responsive design
- âœ… 99%+ uptime SLA

---

# **2. PROJECT VISION & GOALS**

## **2.1 Vision Statement**

*"To become Pakistan's most comprehensive and intelligent job discovery platform, empowering every job seeker with market insights and opportunities that traditional portals cannot provide."*

## **2.2 Mission Statement**

Democratize access to job market data by aggregating, analyzing, and presenting employment opportunities from across Pakistan's digital landscape through ethical, transparent, and technologically advanced methods.[3]

## **2.3 Primary Goals**

### **Goal 1: Comprehensive Data Coverage**
- Scrape 9 major Pakistani job portals daily[1]
- Capture 100% of available job attributes (title, salary, skills, etc.)
- Maintain data freshness within 24 hours of posting[10]
- Achieve 95%+ scraping success rate across all sites

### **Goal 2: Intelligent Insights Generation**
- Provide salary benchmarking by role, location, and experience
- Identify trending skills and in-demand technologies
- Map job concentration by geographic region
- Analyze hiring patterns and company growth indicators

### **Goal 3: Zero-Cost Operation**
- Maintain 100% free infrastructure (no monthly bills)[9][7]
- Utilize only open-source technologies[19][20][14]
- Leverage free tiers of cloud services[8][12]
- Implement resource-efficient scraping (bandwidth <10GB/month)[21][10]

### **Goal 4: Ethical & Legal Compliance**
- Respect robots.txt and site terms of service[3]
- Implement polite scraping (3s delays, off-peak hours)[22][23]
- Avoid personal data collection without consent[3]
- Provide attribution to original job sources

### **Goal 5: Developer-Friendly Integration**
- REST API for external application access[6]
- Support multiple database export formats[11][24]
- Comprehensive documentation for maintainability
- Modular architecture for easy updates[5]

## **2.4 Non-Goals (Out of Scope for Phase 1)**

- âŒ User authentication/registration system
- âŒ Job application functionality (redirect to source)
- âŒ Resume upload or CV parsing
- âŒ Email job alerts to end users (only admin notifications)
- âŒ Mobile native apps (web-responsive only)
- âŒ Real-time chat or support system
- âŒ Paid premium features or monetization
- âŒ LinkedIn scraping (legal/technical constraints)[4]

## **2.5 Success Metrics**

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Scrapers Operational** | 9/9 sites | Daily health checks |
| **Data Freshness** | <24 hours | Timestamp analysis |
| **Scraping Success Rate** | 95%+ | Logs analysis |
| **Database Growth** | 5,000+ jobs/day | PostgreSQL count queries[8] |
| **API Uptime** | 99%+ | Render monitoring[9] |
| **Zero Cost Maintained** | PKR 0/month | Billing dashboard |
| **Error Rate** | <2% | Exception tracking |

***

# **3. TARGET AUDIENCE & USER PERSONAS**

## **3.1 Primary Users**

### **Persona 1: The Active Job Seeker**
**Demographics**:
- Age: 22-35 years
- Education: Bachelor's or Master's degree
- Location: Major cities (Karachi, Lahore, Islamabad)
- Tech Savvy: Moderate to High

**Goals**:
- Find relevant job openings quickly
- Compare salaries across companies
- Identify skills to learn for career growth
- Avoid duplicate applications

**Pain Points**:
- Visits 5-10 job sites daily (2+ hours wasted)[2][1]
- Misses opportunities posted on less popular sites
- No salary transparency on most platforms
- Overwhelmed by irrelevant listings

**How PakJobs Helps**:
- Single search across 9 platforms
- Salary range filters and benchmarks
- Skill demand insights for upskilling
- Deduplicated, relevant results

***

### **Persona 2: The Career Switcher**
**Demographics**:
- Age: 28-40 years
- Education: Experienced professional (5-15 years)
- Location: Nationwide
- Industry: Transitioning careers

**Goals**:
- Research salary expectations in new field
- Identify transferable skills in demand
- Find entry-level opportunities in target industry
- Understand market trends

**Pain Points**:
- Limited salary data for career planning
- Unsure which skills to highlight
- Difficulty finding junior roles for experienced professionals
- No market intelligence on hiring trends

**How PakJobs Helps**:
- Salary trend analytics by experience level
- Skill overlap analysis between industries
- Advanced filters (experience range, industry)
- Market demand reports

***

### **Persona 3: The Developer/Researcher**
**Demographics**:
- Age: 25-45 years
- Role: Software developer, data analyst, researcher
- Tech Level: Expert
- Use Case: Building applications/research projects

**Goals**:
- Access structured job market data via API[6]
- Export data for analysis (CSV, JSON)[11]
- Integrate job listings into own platform
- Conduct labor market research

**Pain Points**:
- No free APIs for Pakistani job data
- Manual scraping is time-consuming
- Inconsistent data formats across sites
- Rate limiting on source sites

**How PakJobs Helps**:
- Free REST API access[6]
- Standardized JSON/CSV exports[11]
- Database connector for direct integration[24]
- Pre-processed, clean data

***

## **3.2 Secondary Users**

### **Persona 4: HR/Recruiter**
- **Goal**: Competitive intelligence (what salaries competitors offer)
- **Use**: Market benchmarking for job postings
- **Benefit**: Salary trends, common job descriptions

### **Persona 5: Educational Institutions**
- **Goal**: Curriculum planning based on market demand
- **Use**: Analyze skill requirements in job postings
- **Benefit**: Skills analytics dashboard

***

## **3.3 User Needs Summary**

| User Type | Primary Need | Secondary Need | Critical Feature |
|-----------|--------------|----------------|------------------|
| **Job Seeker** | Find relevant jobs fast | Salary transparency | Multi-site search |
| **Career Switcher** | Market intelligence | Skill gap analysis | Analytics dashboard |
| **Developer** | Structured data access | Easy integration | REST API[6] |
| **Recruiter** | Competitive insights | Hiring trends | Export functionality[11] |

***

# **4. MARKET ANALYSIS & COMPETITIVE LANDSCAPE**

## **4.1 Pakistani Job Portal Ecosystem**

### **Market Leaders** (Target Scraping Sites)[2][1]

**Tier 1: High Traffic (100,000+ monthly visitors)**
1. **Rozee.pk** - Market leader, 50%+ market share[25]
2. **Indeed.pk** - Global platform, strong local presence[26]
3. **Mustakbil.com** - Second largest local player[27]

**Tier 2: Moderate Traffic (10,000-100,000 monthly visitors)**
4. **Bayt.com** - Middle East focus, Pakistan presence[1]
5. **BrightSpyre.com** - Tech-focused listings
6. **Jobz.pk** - General marketplace

**Tier 3: Niche/Regional (1,000-10,000 monthly visitors)**
7. **Bayrozgar.com** - Urdu content, local jobs
8. **JobsAlert.pk** - Government sector focus
9. **PakPositions.com** - SME job postings

**Excluded Sites**:
- âŒ **LinkedIn** - Anti-scraping measures, legal risks, API costs[4]

***

## **4.2 Competitive Analysis**

### **Direct Competitors** (None Truly Comparable)

| Platform | Coverage | Insights | API | Cost | Limitations |
|----------|----------|----------|-----|------|-------------|
| **Rozee.pk** | 1 site | None | No | Free | Single source[25] |
| **Indeed.pk** | Aggregates some sites | Basic | Paid | Free | International focus[26] |
| **Mustakbil** | 1 site | None | No | Free | Single source[27] |
| **PakJobs Aggregator** | **9 sites** | **Advanced** | **Free** | **Free** | **New entrant** |

### **Key Differentiators**

**Our Competitive Advantages**:
1. âœ… **Only multi-source aggregator** in Pakistani market[1]
2. âœ… **Free analytics** (salary trends, skill demand)
3. âœ… **Developer API** at no cost[6]
4. âœ… **Export flexibility** (CSV, JSON, database push)[11]
5. âœ… **Transparent operations** (open methodology)

**Our Disadvantages**:
1. âŒ No direct employer relationships (rely on scraping)
2. âŒ 24-hour data delay vs. real-time postings
3. âŒ Cannot facilitate applications (redirect to source)
4. âŒ Dependent on source sites' availability

***

## **4.3 Market Opportunity**

### **Market Size Estimation**

**Pakistani Online Job Market**:
- **Total internet users**: ~125 million (2025)[1]
- **Active job seekers**: ~15 million (estimated)
- **Monthly job postings**: 50,000-75,000 across all platforms[2][1]
- **Average listings per site**: 5,000-10,000 active jobs

**Addressable Market**:
- **Primary**: 5 million urban, tech-savvy job seekers
- **Secondary**: 50,000+ developers, researchers, recruiters
- **Growth Rate**: 15-20% annually (internet penetration)[1]

### **Market Gaps We Fill**

1. **Fragmentation Problem**: No single search across all platforms[2]
2. **Data Opacity**: Salary ranges rarely disclosed
3. **No Analytics**: Market trends unavailable to individuals
4. **Developer Tools**: No free APIs for Pakistani job data[6]
5. **Time Inefficiency**: Job seekers waste 10+ hours/week browsing[1]

***

## **4.4 Regulatory & Legal Landscape**

**Web Scraping in Pakistan**:[4][3]
- âœ… No specific anti-scraping laws
- âœ… Public data scraping generally permitted
- âš ï¸ Must respect individual site Terms of Service
- âš ï¸ Personal data subject to data protection norms

**Our Compliance Strategy**:
1. Scrape only public job listings (no personal data)[3]
2. Respect robots.txt on all sites[22][3]
3. Implement rate limiting (3s delays)[23][22]
4. Provide attribution links to original sources
5. Off-peak scraping (3 AM PKT) to minimize load[3]

***

# **5. PRODUCT SCOPE & FEATURES**

## **5.1 Core Features (Phase 1 - Must Have)**

### **Feature 1: Multi-Site Job Scraping**
**Description**: Automated scraping of 9 Pakistani job portals daily[1]

**Functional Requirements**:
- FR-1.1: Scrape Rozee.pk, Mustakbil.com, Indeed.pk, BrightSpyre, Bayt, Jobz.pk, Bayrozgar, JobsAlert, PakPositions
- FR-1.2: Extract ALL available job attributes (title, company, location, salary, description, skills, deadline, etc.)
- FR-1.3: Capture job posting date and application method (email/link)
- FR-1.4: Store source URL for attribution[3]
- FR-1.5: Execute scraping at 3 AM PKT daily (off-peak)[9]

**Technical Requirements**:
- TR-1.1: Use Scrapy framework for static sites[16][14]
- TR-1.2: Use Playwright for JavaScript-heavy sites[15]
- TR-1.3: Implement 3-second delay between requests[23][22]
- TR-1.4: Rotate user-agents to mimic human behavior[22]
- TR-1.5: Handle pagination, dynamic loading, infinite scroll

**Acceptance Criteria**:
- âœ… Each scraper completes within 10-15 minutes
- âœ… 95%+ job capture rate (validated by manual spot checks)
- âœ… <2% error rate across all sites
- âœ… All scrapers run independently (modular)[5]

***

### **Feature 2: Incremental Scraping**
**Description**: Only scrape new/updated jobs since last run (efficiency)[10][21]

**Functional Requirements**:
- FR-2.1: Track last scrape timestamp per site
- FR-2.2: Identify job uniqueness via URL or title+company hash
- FR-2.3: Skip already-scraped jobs
- FR-2.4: Update existing jobs if content changed
- FR-2.5: Perform full refresh weekly (Sundays)[10]

**Technical Requirements**:
- TR-2.1: Store `last_scraped_at` timestamp in database[8]
- TR-2.2: Generate unique job identifiers (UUID v5 from URL)[21]
- TR-2.3: Use date filters in scraping URLs (e.g., `&fromage=1`)[28]
- TR-2.4: Implement "stop when seen" logic for pagination[10]

**Acceptance Criteria**:
- âœ… Incremental scrapes 10x faster than full scrapes (1-2 min vs 10-15 min)
- âœ… Bandwidth usage <500MB/day (vs 5GB for full scrapes)[21]
- âœ… No duplicate jobs in database (unique constraint enforced)[8]

***

### **Feature 3: PostgreSQL Database Storage**
**Description**: Structured storage with full-text search and analytics capability[8]

**Functional Requirements**:
- FR-3.1: Store all scraped job data in normalized schema
- FR-3.2: Enable full-text search on title, description, skills
- FR-3.3: Support complex queries (salary range, location, date filters)
- FR-3.4: Maintain scraping logs for monitoring
- FR-3.5: Archive jobs after 90 days (auto-cleanup)[8]

**Technical Requirements**:
- TR-3.1: Use PostgreSQL 16 on Render (1GB free tier)[9][8]
- TR-3.2: Implement GIN indexes for full-text search[8]
- TR-3.3: Use JSONB columns for flexible skill arrays
- TR-3.4: Enable connection pooling (max 5 connections)[8]
- TR-3.5: Daily automated backups via pg_dump[8]

**Acceptance Criteria**:
- âœ… Database stores 50,000+ jobs without performance degradation
- âœ… Search queries return results in <500ms[8]
- âœ… 99.9% data integrity (no corrupted records)

***

### **Feature 4: Web Dashboard**
**Description**: Flask-based admin interface for monitoring and control[29][9]

**Functional Requirements**:
- FR-4.1: Display scraper status (last run, jobs found, errors)
- FR-4.2: Manual trigger for individual scrapers (testing)
- FR-4.3: Search interface with filters (location, salary, date)
- FR-4.4: Export jobs to CSV/JSON[11]
- FR-4.5: Configure external database connections[24]
- FR-4.6: View analytics (salary trends, top skills)

**Technical Requirements**:
- TR-4.1: Flask 3.0 with Jinja2 templates[29][9]
- TR-4.2: Tailwind CSS for responsive design
- TR-4.3: No authentication (internal use only, Phase 1)
- TR-4.4: AJAX for live scraper status updates
- TR-4.5: Mobile-responsive (works on phones/tablets)

**Acceptance Criteria**:
- âœ… Dashboard loads in <2 seconds[9]
- âœ… All scrapers controllable via UI
- âœ… CSV exports complete in <5 seconds for 10,000 jobs[11]

***

### **Feature 5: External Database Integration**
**Description**: Push scraped data to user-configured databases[24][5]

**Functional Requirements**:
- FR-5.1: Support PostgreSQL, MySQL, MongoDB, SQLite connections[24]
- FR-5.2: User provides connection string via dashboard
- FR-5.3: Test connection before saving credentials
- FR-5.4: Encrypt stored credentials (Fernet encryption)
- FR-5.5: Batch push jobs after each scrape run
- FR-5.6: Handle connection failures gracefully (retry 3x)

**Technical Requirements**:
- TR-5.1: Use SQLAlchemy for relational DBs[24]
- TR-5.2: Use PyMongo for MongoDB[30]
- TR-5.3: Implement connection pooling (prevent leaks)
- TR-5.4: Validate SSL certificates for remote connections
- TR-5.5: Support SSH tunneling for secure connections

**Acceptance Criteria**:
- âœ… Successfully push 10,000 jobs to external DB in <60 seconds[24]
- âœ… Zero data loss during transmission
- âœ… Credentials never logged or exposed

***

### **Feature 6: Email Notifications**
**Description**: Gmail SMTP alerts for scraper events[18][17]

**Functional Requirements**:
- FR-6.1: Send email on scraping completion (summary: jobs found, errors)
- FR-6.2: Alert if scraper fails after 3 retries
- FR-6.3: Alert if <10 jobs found (anomaly detection)
- FR-6.4: Daily digest email with 24-hour summary
- FR-6.5: Configurable recipient list (comma-separated emails)

**Technical Requirements**:
- TR-6.1: Use Gmail SMTP (smtp.gmail.com:587)[17][18]
- TR-6.2: Authenticate via App Password (not account password)[17]
- TR-6.3: HTML email templates with job stats
- TR-6.4: Rate limiting (max 50 emails/day to avoid spam flags)[18]
- TR-6.5: Fallback to text-only if HTML fails

**Acceptance Criteria**:
- âœ… Emails delivered within 60 seconds of event
- âœ… 99%+ delivery rate (no spam folder)
- âœ… Mobile-friendly email formatting

***

### **Feature 7: REST API**
**Description**: JSON endpoints for external application access[6]

**Functional Requirements**:
- FR-7.1: GET /api/jobs - List all jobs (paginated)
- FR-7.2: GET /api/jobs/{id} - Single job details
- FR-7.3: GET /api/jobs/search?q={query} - Full-text search
- FR-7.4: GET /api/stats - Summary statistics
- FR-7.5: GET /api/sites - Scraper status per site
- FR-7.6: Rate limiting (1000 requests/hour per IP)

**Technical Requirements**:
- TR-7.1: Flask-RESTful for endpoint management[6]
- TR-7.2: JSON serialization with job schema validation
- TR-7.3: API versioning (/api/v1/jobs)
- TR-7.4: CORS enabled for browser access
- TR-7.5: OpenAPI/Swagger documentation

**Acceptance Criteria**:
- âœ… API responds in <500ms for paginated queries (100 results)[6]
- âœ… 99.9% uptime (matches Render SLA)[9]
- âœ… Comprehensive error messages (JSON formatted)

***

### **Feature 8: Data Export**
**Description**: Download jobs in multiple formats[11]

**Functional Requirements**:
- FR-8.1: Export filtered results to CSV
- FR-8.2: Export to JSON (structured data)
- FR-8.3: Export to Excel (XLSX) for business users
- FR-8.4: Include metadata (export date, filter criteria)
- FR-8.5: Maximum 10,000 records per export (memory limits)

**Technical Requirements**:
- TR-8.1: Use Pandas for CSV/Excel generation[11]
- TR-8.2: Stream large exports (avoid memory overflow)[11]
- TR-8.3: Generate exports asynchronously (background task)
- TR-8.4: Store exports temporarily (auto-delete after 24 hours)
- TR-8.5: Sanitize data (remove PII if accidentally scraped)

**Acceptance Criteria**:
- âœ… 10,000-job CSV export completes in <10 seconds[11]
- âœ… Excel files open correctly in MS Office/Google Sheets
- âœ… JSON validates against schema

***

## **5.2 Analytics Features (Phase 1 - Should Have)**

### **Feature 9: Salary Trends Analysis**
- Calculate average, median, min, max salary by:
  - Job role (e.g., "Software Engineer" = PKR 80,000-150,000)
  - City (Karachi vs Lahore vs Islamabad)
  - Experience level (0-2, 3-5, 6-10, 10+ years)
  - Company size (startup, SME, enterprise)
- Visualize trends over time (line chart)
- Compare user's salary expectations to market rates

### **Feature 10: Skill Demand Mapping**
- Extract skills from job descriptions (NLP parsing)
- Rank top 20 most in-demand skills
- Track skill trends (emerging vs declining)
- Co-occurrence analysis (skills often paired together)
- Visualize as word cloud or bar chart

### **Feature 11: Location Heatmap**
- Map job concentration by city
- Calculate jobs per capita (normalize by population)
- Identify remote-friendly companies
- Show hiring hotspots on Pakistan map (ASCII representation)

### **Feature 12: Company Insights**
- Rank top hiring companies (by job count)
- Track hiring velocity (jobs posted per week)
- Identify fastest-growing employers
- Average time-to-hire (days job stays open)

***

## **5.3 Nice-to-Have Features (Phase 2)**

- User authentication (email/password login)
- Saved searches and job alerts
- Job recommendation engine (ML-based)
- Resume upload and skill matching
- Mobile apps (React Native)
- Real-time scraping (hourly updates)
- Browser extension (Chrome/Firefox)
- Multi-language support (Urdu translations)

***

# **6. TECHNICAL ARCHITECTURE**

## **6.1 Architecture Overview**

**High-Level System Design** (Zero-Budget Cloud Architecture)[7][9]

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Web Browser  â”‚  â”‚ API Clients  â”‚  â”‚ External Appsâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APPLICATION LAYER                            â”‚
â”‚                (Render.com Free Web Service)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                 Flask Web Application                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚  â”‚  Dashboard  â”‚  â”‚  REST API   â”‚  â”‚  Scheduler  â”‚      â”‚  â”‚
â”‚  â”‚  â”‚  (Jinja2)   â”‚  â”‚(Flask-REST) â”‚  â”‚(APScheduler)â”‚      â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCRAPING LAYER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Scraping Orchestrator                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚Scrapy  â”‚  â”‚Scrapy  â”‚  â”‚Scrapy  â”‚  â”‚Playwright        â”‚  â”‚
â”‚  â”‚  â”‚Spider 1â”‚  â”‚Spider 2â”‚  â”‚Spider 3â”‚  â”‚ (JS Sites)â”‚      â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚           â”‚           â”‚            â”‚
          â–¼           â–¼           â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TARGET SITES (9 Job Portals)                 â”‚
â”‚  Rozee.pk â”‚ Mustakbil â”‚ Indeed.pk â”‚ BrightSpyre â”‚ Bayt         â”‚
â”‚  Jobz.pk  â”‚ Bayrozgar â”‚ JobsAlert â”‚ PakPositions               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚           â”‚           â”‚            â”‚
          â–¼           â–¼           â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LAYER                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚       PostgreSQL 16 Database (Render - 1GB Free)         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚ jobs       â”‚  â”‚ scrape_logsâ”‚  â”‚ user_configâ”‚         â”‚  â”‚
â”‚  â”‚  â”‚ (50K rows) â”‚  â”‚ (monitoring)â”‚  â”‚ (settings) â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INTEGRATION LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ External   â”‚  â”‚  Gmail     â”‚  â”‚  CSV/JSON  â”‚               â”‚
â”‚  â”‚ Databases  â”‚  â”‚  SMTP      â”‚  â”‚  Export    â”‚               â”‚
â”‚  â”‚ (User-DB)  â”‚  â”‚(Notify)    â”‚  â”‚  (Local)   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DEPLOYMENT & CI/CD                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  GitHub    â”‚  â”‚  GitHub    â”‚  â”‚   Render   â”‚               â”‚
â”‚  â”‚  Repositoryâ”‚â”€â–¶â”‚  Actions   â”‚â”€â–¶â”‚  Auto-     â”‚               â”‚
â”‚  â”‚  (Code)    â”‚  â”‚  (CI/CD)   â”‚  â”‚  Deploy    â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **6.2 Architecture Principles**

### **1. Modularity**
- Each scraper is independent Python module[5]
- Failure of one scraper doesn't affect others
- Easy to add/remove/update individual scrapers

### **2. Scalability (Within Free Limits)**
- Sequential scraping prevents resource exhaustion[7][9]
- Incremental scraping reduces bandwidth 10x[21][10]
- Database indexes optimize query performance[8]

### **3. Reliability**
- 3-retry mechanism with exponential backoff
- Email alerts for failures[18][17]
- Daily automated health checks
- Graceful degradation (skip failed sites, continue others)

### **4. Maintainability**
- Comprehensive inline documentation
- Type hints (Python 3.12+)
- Unit tests for critical functions (pytest)
- Git version control with meaningful commits[7]

### **5. Security**
- Environment variables for secrets (no hardcoding)
- Encrypted credentials for external DBs (Fernet)
- Input validation on all user inputs
- SQL injection prevention (parameterized queries)[8]

### **6. Cost Optimization**
- Use free tiers exclusively[7][9][8]
- Efficient resource usage (512MB RAM limit)[9]
- Minimal bandwidth consumption[10][21]
- Cron scheduling vs. continuous polling[9]

***

## **6.3 Component Interaction Flow**

### **Daily Scraping Workflow** (Automated at 3 AM PKT)[9]

```
1. Render Cron Trigger (3:00 AM PKT)
      â”‚
      â–¼
2. APScheduler Wakes Up
      â”‚
      â–¼
3. Scraping Orchestrator Starts
      â”‚
      â”œâ”€â”€â–¶ Load last_scraped_at timestamps from DB
      â”‚
      â”œâ”€â”€â–¶ For Each Site (Sequential):
      â”‚    â”‚
      â”‚    â”œâ”€â”€â–¶ Initialize Scraper (Scrapy or Playwright)
      â”‚    â”‚
      â”‚    â”œâ”€â”€â–¶ Scrape New Jobs (Incremental Mode)
      â”‚    â”‚    â”‚
      â”‚    â”‚    â”œâ”€â”€â–¶ Check if job URL exists in DB
      â”‚    â”‚    â”‚    â”‚
      â”‚    â”‚    â”‚    â”œâ”€â–¶ Yes: Skip (already scraped)
      â”‚    â”‚    â”‚    â”‚
      â”‚    â”‚    â”‚    â””â”€â–¶ No: Parse job details
      â”‚    â”‚    â”‚         â”‚
      â”‚    â”‚    â”‚         â”œâ”€â–¶ Extract: title, company, salary, etc.
      â”‚    â”‚    â”‚         â”‚
      â”‚    â”‚    â”‚         â”œâ”€â–¶ Validate data (non-null checks)
      â”‚    â”‚    â”‚         â”‚
      â”‚    â”‚    â”‚         â””â”€â–¶ Insert into PostgreSQL
      â”‚    â”‚    â”‚
      â”‚    â”‚    â””â”€â”€â–¶ Stop When: Hit previously-seen job OR max pages
      â”‚    â”‚
      â”‚    â”œâ”€â”€â–¶ Log Results (jobs_found, errors, duration)
      â”‚    â”‚
      â”‚    â””â”€â”€â–¶ 3-Second Delay (politeness)
      â”‚
      â–¼
4. All Scrapers Complete
      â”‚
      â”œâ”€â”€â–¶ Calculate Summary Stats
      â”‚    â”‚
      â”‚    â”œâ”€â–¶ Total jobs scraped across all sites
      â”‚    â”œâ”€â–¶ Errors encountered
      â”‚    â”œâ”€â–¶ Total execution time
      â”‚    â””â”€â–¶ New jobs added to DB
      â”‚
      â”œâ”€â”€â–¶ Push to External Database (if configured)
      â”‚    â”‚
      â”‚    â”œâ”€â–¶ Read user_config table
      â”‚    â”œâ”€â–¶ Connect to external DB
      â”‚    â”œâ”€â–¶ Batch insert new jobs
      â”‚    â””â”€â–¶ Log push status
      â”‚
      â”œâ”€â”€â–¶ Generate Analytics
      â”‚    â”‚
      â”‚    â”œâ”€â–¶ Update salary trends cache
      â”‚    â”œâ”€â–¶ Recalculate top skills
      â”‚    â””â”€â–¶ Refresh location stats
      â”‚
      â””â”€â”€â–¶ Send Email Notification
           â”‚
           â”œâ”€â–¶ Compose HTML email with stats
           â”œâ”€â–¶ Attach error log (if failures)
           â””â”€â–¶ Send via Gmail SMTP
```

***

## **6.4 Data Flow Diagram**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job Sites   â”‚ (Public Web Pages)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP Requests (3s delay)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scrapers   â”‚ (Scrapy/Playwright)
â”‚  - Parse    â”‚
â”‚  - Extract  â”‚
â”‚  - Validate â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Structured Data (Python dicts)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data       â”‚ (Deduplication, Enrichment)
â”‚  Processing â”‚
â”‚  - Hash URL â”‚
â”‚  - Parse    â”‚
â”‚  - Clean    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Validated Job Objects
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚ (Primary Storage)
â”‚  Database   â”‚
â”‚  - Insert   â”‚
â”‚  - Index    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Reads/Writes
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Web         â”‚      â”‚ REST API    â”‚      â”‚ External DB â”‚
â”‚ Dashboard   â”‚      â”‚ (JSON)      â”‚      â”‚ Connector   â”‚
â”‚ (HTML)      â”‚      â”‚             â”‚      â”‚ (Batch Push)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚
       â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ End Users   â”‚      â”‚ Developers  â”‚      â”‚ User's DB   â”‚
â”‚ (Browsers)  â”‚      â”‚ (API Calls) â”‚      â”‚ (MySQL/etc.)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **6.5 Technology Justifications**

### **Why Render.com?**[12][7][9]
| Requirement | Render Solution |
|-------------|-----------------|
| **Free Hosting** | 512MB RAM, 0.1 CPU free tier[9] |
| **Database** | PostgreSQL 1GB included[8][9] |
| **Auto-Deploy** | GitHub integration (push to deploy)[9] |
| **Cron Jobs** | Built-in scheduling support[9] |
| **SSL** | Free HTTPS certificates[9] |
| **Uptime** | 99.9% SLA (better than PythonAnywhere)[12] |

**Alternatives Considered**:
- âŒ Heroku: Removed free tier in 2022[12]
- âŒ PythonAnywhere: CPU limits too strict for scraping[31]
- âŒ Railway: Only $5 credit/month (burns fast)[32]

### **Why Scrapy?**[33][14][16]
| Feature | Scrapy | BeautifulSoup | Selenium |
|---------|--------|---------------|----------|
| **Speed** | Fast (async) | Slow (sync) | Very Slow |
| **Memory** | Efficient | Efficient | Heavy (Chrome) |
| **Rate Limiting** | Built-in | Manual | Manual |
| **Best For** | Large-scale | Simple | JavaScript |
| **Learning Curve** | Moderate | Easy | Easy |

**Decision**: Scrapy for primary scraping, Playwright for JS-heavy sites[14][15][16]

### **Why PostgreSQL?**[24][8]
| Feature | PostgreSQL | MongoDB | MySQL |
|---------|-----------|---------|-------|
| **Free Tier** | 1GB Render | 512MB Atlas | Limited options |
| **Full-Text Search** | GIN indexes | Text indexes | Limited |
| **JSON Support** | JSONB columns | Native | JSON type |
| **Joins** | Excellent | None (NoSQL) | Good |
| **Best For** | Structured data | Flexible schemas | Simple apps |

**Decision**: PostgreSQL for relational integrity + flexible JSONB for skills[24][8]

***
# **PRODUCT REQUIREMENTS DOCUMENT (PRD) - CONTINUED**

---

# **7. TECHNOLOGY STACK SPECIFICATIONS**

## **7.1 Complete Technology Stack**

### **Backend Technologies**

| Category | Technology | Version | Purpose | Justification |
|----------|-----------|---------|---------|---------------|
| **Programming Language** | Python | 3.12+ | Core development | Industry standard for web scraping[1][2] |
| **Web Framework** | Flask | 3.0+ | Dashboard & API | Lightweight, perfect for free hosting[3][4] |
| **Scraping Framework** | Scrapy | 2.11+ | Primary scraper | Fast, scalable, built-in middleware[5][6] |
| **Browser Automation** | Playwright | 1.40+ | JS-heavy sites | 40% faster than Selenium[7] |
| **HTML Parser** | BeautifulSoup4 | 4.12+ | HTML extraction | Simple, reliable parsing[1][2] |
| **HTTP Client** | Requests | 2.31+ | API calls | De-facto HTTP library |
| **Database ORM** | SQLAlchemy | 2.0+ | Database abstraction | Multi-DB support[8] |
| **Task Scheduling** | APScheduler | 3.10+ | Cron jobs | Python-native scheduler[9] |
| **Data Processing** | Pandas | 2.2+ | CSV/Excel exports | Data manipulation standard[10] |
| **Email** | smtplib (stdlib) | - | Gmail SMTP | Built-in, no dependencies[11][12] |

### **Database Technologies**

| Technology | Version | Purpose | Free Tier Limits |
|-----------|---------|---------|------------------|
| **PostgreSQL** | 16.x | Primary database | 1GB storage, 97 connections[13][4] |
| **Redis** (Optional) | 7.x | Caching | 25MB on Render[4] |

### **Frontend Technologies**

| Technology | Version | Purpose | Why Chosen |
|-----------|---------|---------|------------|
| **HTML5** | - | Markup | Standard |
| **Tailwind CSS** | 3.4+ | Styling | Utility-first, fast development |
| **JavaScript (Vanilla)** | ES6+ | Interactivity | No framework overhead |
| **Jinja2** | 3.1+ | Templating | Flask integration[3] |

### **DevOps & Infrastructure**

| Category | Technology | Cost | Purpose |
|----------|-----------|------|---------|
| **Hosting** | Render.com | Free | Web service + DB[14][4][15] |
| **Version Control** | Git + GitHub | Free | Code management[14] |
| **CI/CD** | GitHub Actions | Free (2000 min/month) | Automated testing/deployment[14] |
| **Monitoring** | Render Logs | Free | Application monitoring[4] |
| **DNS** (Future) | Cloudflare | Free | Domain management |

### **Development Tools**

| Tool | Purpose | Cost |
|------|---------|------|
| **VS Code** | Code editor | Free |
| **Postman** | API testing | Free |
| **DBeaver** | Database management | Free |
| **Git Bash/Terminal** | Command line | Free |
| **Pytest** | Unit testing | Free |

***

## **7.2 Python Package Requirements**

**requirements.txt**:
```txt
# Core Framework
Flask==3.0.0
Flask-RESTful==0.3.10
Flask-CORS==4.0.0

# Web Scraping
scrapy==2.11.0
playwright==1.40.0
beautifulsoup4==4.12.2
lxml==5.1.0
requests==2.31.0

# Database
psycopg2-binary==2.9.9
SQLAlchemy==2.0.23
pymongo==4.6.1  # For MongoDB external DB support

# Data Processing
pandas==2.2.0
numpy==1.26.3

# Task Scheduling
APScheduler==3.10.4

# Security
cryptography==41.0.7  # For Fernet encryption

# Utilities
python-dotenv==1.0.0
pytz==2024.1  # Timezone handling
validators==0.22.0  # URL validation

# Monitoring
structlog==24.1.0  # Structured logging

# Testing (Development only)
pytest==7.4.4
pytest-cov==4.1.0
faker==22.0.0  # Test data generation
```

**Estimated Total Size**: ~250MB (within Render's 512MB limit)[4]

***

## **7.3 External Services**

| Service | Provider | Tier | Usage | Limits |
|---------|----------|------|-------|--------|
| **Email** | Gmail SMTP | Free | Notifications | 500 emails/day[11][12] |
| **Hosting** | Render.com | Free | Web + DB | 512MB RAM, 1GB DB[4] |
| **Repository** | GitHub | Free | Code storage | Unlimited public repos[14] |
| **CI/CD** | GitHub Actions | Free | Automation | 2000 minutes/month[14] |

***

## **7.4 Development Environment Setup**

### **Local Development (Your Laptop)**

**Prerequisites**:
- Python 3.12+ installed
- Git installed
- PostgreSQL 16 installed (or use Docker)
- Text editor (VS Code recommended)

**Setup Commands**:
```bash
# Clone repository
git clone https://github.com/yourusername/pakjobs-aggregator.git
cd pakjobs-aggregator

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium

# Setup database
createdb pakjobs_dev

# Copy environment template
cp .env.example .env
# Edit .env with your credentials

# Run database migrations
python scripts/init_db.py

# Start development server
flask run --debug
```

**Estimated Setup Time**: 15-20 minutes

---

# **8. SYSTEM COMPONENTS DESIGN**

## **8.1 Scraping Orchestrator**

**Purpose**: Centralized controller for all 9 scrapers[9]

**Class Structure**:
```python
class ScrapingOrchestrator:
    """
    Manages execution of all scrapers with error handling,
    logging, and notification.
    """
    
    def __init__(self, db_session, config):
        self.db = db_session
        self.config = config
        self.scrapers = self._load_scrapers()
        self.results = {}
        
    def run_all_scrapers(self, mode='incremental'):
        """
        Execute all scrapers sequentially.
        
        Args:
            mode: 'incremental' or 'full_refresh'
        
        Returns:
            dict: Summary of scraping results
        """
        start_time = datetime.now()
        
        for scraper_name, scraper_class in self.scrapers.items():
            try:
                logger.info(f"Starting {scraper_name}")
                
                # Get last scrape timestamp
                last_scraped = self.db.get_last_scrape_time(scraper_name)
                
                # Initialize scraper
                scraper = scraper_class(
                    mode=mode,
                    last_scraped=last_scraped
                )
                
                # Execute scraping with retry logic
                jobs = self._execute_with_retry(scraper, max_retries=3)
                
                # Store results
                self.results[scraper_name] = {
                    'jobs_found': len(jobs),
                    'status': 'success',
                    'duration': scraper.duration
                }
                
                # Save to database
                self.db.bulk_insert_jobs(jobs)
                
                # Update last scrape timestamp
                self.db.update_scrape_log(scraper_name, len(jobs))
                
                # Politeness delay
                time.sleep(3)
                
            except Exception as e:
                logger.error(f"{scraper_name} failed: {e}")
                self.results[scraper_name] = {
                    'status': 'failed',
                    'error': str(e)
                }
                
                # Send alert email
                self.send_failure_alert(scraper_name, e)
        
        # Calculate summary
        total_jobs = sum(
            r['jobs_found'] for r in self.results.values() 
            if r.get('status') == 'success'
        )
        
        # Push to external DB if configured
        if self.config.external_db_enabled:
            self._push_to_external_db()
        
        # Send completion email
        self.send_summary_email(total_jobs, self.results)
        
        return self.results
```

***

## **8.2 Base Scraper Class**

**Purpose**: Abstract base class for all site-specific scrapers[16][9]

```python
from abc import ABC, abstractmethod
import hashlib
from datetime import datetime

class BaseScraper(ABC):
    """
    Abstract base class for all job site scrapers.
    Enforces consistent interface and implements shared logic.
    """
    
    def __init__(self, mode='incremental', last_scraped=None):
        self.mode = mode
        self.last_scraped = last_scraped or datetime.now()
        self.jobs = []
        self.errors = []
        self.duration = 0
        self.user_agent = self._get_user_agent()
        
    @abstractmethod
    def scrape(self):
        """
        Main scraping logic - must be implemented by subclasses.
        Returns list of job dictionaries.
        """
        pass
    
    @abstractmethod
    def get_site_name(self):
        """Return site identifier (e.g., 'rozee')"""
        pass
    
    def generate_job_id(self, job_url):
        """
        Generate unique UUID from job URL.
        Uses UUID v5 (SHA-1 hash of URL).
        """
        namespace = uuid.UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')
        return str(uuid.uuid5(namespace, job_url))
    
    def is_job_new(self, job_url):
        """
        Check if job already exists in database.
        Enables incremental scraping.
        """
        job_id = self.generate_job_id(job_url)
        return not db.job_exists(job_id)
    
    def validate_job(self, job_dict):
        """
        Validate required fields are present.
        """
        required_fields = ['title', 'company', 'url', 'site_source']
        return all(field in job_dict for field in required_fields)
    
    def clean_text(self, text):
        """
        Remove extra whitespace, newlines, special chars.
        """
        if not text:
            return None
        return ' '.join(text.strip().split())
    
    def extract_salary(self, text):
        """
        Parse salary from text using regex.
        Examples: "PKR 50,000-80,000", "50K-80K"
        """
        import re
        if not text:
            return None, None
            
        # Pattern: digits with optional K/k suffix
        pattern = r'(\d{1,3}(?:,\d{3})*|\d+)\s*[Kk]?\s*[-â€“to]\s*(\d{1,3}(?:,\d{3})*|\d+)\s*[Kk]?'
        match = re.search(pattern, text)
        
        if match:
            min_sal = self._parse_number(match.group(1))
            max_sal = self._parse_number(match.group(2))
            return min_sal, max_sal
        
        return None, None
    
    def _get_user_agent(self):
        """
        Return polite user agent identifying scraper.
        """
        return (
            'Mozilla/5.0 (compatible; PakJobsBot/1.0; '
            '+https://pakjobs.example.com/bot)'
        )
    
    def _parse_number(self, text):
        """
        Convert "50,000" or "50K" to integer.
        """
        text = text.replace(',', '')
        if 'K' in text.upper():
            return int(float(text.replace('K', '').replace('k', ''))) * 1000
        return int(text)
```

***

## **8.3 Scrapy Spider Template**

**Example: Rozee.pk Scraper**[5][6]

```python
import scrapy
from scrapy.crawler import CrawlerProcess
from datetime import datetime, timedelta

class RozeeScraper(BaseScraper, scrapy.Spider):
    """
    Scraper for Rozee.pk (Pakistan's largest job site).
    Uses Scrapy for efficient crawling.
    """
    
    name = 'rozee'
    allowed_domains = ['rozee.pk']
    
    # Custom settings for politeness
    custom_settings = {
        'DOWNLOAD_DELAY': 3,  # 3 seconds between requests
        'CONCURRENT_REQUESTS': 1,  # Sequential only
        'ROBOTSTXT_OBEY': True,  # Respect robots.txt
        'USER_AGENT': 'Mozilla/5.0 (compatible; PakJobsBot/1.0)',
        'HTTPCACHE_ENABLED': False  # Always fetch fresh data
    }
    
    def start_requests(self):
        """
        Generate initial URLs based on scraping mode.
        """
        base_url = 'https://www.rozee.pk/job/jsearch/q/'
        
        if self.mode == 'incremental':
            # Only fetch jobs from last 24 hours
            yield scrapy.Request(
                url=f'{base_url}?fpn=1&sort=date',
                callback=self.parse_job_list,
                meta={'page': 1}
            )
        else:
            # Full refresh - all pages
            yield scrapy.Request(
                url=f'{base_url}?fpn=1',
                callback=self.parse_job_list,
                meta={'page': 1}
            )
    
    def parse_job_list(self, response):
        """
        Parse job listing page, extract job URLs.
        """
        page = response.meta['page']
        
        # Extract job links
        job_urls = response.css('div.job h3 a::attr(href)').getall()
        
        for job_url in job_urls:
            full_url = response.urljoin(job_url)
            
            # Check if already scraped (incremental mode)
            if self.mode == 'incremental' and not self.is_job_new(full_url):
                # Stop pagination when hitting old job
                self.logger.info(f"Hit existing job, stopping at page {page}")
                return
            
            # Scrape job detail page
            yield scrapy.Request(
                url=full_url,
                callback=self.parse_job_detail,
                meta={'job_url': full_url}
            )
        
        # Pagination - next page
        next_page = response.css('a.next::attr(href)').get()
        if next_page and page < 50:  # Max 50 pages safety limit
            yield scrapy.Request(
                url=response.urljoin(next_page),
                callback=self.parse_job_list,
                meta={'page': page + 1}
            )
    
    def parse_job_detail(self, response):
        """
        Extract all job details from detail page.
        """
        job_url = response.meta['job_url']
        
        # Extract fields using CSS selectors
        job = {
            'id': self.generate_job_id(job_url),
            'url': job_url,
            'site_source': 'rozee',
            'title': self.clean_text(response.css('h1.job-title::text').get()),
            'company': self.clean_text(response.css('a.company-name::text').get()),
            'location': self.clean_text(response.css('span.location::text').get()),
            'posted_date': self.parse_date(response.css('span.posted-date::text').get()),
            'deadline': self.parse_date(response.css('span.deadline::text').get()),
            'experience': self.clean_text(response.css('span.experience::text').get()),
            'education': self.clean_text(response.css('span.education::text').get()),
            'job_type': self.clean_text(response.css('span.job-type::text').get()),
            'description': self.clean_text(response.css('div.job-description::text').getall()),
            'skills': response.css('span.skill-tag::text').getall(),
            'salary_text': response.css('span.salary::text').get(),
            'application_email': response.css('a.apply-email::attr(href)').get(),
            'application_url': response.css('a.apply-button::attr(href)').get(),
            'is_remote': 'remote' in response.text.lower(),
            'scraped_at': datetime.utcnow().isoformat()
        }
        
        # Parse salary range
        job['salary_min'], job['salary_max'] = self.extract_salary(job['salary_text'])
        
        # Validate before yielding
        if self.validate_job(job):
            yield job
        else:
            self.logger.warning(f"Invalid job data: {job_url}")
    
    def parse_date(self, date_text):
        """
        Convert relative date ("2 days ago") to ISO format.
        """
        if not date_text:
            return None
            
        # Example parsing logic
        if 'today' in date_text.lower():
            return datetime.now().date().isoformat()
        elif 'yesterday' in date_text.lower():
            return (datetime.now() - timedelta(days=1)).date().isoformat()
        elif 'days ago' in date_text.lower():
            days = int(date_text.split()[0])
            return (datetime.now() - timedelta(days=days)).date().isoformat()
        
        # Add more parsing logic as needed
        return None
    
    def get_site_name(self):
        return 'rozee'
```

***

## **8.4 Playwright Scraper Template**

**Example: Indeed.pk Scraper (JavaScript-heavy)**[7][17]

```python
from playwright.sync_api import sync_playwright
import time

class IndeedScraper(BaseScraper):
    """
    Scraper for Indeed.pk using Playwright for JavaScript rendering.
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.base_url = 'https://pk.indeed.com'
        
    def scrape(self):
        """
        Main scraping logic using Playwright.
        """
        start_time = time.time()
        
        with sync_playwright() as p:
            # Launch browser (headless mode for efficiency)
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent=self.user_agent,
                viewport={'width': 1920, 'height': 1080}
            )
            page = context.new_page()
            
            try:
                # Navigate to job search
                search_url = f'{self.base_url}/jobs?q=&l=Pakistan&fromage=1'  # Last 24 hours
                page.goto(search_url, wait_until='networkidle')
                
                page_num = 1
                while page_num <= 50:  # Max 50 pages
                    # Wait for job cards to load
                    page.wait_for_selector('div.job_seen_beacon', timeout=10000)
                    
                    # Extract job URLs
                    job_elements = page.query_selector_all('h2.jobTitle a')
                    
                    for element in job_elements:
                        job_url = self.base_url + element.get_attribute('href')
                        
                        # Check if new (incremental mode)
                        if self.mode == 'incremental' and not self.is_job_new(job_url):
                            self.logger.info("Hit existing job, stopping")
                            browser.close()
                            self.duration = time.time() - start_time
                            return self.jobs
                        
                        # Scrape job detail
                        job_data = self._scrape_job_detail(page, job_url)
                        if job_data:
                            self.jobs.append(job_data)
                        
                        # Politeness delay
                        time.sleep(3)
                    
                    # Next page
                    next_button = page.query_selector('a[aria-label="Next Page"]')
                    if next_button:
                        next_button.click()
                        page.wait_for_load_state('networkidle')
                        page_num += 1
                    else:
                        break
                
            except Exception as e:
                self.logger.error(f"Indeed scraping error: {e}")
                self.errors.append(str(e))
            
            finally:
                browser.close()
        
        self.duration = time.time() - start_time
        return self.jobs
    
    def _scrape_job_detail(self, page, job_url):
        """
        Navigate to job detail page and extract data.
        """
        try:
            page.goto(job_url, wait_until='networkidle')
            
            job = {
                'id': self.generate_job_id(job_url),
                'url': job_url,
                'site_source': 'indeed',
                'title': self.clean_text(page.text_content('h1.jobsearch-JobInfoHeader-title')),
                'company': self.clean_text(page.text_content('div.jobsearch-CompanyInfoContainer a')),
                'location': self.clean_text(page.text_content('div.jobsearch-JobInfoHeader-subtitle div:nth-child(2)')),
                'description': self.clean_text(page.text_content('div.jobsearch-jobDescriptionText')),
                'salary_text': self.clean_text(page.text_content('div.salary-snippet')),
                'job_type': self.clean_text(page.text_content('div.jobsearch-JobMetadataHeader-item')),
                'posted_date': self._parse_indeed_date(page.text_content('div.jobsearch-JobMetadataFooter')),
                'scraped_at': datetime.utcnow().isoformat()
            }
            
            # Parse salary
            job['salary_min'], job['salary_max'] = self.extract_salary(job['salary_text'])
            
            # Extract skills from description (simple keyword matching)
            job['skills'] = self._extract_skills(job['description'])
            
            if self.validate_job(job):
                return job
            
        except Exception as e:
            self.logger.error(f"Error scraping {job_url}: {e}")
            self.errors.append(f"{job_url}: {e}")
        
        return None
    
    def _extract_skills(self, description):
        """
        Extract common tech skills from description using keyword matching.
        """
        if not description:
            return []
        
        skill_keywords = [
            'python', 'javascript', 'java', 'react', 'node.js', 'sql',
            'aws', 'docker', 'kubernetes', 'django', 'flask', 'mongodb',
            'postgresql', 'redis', 'git', 'agile', 'scrum', 'machine learning',
            'data science', 'tableau', 'power bi', 'excel'
        ]
        
        desc_lower = description.lower()
        found_skills = [skill for skill in skill_keywords if skill in desc_lower]
        
        return list(set(found_skills))  # Remove duplicates
    
    def get_site_name(self):
        return 'indeed'
```

***

# **9. DATABASE ARCHITECTURE**

## **9.1 Database Schema Design**

### **Primary Tables**

**1. jobs** (Main table - stores all scraped jobs)[13]

```sql
CREATE TABLE jobs (
    -- Primary Key
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    
    -- Source Information
    site_source VARCHAR(50) NOT NULL,  -- rozee, mustakbil, indeed, etc.
    source_url TEXT NOT NULL UNIQUE,  -- Original job URL
    
    -- Job Details
    title VARCHAR(255) NOT NULL,
    company VARCHAR(255) NOT NULL,
    location VARCHAR(255),
    city VARCHAR(100),
    country VARCHAR(50) DEFAULT 'Pakistan',
    
    -- Salary Information
    salary_text TEXT,  -- Original text (e.g., "PKR 50,000-80,000")
    salary_min INTEGER,  -- Parsed minimum (50000)
    salary_max INTEGER,  -- Parsed maximum (80000)
    salary_currency VARCHAR(10) DEFAULT 'PKR',
    
    -- Job Content
    description TEXT,  -- Full job description
    requirements TEXT,  -- Specific requirements section
    benefits TEXT,  -- Benefits/perks section
    
    -- Skills & Qualifications
    skills JSONB,  -- Array of skills: ["Python", "SQL", "React"]
    experience_level VARCHAR(50),  -- Entry, Mid, Senior, Executive
    experience_years VARCHAR(50),  -- "2-4 years", "5+ years"
    education VARCHAR(255),  -- "Bachelor's in Computer Science"
    
    -- Job Type & Mode
    job_type VARCHAR(50),  -- Full-time, Part-time, Contract, Internship
    is_remote BOOLEAN DEFAULT FALSE,
    is_hybrid BOOLEAN DEFAULT FALSE,
    is_onsite BOOLEAN DEFAULT TRUE,
    
    -- Application Details
    application_method VARCHAR(20),  -- email, online, url
    application_email VARCHAR(255),
    application_url TEXT,
    deadline_date DATE,
    
    -- Dates
    posted_date DATE,
    scraped_at TIMESTAMP NOT NULL DEFAULT NOW(),
    last_updated TIMESTAMP NOT NULL DEFAULT NOW(),
    
    -- Status
    is_active BOOLEAN DEFAULT TRUE,
    is_duplicate BOOLEAN DEFAULT FALSE,
    duplicate_of UUID REFERENCES jobs(id),
    
    -- Indexes for fast queries
    CONSTRAINT jobs_source_url_key UNIQUE (source_url)
);

-- Indexes for performance
CREATE INDEX idx_jobs_site_source ON jobs(site_source);
CREATE INDEX idx_jobs_location ON jobs(location);
CREATE INDEX idx_jobs_city ON jobs(city);
CREATE INDEX idx_jobs_posted_date ON jobs(posted_date DESC);
CREATE INDEX idx_jobs_scraped_at ON jobs(scraped_at DESC);
CREATE INDEX idx_jobs_salary_range ON jobs(salary_min, salary_max) WHERE salary_min IS NOT NULL;

-- Full-text search index
CREATE INDEX idx_jobs_search ON jobs USING GIN(
    to_tsvector('english', 
        COALESCE(title, '') || ' ' || 
        COALESCE(description, '') || ' ' ||
        COALESCE(company, '')
    )
);

-- JSONB index for skills
CREATE INDEX idx_jobs_skills ON jobs USING GIN(skills);
```

***

**2. scrape_logs** (Monitoring table)[16]

```sql
CREATE TABLE scrape_logs (
    id SERIAL PRIMARY KEY,
    site_name VARCHAR(50) NOT NULL,
    scrape_mode VARCHAR(20) NOT NULL,  -- incremental, full_refresh
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    duration_seconds INTEGER,
    
    -- Results
    jobs_found INTEGER DEFAULT 0,
    jobs_new INTEGER DEFAULT 0,
    jobs_updated INTEGER DEFAULT 0,
    
    -- Status
    status VARCHAR(20) NOT NULL,  -- success, partial, failed
    error_message TEXT,
    error_count INTEGER DEFAULT 0,
    
    -- Metadata
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_scrape_logs_site ON scrape_logs(site_name, start_time DESC);
```

***

**3. user_config** (System configuration)[8]

```sql
CREATE TABLE user_config (
    id SERIAL PRIMARY KEY,
    config_key VARCHAR(100) NOT NULL UNIQUE,
    config_value TEXT,
    is_encrypted BOOLEAN DEFAULT FALSE,
    description TEXT,
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Sample configuration entries
INSERT INTO user_config (config_key, config_value, description) VALUES
('scrape_schedule', '0 3 * * *', 'Cron expression for daily scraping'),
('email_recipients', 'admin@example.com', 'Comma-separated email list'),
('external_db_enabled', 'false', 'Enable external database push'),
('external_db_type', 'postgresql', 'Database type: postgresql, mysql, mongodb'),
('external_db_connection', '', 'Encrypted connection string'),
('max_scrape_pages', '50', 'Maximum pages per site');
```

***

**4. analytics_cache** (Pre-computed analytics for dashboard)

```sql
CREATE TABLE analytics_cache (
    id SERIAL PRIMARY KEY,
    metric_name VARCHAR(100) NOT NULL,
    metric_value JSONB NOT NULL,
    computed_at TIMESTAMP DEFAULT NOW(),
    expires_at TIMESTAMP,
    
    CONSTRAINT analytics_cache_metric_key UNIQUE (metric_name)
);

-- Examples of cached metrics:
-- metric_name: 'salary_trends_by_role'
-- metric_value: {"Software Engineer": {"avg": 85000, "min": 50000, "max": 150000}}

-- metric_name: 'top_skills_30days'
-- metric_value: {"Python": 450, "JavaScript": 380, "SQL": 320}
```

***

## **9.2 Database Relationships**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       jobs          â”‚  (Main table - 50K+ rows)
â”‚  - id (PK)          â”‚
â”‚  - site_source      â”‚
â”‚  - source_url (UK)  â”‚
â”‚  - title, company   â”‚
â”‚  - location, salary â”‚
â”‚  - skills (JSONB)   â”‚
â”‚  - posted_date      â”‚
â”‚  - scraped_at       â”‚
â”‚  - duplicate_of (FK)â”œâ”€â”€â” Self-referencing
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ for duplicate detection
            â”‚            â”‚
            â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                           â”‚
            â”‚                           â–¼
            â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              â”‚  jobs (duplicates)  â”‚
            â”‚              â”‚  - is_duplicate=TRUEâ”‚
            â”‚              â”‚  - duplicate_of     â”‚
            â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ (Referenced by)
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   scrape_logs       â”‚  (Monitoring - 100s of rows)
â”‚  - id (PK)          â”‚
â”‚  - site_name        â”‚
â”‚  - jobs_found       â”‚
â”‚  - status           â”‚
â”‚  - start_time       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     (No FK, but conceptually related)
     
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   user_config       â”‚  (Configuration - ~20 rows)
â”‚  - config_key (UK)  â”‚
â”‚  - config_value     â”‚
â”‚  - is_encrypted     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  analytics_cache    â”‚  (Performance - ~50 rows)
â”‚  - metric_name (UK) â”‚
â”‚  - metric_value     â”‚
â”‚  - expires_at       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **9.3 Sample Queries**

**Query 1: Full-text search with filters**[13]
```sql
SELECT id, title, company, location, salary_min, salary_max, posted_date
FROM jobs
WHERE 
    to_tsvector('english', title || ' ' || description) @@ to_tsquery('python & developer')
    AND location ILIKE '%karachi%'
    AND salary_min >= 50000
    AND posted_date > CURRENT_DATE - INTERVAL '30 days'
    AND is_active = TRUE
ORDER BY posted_date DESC
LIMIT 100;
```

**Query 2: Salary trends by role**
```sql
SELECT 
    LOWER(TRIM(title)) AS role,
    COUNT(*) AS job_count,
    AVG(salary_min) AS avg_min_salary,
    AVG(salary_max) AS avg_max_salary,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY salary_min) AS median_salary
FROM jobs
WHERE 
    salary_min IS NOT NULL
    AND posted_date > CURRENT_DATE - INTERVAL '90 days'
GROUP BY LOWER(TRIM(title))
HAVING COUNT(*) >= 10
ORDER BY job_count DESC
LIMIT 20;
```

**Query 3: Top skills in demand**
```sql
SELECT 
    skill,
    COUNT(*) AS mention_count
FROM jobs,
    LATERAL jsonb_array_elements_text(skills) AS skill
WHERE 
    posted_date > CURRENT_DATE - INTERVAL '30 days'
GROUP BY skill
ORDER BY mention_count DESC
LIMIT 30;
```

**Query 4: Scraper health check**
```sql
SELECT 
    site_name,
    status,
    jobs_found,
    duration_seconds,
    start_time,
    error_message
FROM scrape_logs
WHERE start_time >= CURRENT_DATE
ORDER BY start_time DESC;
```

***

## **9.4 Data Retention Policy**

| Data Type | Retention Period | Cleanup Method |
|-----------|-----------------|----------------|
| **Active Jobs** | 90 days | Auto-archive (is_active=FALSE) |
| **Archived Jobs** | 1 year | Manual purge |
| **Scrape Logs** | 6 months | DELETE WHERE created_at < NOW() - INTERVAL '6 months' |
| **Analytics Cache** | 7 days | Auto-refresh on expiry |
| **Config** | Permanent | Manual updates only |

**Automated Cleanup Job** (runs weekly):
```python
def cleanup_old_data():
    """Remove expired data to stay within 1GB free tier limit."""
    
    # Archive jobs older than 90 days
    db.execute("""
        UPDATE jobs 
        SET is_active = FALSE 
        WHERE posted_date < CURRENT_DATE - INTERVAL '90 days'
        AND is_active = TRUE
    """)
    
    # Delete old scrape logs
    db.execute("""
        DELETE FROM scrape_logs 
        WHERE created_at < NOW() - INTERVAL '6 months'
    """)
    
    # Refresh expired analytics
    db.execute("""
        DELETE FROM analytics_cache 
        WHERE expires_at < NOW()
    """)
```

***

## **9.5 Database Backup Strategy**

**Automated Backups** (via Render):[4][13]
- **Daily snapshots**: Render auto-backup (7-day retention on free tier)
- **Manual export**: Weekly pg_dump to GitHub (encrypted)

**Backup Script**:
```bash
#!/bin/bash
# backup_db.sh

# Set variables
DB_NAME="pakjobs_prod"
BACKUP_DIR="./backups"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/pakjobs_$DATE.sql.gz"

# Create backup directory
mkdir -p $BACKUP_DIR

# Dump database (compressed)
pg_dump $DATABASE_URL | gzip > $BACKUP_FILE

# Keep only last 7 backups
ls -t $BACKUP_DIR/*.sql.gz | tail -n +8 | xargs rm -f

echo "Backup completed: $BACKUP_FILE"
```

***

# **10. SCRAPER SPECIFICATIONS (9 SITES)**

## **10.1 Site-by-Site Analysis**

### **Site 1: Rozee.pk**

| Attribute | Details |
|-----------|---------|
| **URL** | https://www.rozee.pk[18] |
| **Market Position** | #1 in Pakistan (50%+ market share)[19] |
| **Estimated Jobs** | 8,000-12,000 active listings |
| **Tech Stack** | PHP, MySQL (traditional server-rendered) |
| **JavaScript Dependency** | Low (mostly static HTML) |
| **Scraping Approach** | Scrapy (fast, efficient)[5] |
| **Pagination** | URL-based (?fpn=1, ?fpn=2...) |
| **Rate Limiting** | Moderate (3s delay sufficient) |
| **robots.txt** | Allows crawling (as of Oct 2025) |

**Key Selectors**:
```python
ROZEE_SELECTORS = {
    'job_list': 'div.job',
    'job_link': 'h3.job-title a::attr(href)',
    'title': 'h1.job-title::text',
    'company': 'a.company-name::text',
    'location': 'span.location::text',
    'salary': 'span.salary::text',
    'description': 'div.job-description::text',
    'posted_date': 'span.posted-date::text',
    'skills': 'span.skill-tag::text',
    'apply_link': 'a.apply-button::attr(href)'
}
```

**Estimated Scrape Time**: 12-15 minutes (full), 2-3 minutes (incremental)

***

### **Site 2: Mustakbil.com**

| Attribute | Details |
|-----------|---------|
| **URL** | https://www.mustakbil.com |
| **Market Position** | #2 local player[19][20] |
| **Estimated Jobs** | 5,000-8,000 active listings |
| **Tech Stack** | ASP.NET (Microsoft stack) |
| **JavaScript Dependency** | Low |
| **Scraping Approach** | Scrapy |
| **Pagination** | URL parameters (?page=1) |
| **Rate Limiting** | Low (friendly site) |

**Key Selectors**:
```python
MUSTAKBIL_SELECTORS = {
    'job_list': 'div.job-item',
    'job_link': 'a.job-link::attr(href)',
    'title': 'h2.job-title::text',
    'company': 'div.company::text',
    'location': 'div.location::text',
    'salary': 'div.salary-info::text',
    'description': 'div.description p::text',
    'experience': 'span.experience::text',
    'education': 'span.education::text'
}
```

**Estimated Scrape Time**: 8-10 minutes (full), 1-2 minutes (incremental)

***

### **Site 3: Indeed.pk**

| Attribute | Details |
|-----------|---------|
| **URL** | https://pk.indeed.com[21] |
| **Market Position** | Global brand, strong Pakistan presence[19] |
| **Estimated Jobs** | 6,000-10,000 (aggregates from other sites too)[17] |
| **Tech Stack** | React (heavy JavaScript) |
| **JavaScript Dependency** | High |
| **Scraping Approach** | Playwright (JavaScript rendering required)[7][17] |
| **Pagination** | Infinite scroll + URL parameters |
| **Rate Limiting** | Moderate (use delays) |
| **Anti-Bot Measures** | CAPTCHA (occasionally) |

**Key Selectors**:
```python
INDEED_SELECTORS = {
    'job_cards': 'div.job_seen_beacon',
    'job_link': 'h2.jobTitle a',
    'title': 'h1.jobsearch-JobInfoHeader-title span',
    'company': 'div[data-testid="inlineHeader-companyName"] a',
    'location': 'div[data-testid="inlineHeader-companyLocation"]',
    'salary': 'div#salaryInfoAndJobType span',
    'description': 'div#jobDescriptionText',
    'job_type': 'div.jobsearch-JobMetadataHeader-item'
}
```

**Estimated Scrape Time**: 15-20 minutes (Playwright slower)[7]

***

### **Site 4: BrightSpyre.com**

| Attribute | Details |
|-----------|---------|
| **URL** | https://www.brightspyre.com |
| **Market Position** | Tech/IT focused niche |
| **Estimated Jobs** | 1,000-2,000 (mostly tech) |
| **Tech Stack** | WordPress |
| **JavaScript Dependency** | Low |
| **Scraping Approach** | Scrapy |
| **Pagination** | Standard WordPress (/page/2/) |

**Estimated Scrape Time**: 5-7 minutes

***

### **Site 5: Bayt.com**

| Attribute | Details |
|-----------|---------|
| **URL** | https://www.bayt.com/en/pakistan |
| **Market Position** | Middle East leader, Pakistan section[19] |
| **Estimated Jobs** | 3,000-5,000 |
| **Tech Stack** | ASP.NET MVC |
| **JavaScript Dependency** | Moderate |
| **Scraping Approach** | Scrapy with AJAX handling |
| **Anti-Bot Measures** | Rate limiting (be polite) |

**Estimated Scrape Time**: 10-12 minutes

***

### **Site 6: Jobz.pk**

| Attribute | Details |
|-----------|---------|
| **URL** | https://www.jobz.pk |
| **Market Position** | General marketplace |
| **Estimated Jobs** | 2,000-4,000 |
| **Tech Stack** | PHP Laravel |
| **JavaScript Dependency** | Low |
| **Scraping Approach** | Scrapy |

**Estimated Scrape Time**: 6-8 minutes

***

### **Site 7: Bayrozgar.com**

| Attribute | Details |
|-----------|---------|
| **URL** | https://www.bayrozgar.com |
| **Market Position** | Urdu/local focus |
| **Estimated Jobs** | 1,500-3,000 |
| **Tech Stack** | Custom PHP |
| **JavaScript Dependency** | Low |
| **Scraping Approach** | Scrapy |
| **Special Note** | Urdu text handling (UTF-8 encoding) |

**Estimated Scrape Time**: 5-7 minutes

***

### **Site 8: JobsAlert.pk**

| Attribute | Details |
|-----------|---------|
| **URL** | https://www.jobsalert.pk |
| **Market Position** | Government + private jobs |
| **Estimated Jobs** | 2,000-3,500 |
| **Tech Stack** | WordPress |
| **JavaScript Dependency** | Low |
| **Scraping Approach** | Scrapy |

**Estimated Scrape Time**: 5-6 minutes

***

### **Site 9: PakPositions.com**

| Attribute | Details |
|-----------|---------|
| **URL** | https://www.pakpositions.com |
| **Market Position** | SME focus |
| **Estimated Jobs** | 1,000-2,000 |
| **Tech Stack** | PHP |
| **JavaScript Dependency** | Low |
| **Scraping Approach** | Scrapy |

**Estimated Scrape Time**: 4-5 minutes

***

## **10.2 Total Scraping Estimates**

| Metric | Full Scrape | Incremental Scrape |
|--------|-------------|-------------------|
| **Total Jobs** | 30,000-50,000 | 500-2,000 (daily new) |
| **Total Time** | 80-110 minutes | 15-25 minutes |
| **Bandwidth** | 3-5 GB | 200-500 MB |
| **Database Growth** | Initial load | ~5,000 rows/day |

***

# **11. INCREMENTAL SCRAPING LOGIC**

## **11.1 Algorithm Overview**[22][16]

**Core Concept**: Only scrape jobs posted since last successful run, dramatically reducing time/bandwidth.[16]

### **Incremental Scraping Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  START INCREMENTAL SCRAPE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Database: last_scraped_at for site       â”‚
â”‚  Example: "2025-10-13 03:00:00"                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build Scrape URL with Date Filter              â”‚
â”‚  Example: "rozee.pk/jobs?sort=date&fpn=1"      â”‚
â”‚  (Most recent jobs first)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOR EACH Job in Page:                          â”‚
â”‚    â”œâ”€â–¶ Extract job_url                          â”‚
â”‚    â”œâ”€â–¶ Generate job_id = UUID5(job_url)        â”‚
â”‚    â”œâ”€â–¶ CHECK: Does job_id exist in database?   â”‚
â”‚    â”‚   â”‚                                        â”‚
â”‚    â”‚   â”œâ”€â–¶ YES: Job already scraped             â”‚
â”‚    â”‚   â”‚   â”‚                                    â”‚
â”‚    â”‚   â”‚   â””â”€â–¶ STOP PAGINATION (hit old jobs)  â”‚
â”‚    â”‚   â”‚                                        â”‚
â”‚    â”‚   â””â”€â–¶ NO: Job is NEW                       â”‚
â”‚    â”‚       â”‚                                    â”‚
â”‚    â”‚       â”œâ”€â–¶ Scrape job details               â”‚
â”‚    â”‚       â”œâ”€â–¶ Validate data                    â”‚
â”‚    â”‚       â””â”€â–¶ Insert into database             â”‚
â”‚    â”‚                                            â”‚
â”‚    â””â”€â–¶ NEXT Job                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Update scrape_logs:                            â”‚
â”‚    - jobs_new = X                               â”‚
â”‚    - last_scraped_at = NOW()                    â”‚
â”‚    - duration_seconds = Y                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  END INCREMENTAL SCRAPE                         â”‚
â”‚  Result: Scraped only NEW jobs (10x faster!)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **11.2 Implementation Code**

```python
class IncrementalScrapingMixin:
    """
    Mixin providing incremental scraping logic for all scrapers.
    """
    
    def get_last_scrape_timestamp(self):
        """
        Retrieve last successful scrape time for this site from database.
        """
        query = """
            SELECT MAX(end_time) 
            FROM scrape_logs 
            WHERE site_name = %s AND status = 'success'
        """
        result = db.execute(query, (self.get_site_name(),))
        return result[0][0] if result else None
    
    def is_job_already_scraped(self, job_url):
        """
        Check if job URL exists in database (prevents duplicates).
        Uses unique index on source_url for O(1) lookup.
        """
        query = "SELECT EXISTS(SELECT 1 FROM jobs WHERE source_url = %s)"
        result = db.execute(query, (job_url,))
        return result[0][0]
    
    def should_stop_pagination(self, jobs_found_this_page, jobs_skipped_this_page):
        """
        Stop pagination when hitting mostly old jobs (efficiency).
        
        Logic: If 80%+ of jobs on current page are already scraped,
        assume remaining pages are also old.
        """
        if jobs_found_this_page == 0:
            return False  # First page, keep going
        
        skip_ratio = jobs_skipped_this_page / (jobs_found_this_page + jobs_skipped_this_page)
        return skip_ratio > 0.8  # 80% threshold
    
    def scrape_with_incremental_logic(self):
        """
        Main scraping method with incremental optimization.
        """
        last_scraped = self.get_last_scrape_timestamp()
        
        for page_num in range(1, self.max_pages + 1):
            jobs_this_page = self.scrape_page(page_num)
            jobs_new = 0
            jobs_skipped = 0
            
            for job_url in jobs_this_page:
                if self.is_job_already_scraped(job_url):
                    jobs_skipped += 1
                    self.logger.debug(f"Skipping existing job: {job_url}")
                    continue
                
                # Scrape new job
                job_data = self.scrape_job_detail(job_url)
                if job_data:
                    self.save_job(job_data)
                    jobs_new += 1
            
            # Early termination check
            if self.should_stop_pagination(jobs_new, jobs_skipped):
                self.logger.info(
                    f"Stopping at page {page_num}: {jobs_skipped}/{jobs_new+jobs_skipped} "
                    f"already scraped (efficiency optimization)"
                )
                break
        
        return jobs_new
```

***

## **11.3 Optimization Strategies**

### **Strategy 1: Date-Based Filtering** (Site-Specific)[17][16]

Some sites support date filters in search URLs:

```python
# Indeed.pk: Last 24 hours
url = "https://pk.indeed.com/jobs?q=&l=Pakistan&fromage=1"

# Rozee.pk: Sort by date (most recent first)
url = "https://www.rozee.pk/job/jsearch/q/?sort=date"

# Mustakbil: Date range filter
url = "https://www.mustakbil.com/jobs?posted_after=2025-10-13"
```

**Benefit**: Site pre-filters results, scraper only sees new jobs[16]

***

### **Strategy 2: Early Termination** (Pagination Optimization)

```python
def scrape_with_early_termination(self):
    """
    Stop pagination as soon as we hit previously-scraped jobs.
    """
    for page_num in range(1, 100):  # Max 100 pages safety limit
        job_urls = self.extract_job_urls(page_num)
        
        for job_url in job_urls:
            if self.is_job_already_scraped(job_url):
                # Hit old job - stop pagination immediately
                self.logger.info(f"Early termination at page {page_num}")
                return
            
            self.scrape_and_save(job_url)
```

**Benefit**: Stops after 1-2 pages instead of scraping all 50+ pages[16]

***

### **Strategy 3: Timestamp Tracking** (Precision)

```python
# Store precise scrape timestamps per site
scrape_logs = {
    'rozee': datetime(2025, 10, 13, 3, 15, 0),
    'indeed': datetime(2025, 10, 13, 3, 28, 0),
    'mustakbil': datetime(2025, 10, 13, 3, 40, 0)
}

# Next run: Only scrape jobs posted after these times
for site, last_time in scrape_logs.items():
    scraper = get_scraper(site)
    scraper.scrape_since(last_time)
```

***

## **11.4 Full Refresh Strategy**

**When to Run Full Refresh**:[16]
- Weekly (every Sunday at 2 AM)
- After scraper code updates
- After site layout changes
- Manual trigger from dashboard

**Full Refresh Logic**:
```python
def run_full_refresh(site_name):
    """
    Scrape ALL jobs, update existing records, find missed jobs.
    """
    scraper = get_scraper(site_name)
    scraper.mode = 'full_refresh'
    scraper.max_pages = 100  # No early termination
    
    all_jobs = scraper.scrape()
    
    # Update existing jobs (check for changes)
    for job in all_jobs:
        existing = db.get_job_by_url(job['url'])
        if existing:
            # Check if any field changed
            if job_has_changes(existing, job):
                db.update_job(job)
        else:
            db.insert_job(job)
```

***

## **11.5 Performance Comparison**

| Scraping Mode | Jobs Scraped | Time | Bandwidth | Database Writes |
|---------------|--------------|------|-----------|-----------------|
| **Full Scrape** | 50,000 | 90 min | 4 GB | 50,000 inserts |
| **Incremental** | 2,000 (new) | 15 min | 400 MB | 2,000 inserts |
| **Efficiency Gain** | 96% less | 83% faster | 90% less | 96% less |

**Conclusion**: Incremental scraping is critical for free-tier operations[22][16]

***

# **PRODUCT REQUIREMENTS DOCUMENT (PRD) - PART 3**

***

# **12. DATA PIPELINE & INTEGRATION**

## **12.1 Data Flow Architecture**

### **End-to-End Data Pipeline**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: DATA ACQUISITION                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Web Scrapers (9 sites) â†’ Raw HTML/JSON                  â”‚  â”‚
â”‚  â”‚  Output: Python dictionaries with job data               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 2: DATA VALIDATION & CLEANING                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  âœ“ Required fields present (title, company, URL)         â”‚  â”‚
â”‚  â”‚  âœ“ Remove HTML tags, extra whitespace                    â”‚  â”‚
â”‚  â”‚  âœ“ Parse salary ranges to integers                       â”‚  â”‚
â”‚  â”‚  âœ“ Standardize location names (KARACHI â†’ Karachi)        â”‚  â”‚
â”‚  â”‚  âœ“ Extract skills from description (NLP)                 â”‚  â”‚
â”‚  â”‚  âœ“ Validate URLs, emails                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 3: DEDUPLICATION                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Check if job URL already exists (exact match)          â”‚  â”‚
â”‚  â”‚  â€¢ Fuzzy match: title + company similarity (85%+)        â”‚  â”‚
â”‚  â”‚  â€¢ Mark duplicates: is_duplicate=TRUE, link to original  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 4: ENRICHMENT                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Extract city from location (regex patterns)            â”‚  â”‚
â”‚  â”‚  â€¢ Categorize experience level (junior/mid/senior)        â”‚  â”‚
â”‚  â”‚  â€¢ Detect remote/hybrid keywords                         â”‚  â”‚
â”‚  â”‚  â€¢ Calculate posting age (days since posted)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 5: PRIMARY STORAGE (PostgreSQL)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  INSERT INTO jobs (...) VALUES (...)                      â”‚  â”‚
â”‚  â”‚  ON CONFLICT (source_url) DO UPDATE ...                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
                       â–¼                 â–¼                 â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 6: DATA DISTRIBUTION                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Local CSV    â”‚  â”‚ External DB  â”‚  â”‚ Analytics    â”‚         â”‚
â”‚  â”‚ Export       â”‚  â”‚ Push         â”‚  â”‚ Cache Update â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **12.2 Data Validation Module**

### **Validation Rules**

```python
from pydantic import BaseModel, validator, HttpUrl
from typing import Optional, List
from datetime import date

class JobSchema(BaseModel):
    """
    Pydantic model for job data validation.
    Ensures data integrity before database insertion.
    """
    
    # Required fields
    title: str
    company: str
    source_url: HttpUrl
    site_source: str
    
    # Optional fields
    location: Optional[str] = None
    city: Optional[str] = None
    salary_text: Optional[str] = None
    salary_min: Optional[int] = None
    salary_max: Optional[int] = None
    description: Optional[str] = None
    skills: Optional[List[str]] = []
    experience_level: Optional[str] = None
    job_type: Optional[str] = None
    is_remote: bool = False
    posted_date: Optional[date] = None
    application_url: Optional[HttpUrl] = None
    application_email: Optional[str] = None
    
    @validator('title')
    def title_not_empty(cls, v):
        """Title must be non-empty after stripping."""
        if not v or not v.strip():
            raise ValueError('Title cannot be empty')
        if len(v) > 255:
            raise ValueError('Title too long (max 255 chars)')
        return v.strip()
    
    @validator('company')
    def company_valid(cls, v):
        """Company name validation."""
        if not v or not v.strip():
            raise ValueError('Company name required')
        return v.strip()
    
    @validator('site_source')
    def valid_site(cls, v):
        """Ensure site_source is from allowed list."""
        valid_sites = [
            'rozee', 'mustakbil', 'indeed', 'brightspyre',
            'bayt', 'jobz', 'bayrozgar', 'jobsalert', 'pakpositions'
        ]
        if v not in valid_sites:
            raise ValueError(f'Invalid site_source: {v}')
        return v
    
    @validator('salary_min', 'salary_max')
    def salary_positive(cls, v):
        """Salary must be positive if present."""
        if v is not None and v < 0:
            raise ValueError('Salary cannot be negative')
        if v is not None and v > 10000000:  # 10 million PKR sanity check
            raise ValueError('Salary unrealistically high')
        return v
    
    @validator('skills')
    def skills_unique(cls, v):
        """Remove duplicate skills."""
        if v:
            return list(set(skill.lower() for skill in v))
        return []
    
    @validator('experience_level')
    def standardize_experience(cls, v):
        """Standardize experience level values."""
        if not v:
            return None
        
        v_lower = v.lower()
        if any(term in v_lower for term in ['entry', 'junior', 'fresh', '0-2']):
            return 'Entry Level'
        elif any(term in v_lower for term in ['mid', 'intermediate', '3-5']):
            return 'Mid Level'
        elif any(term in v_lower for term in ['senior', 'lead', '6-10']):
            return 'Senior Level'
        elif any(term in v_lower for term in ['executive', 'director', 'c-level', '10+']):
            return 'Executive'
        
        return 'Not Specified'
    
    class Config:
        """Pydantic configuration."""
        validate_assignment = True
        use_enum_values = True
```

***

## **12.3 Data Cleaning Functions**

```python
import re
from bs4 import BeautifulSoup

class DataCleaner:
    """
    Utilities for cleaning scraped data.
    """
    
    @staticmethod
    def strip_html(text):
        """Remove HTML tags from text."""
        if not text:
            return None
        soup = BeautifulSoup(text, 'lxml')
        return soup.get_text(separator=' ', strip=True)
    
    @staticmethod
    def clean_whitespace(text):
        """Normalize whitespace (multiple spaces â†’ single space)."""
        if not text:
            return None
        # Replace multiple whitespace with single space
        return re.sub(r'\s+', ' ', text).strip()
    
    @staticmethod
    def extract_city(location_text):
        """
        Extract city name from location string.
        Examples:
          "Karachi, Sindh, Pakistan" â†’ "Karachi"
          "Lahore - Pakistan" â†’ "Lahore"
        """
        if not location_text:
            return None
        
        # List of Pakistani cities (major ones)
        cities = [
            'karachi', 'lahore', 'islamabad', 'rawalpindi', 'faisalabad',
            'multan', 'peshawar', 'quetta', 'sialkot', 'gujranwala',
            'hyderabad', 'sukkur', 'abbottabad', 'mardan'
        ]
        
        location_lower = location_text.lower()
        for city in cities:
            if city in location_lower:
                return city.capitalize()
        
        # Fallback: return first part before comma
        return location_text.split(',')[0].strip()
    
    @staticmethod
    def parse_salary_range(salary_text):
        """
        Extract min and max salary from various formats.
        
        Examples:
          "PKR 50,000 - 80,000 per month" â†’ (50000, 80000)
          "50K-80K" â†’ (50000, 80000)
          "Salary: 60,000" â†’ (60000, 60000)
          "Market Competitive" â†’ (None, None)
        """
        if not salary_text:
            return None, None
        
        # Remove currency symbols and words
        text = re.sub(r'(PKR|Rs\.?|Rupees|per month|monthly)', '', salary_text, flags=re.IGNORECASE)
        
        # Pattern 1: Range with dash (50,000 - 80,000 or 50K-80K)
        pattern1 = r'(\d{1,3}(?:,\d{3})*|\d+)\s*([Kk])?\s*[-â€“to]\s*(\d{1,3}(?:,\d{3})*|\d+)\s*([Kk])?'
        match = re.search(pattern1, text)
        if match:
            min_val = DataCleaner._parse_number(match.group(1), match.group(2))
            max_val = DataCleaner._parse_number(match.group(3), match.group(4))
            return min_val, max_val
        
        # Pattern 2: Single value (60,000)
        pattern2 = r'(\d{1,3}(?:,\d{3})*|\d+)\s*([Kk])?'
        match = re.search(pattern2, text)
        if match:
            val = DataCleaner._parse_number(match.group(1), match.group(2))
            return val, val
        
        return None, None
    
    @staticmethod
    def _parse_number(num_str, suffix):
        """Convert "50,000" or "50K" to integer."""
        num_str = num_str.replace(',', '')
        num = int(num_str)
        if suffix and suffix.upper() == 'K':
            num *= 1000
        return num
    
    @staticmethod
    def extract_skills(description):
        """
        Extract technical skills from job description using keyword matching.
        
        Returns list of found skills.
        """
        if not description:
            return []
        
        # Comprehensive skill keyword list (300+ skills)
        skill_keywords = {
            # Programming Languages
            'python', 'java', 'javascript', 'typescript', 'c++', 'c#', 'php',
            'ruby', 'go', 'golang', 'rust', 'swift', 'kotlin', 'scala', 'r',
            
            # Web Technologies
            'html', 'css', 'react', 'angular', 'vue', 'vue.js', 'node.js',
            'express', 'django', 'flask', 'fastapi', 'spring boot', 'asp.net',
            'laravel', 'symfony', 'rails', 'next.js', 'nuxt.js', 'svelte',
            
            # Databases
            'sql', 'mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch',
            'cassandra', 'oracle', 'sql server', 'sqlite', 'dynamodb',
            
            # Cloud & DevOps
            'aws', 'azure', 'gcp', 'docker', 'kubernetes', 'jenkins', 'gitlab',
            'github actions', 'terraform', 'ansible', 'ci/cd', 'linux', 'nginx',
            
            # Data Science & ML
            'machine learning', 'deep learning', 'tensorflow', 'pytorch',
            'scikit-learn', 'pandas', 'numpy', 'data science', 'nlp',
            'computer vision', 'ai', 'artificial intelligence',
            
            # Mobile Development
            'android', 'ios', 'react native', 'flutter', 'xamarin', 'swift',
            
            # Business Tools
            'excel', 'power bi', 'tableau', 'sap', 'salesforce', 'jira',
            'confluence', 'agile', 'scrum', 'kanban',
            
            # Other
            'git', 'rest api', 'graphql', 'microservices', 'blockchain',
            'cybersecurity', 'penetration testing', 'seo', 'digital marketing'
        }
        
        desc_lower = description.lower()
        found_skills = []
        
        for skill in skill_keywords:
            # Word boundary matching to avoid false positives
            # e.g., "express" shouldn't match "expressed"
            pattern = r'\b' + re.escape(skill) + r'\b'
            if re.search(pattern, desc_lower):
                found_skills.append(skill.title())
        
        return list(set(found_skills))  # Remove duplicates
    
    @staticmethod
    def detect_remote_mode(text):
        """
        Detect if job is remote, hybrid, or onsite.
        Returns tuple: (is_remote, is_hybrid, is_onsite)
        """
        if not text:
            return False, False, True
        
        text_lower = text.lower()
        
        remote_keywords = ['remote', 'work from home', 'wfh', 'telecommute']
        hybrid_keywords = ['hybrid', 'flexible', 'remote/onsite']
        
        is_remote = any(keyword in text_lower for keyword in remote_keywords)
        is_hybrid = any(keyword in text_lower for keyword in hybrid_keywords)
        is_onsite = not (is_remote or is_hybrid)
        
        return is_remote, is_hybrid, is_onsite
```

***

## **12.4 Deduplication Logic**

### **Exact Match Deduplication** (Fast)[1]

```python
def check_exact_duplicate(job_url):
    """
    Check if job URL already exists in database.
    Uses unique index for O(1) lookup performance.
    """
    query = """
        SELECT id, title, company 
        FROM jobs 
        WHERE source_url = %s
        LIMIT 1
    """
    result = db.execute(query, (job_url,))
    
    if result:
        return True, result[0]['id']  # Is duplicate, original ID
    return False, None
```

### **Fuzzy Match Deduplication** (Cross-Site)

```python
from difflib import SequenceMatcher

def find_fuzzy_duplicates(new_job):
    """
    Find similar jobs across different sites (cross-posting detection).
    
    Uses:
    1. Title similarity (85%+)
    2. Company exact match
    3. Posted within 7 days
    
    Returns ID of original job if duplicate found.
    """
    query = """
        SELECT id, title, company, source_url
        FROM jobs
        WHERE 
            company = %s
            AND posted_date >= CURRENT_DATE - INTERVAL '7 days'
            AND is_active = TRUE
        LIMIT 20
    """
    
    candidates = db.execute(query, (new_job['company'],))
    
    for candidate in candidates:
        similarity = SequenceMatcher(
            None, 
            new_job['title'].lower(), 
            candidate['title'].lower()
        ).ratio()
        
        if similarity >= 0.85:  # 85% similarity threshold
            logger.info(
                f"Fuzzy duplicate found: {new_job['url']} â†’ {candidate['source_url']} "
                f"(similarity: {similarity:.2%})"
            )
            return candidate['id']
    
    return None

def mark_as_duplicate(duplicate_job_id, original_job_id):
    """
    Mark job as duplicate and link to original.
    """
    query = """
        UPDATE jobs
        SET is_duplicate = TRUE,
            duplicate_of = %s
        WHERE id = %s
    """
    db.execute(query, (original_job_id, duplicate_job_id))
```

***

## **12.5 External Database Push**

### **Multi-Database Connector**[2]

```python
from abc import ABC, abstractmethod
import psycopg2
import pymongo
from sqlalchemy import create_engine

class DatabaseConnector(ABC):
    """Abstract base for external database connectors."""
    
    @abstractmethod
    def connect(self):
        """Establish database connection."""
        pass
    
    @abstractmethod
    def push_jobs(self, jobs):
        """Push job records to external database."""
        pass
    
    @abstractmethod
    def test_connection(self):
        """Test if connection is valid (non-destructive)."""
        pass


class PostgreSQLConnector(DatabaseConnector):
    """Connector for external PostgreSQL databases."""
    
    def __init__(self, connection_string):
        self.connection_string = connection_string
        self.conn = None
    
    def connect(self):
        """Connect to PostgreSQL."""
        self.conn = psycopg2.connect(self.connection_string)
        return self.conn
    
    def test_connection(self):
        """Test connection with simple query."""
        try:
            conn = self.connect()
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            conn.close()
            return result[0] == 1
        except Exception as e:
            logger.error(f"PostgreSQL connection test failed: {e}")
            return False
    
    def push_jobs(self, jobs):
        """
        Bulk insert jobs into external PostgreSQL database.
        Uses COPY for efficiency (10x faster than INSERT).
        """
        if not self.conn:
            self.connect()
        
        cursor = self.conn.cursor()
        
        # Create table if not exists
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS jobs (
                id UUID PRIMARY KEY,
                title VARCHAR(255),
                company VARCHAR(255),
                location VARCHAR(255),
                salary_min INTEGER,
                salary_max INTEGER,
                description TEXT,
                skills JSONB,
                source_url TEXT UNIQUE,
                posted_date DATE,
                scraped_at TIMESTAMP
            )
        """)
        
        # Bulk insert using COPY (fastest method)
        from io import StringIO
        
        buffer = StringIO()
        for job in jobs:
            buffer.write('\t'.join([
                str(job.get('id', '')),
                job.get('title', ''),
                job.get('company', ''),
                job.get('location', ''),
                str(job.get('salary_min', '') or ''),
                str(job.get('salary_max', '') or ''),
                job.get('description', ''),
                json.dumps(job.get('skills', [])),
                job.get('source_url', ''),
                str(job.get('posted_date', '')),
                str(job.get('scraped_at', ''))
            ]) + '\n')
        
        buffer.seek(0)
        cursor.copy_from(
            buffer, 
            'jobs',
            columns=['id', 'title', 'company', 'location', 'salary_min', 
                    'salary_max', 'description', 'skills', 'source_url', 
                    'posted_date', 'scraped_at']
        )
        
        self.conn.commit()
        logger.info(f"Pushed {len(jobs)} jobs to external PostgreSQL")


class MongoDBConnector(DatabaseConnector):
    """Connector for external MongoDB databases."""
    
    def __init__(self, connection_string, database_name='pakjobs'):
        self.connection_string = connection_string
        self.database_name = database_name
        self.client = None
        self.db = None
    
    def connect(self):
        """Connect to MongoDB."""
        self.client = pymongo.MongoClient(self.connection_string)
        self.db = self.client[self.database_name]
        return self.db
    
    def test_connection(self):
        """Test MongoDB connection."""
        try:
            self.connect()
            # Ping database
            self.client.admin.command('ping')
            return True
        except Exception as e:
            logger.error(f"MongoDB connection test failed: {e}")
            return False
    
    def push_jobs(self, jobs):
        """Bulk insert jobs into MongoDB collection."""
        if not self.db:
            self.connect()
        
        collection = self.db['jobs']
        
        # Convert to MongoDB documents
        documents = []
        for job in jobs:
            doc = {
                '_id': job['id'],  # Use our UUID as MongoDB _id
                'title': job.get('title'),
                'company': job.get('company'),
                'location': job.get('location'),
                'salary': {
                    'min': job.get('salary_min'),
                    'max': job.get('salary_max'),
                    'currency': 'PKR'
                },
                'description': job.get('description'),
                'skills': job.get('skills', []),
                'source_url': job.get('source_url'),
                'posted_date': job.get('posted_date'),
                'scraped_at': job.get('scraped_at')
            }
            documents.append(doc)
        
        # Bulk insert with upsert (update if exists)
        operations = [
            pymongo.UpdateOne(
                {'_id': doc['_id']},
                {'$set': doc},
                upsert=True
            )
            for doc in documents
        ]
        
        result = collection.bulk_write(operations)
        logger.info(
            f"MongoDB push: {result.inserted_count} inserted, "
            f"{result.modified_count} updated"
        )


class MySQLConnector(DatabaseConnector):
    """Connector for external MySQL databases."""
    
    def __init__(self, connection_string):
        self.engine = create_engine(connection_string)
    
    def connect(self):
        return self.engine.connect()
    
    def test_connection(self):
        try:
            conn = self.connect()
            conn.execute("SELECT 1")
            conn.close()
            return True
        except:
            return False
    
    def push_jobs(self, jobs):
        """Push jobs to MySQL using SQLAlchemy."""
        import pandas as pd
        
        # Convert to DataFrame for easy insertion
        df = pd.DataFrame(jobs)
        
        # Push to MySQL (replace if exists)
        df.to_sql(
            'jobs', 
            self.engine, 
            if_exists='append',  # Or 'replace' for full refresh
            index=False
        )
        
        logger.info(f"Pushed {len(jobs)} jobs to external MySQL")


# Factory function
def get_database_connector(db_type, connection_string):
    """
    Factory to create appropriate database connector.
    
    Args:
        db_type: 'postgresql', 'mysql', 'mongodb', 'sqlite'
        connection_string: Database connection URL
    
    Returns:
        DatabaseConnector instance
    """
    connectors = {
        'postgresql': PostgreSQLConnector,
        'mysql': MySQLConnector,
        'mongodb': MongoDBConnector
    }
    
    if db_type not in connectors:
        raise ValueError(f"Unsupported database type: {db_type}")
    
    return connectors[db_type](connection_string)
```

***

## **12.6 CSV/JSON Export**[3]

```python
import pandas as pd
import json
from datetime import datetime

class DataExporter:
    """
    Export job data to various formats.
    """
    
    @staticmethod
    def export_to_csv(jobs, filename=None):
        """
        Export jobs to CSV file.
        
        Args:
            jobs: List of job dictionaries
            filename: Output filename (auto-generated if None)
        
        Returns:
            Path to generated CSV file
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'jobs_export_{timestamp}.csv'
        
        # Convert to DataFrame
        df = pd.DataFrame(jobs)
        
        # Flatten JSONB columns for CSV
        if 'skills' in df.columns:
            df['skills'] = df['skills'].apply(
                lambda x: ', '.join(x) if isinstance(x, list) else ''
            )
        
        # Reorder columns (most important first)
        column_order = [
            'title', 'company', 'location', 'salary_min', 'salary_max',
            'experience_level', 'job_type', 'skills', 'posted_date',
            'source_url', 'site_source', 'description'
        ]
        
        # Keep only existing columns
        column_order = [col for col in column_order if col in df.columns]
        df = df[column_order]
        
        # Export
        df.to_csv(filename, index=False, encoding='utf-8')
        
        logger.info(f"Exported {len(jobs)} jobs to {filename}")
        return filename
    
    @staticmethod
    def export_to_json(jobs, filename=None, pretty=True):
        """
        Export jobs to JSON file.
        
        Args:
            jobs: List of job dictionaries
            filename: Output filename
            pretty: Pretty-print JSON (indented)
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'jobs_export_{timestamp}.json'
        
        # Convert datetime objects to strings
        def json_serial(obj):
            if isinstance(obj, (datetime, date)):
                return obj.isoformat()
            raise TypeError(f"Type {type(obj)} not serializable")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(
                jobs, 
                f, 
                indent=2 if pretty else None,
                default=json_serial,
                ensure_ascii=False  # Support Urdu characters
            )
        
        logger.info(f"Exported {len(jobs)} jobs to {filename}")
        return filename
    
    @staticmethod
    def export_to_excel(jobs, filename=None):
        """
        Export jobs to Excel file with formatting.
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'jobs_export_{timestamp}.xlsx'
        
        df = pd.DataFrame(jobs)
        
        # Create Excel writer
        with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
            df.to_excel(writer, sheet_name='Jobs', index=False)
            
            # Get workbook and worksheet
            workbook = writer.book
            worksheet = writer.sheets['Jobs']
            
            # Format header row
            header_format = workbook.add_format({
                'bold': True,
                'bg_color': '#4CAF50',
                'font_color': 'white',
                'border': 1
            })
            
            for col_num, value in enumerate(df.columns.values):
                worksheet.write(0, col_num, value, header_format)
            
            # Auto-adjust column widths
            for i, col in enumerate(df.columns):
                max_len = max(
                    df[col].astype(str).str.len().max(),
                    len(col)
                )
                worksheet.set_column(i, i, min(max_len + 2, 50))
        
        logger.info(f"Exported {len(jobs)} jobs to {filename}")
        return filename
```

***

# **13. WEB DASHBOARD SPECIFICATIONS**

## **13.1 Dashboard Features Overview**

### **Core Pages**

| Page | URL Route | Purpose | Auth Required |
|------|-----------|---------|---------------|
| **Home** | `/` | Overview stats, recent jobs | No (Phase 1) |
| **Scraper Status** | `/scrapers` | Monitor scraper health | No |
| **Job Search** | `/jobs` | Search/filter jobs | No |
| **Analytics** | `/analytics` | Insights dashboard | No |
| **Settings** | `/settings` | Configure system | No |
| **API Docs** | `/api/docs` | Swagger documentation | No |

---

## **13.2 Home Page Wireframe (ASCII)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PakJobs Aggregator                    [Search] ðŸ”  [Settings] âš™ï¸ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Total Jobs  â”‚  â”‚  New Today   â”‚  â”‚  Active Sitesâ”‚          â”‚
â”‚  â”‚   52,847     â”‚  â”‚    1,243     â”‚  â”‚     9/9      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                   â”‚
â”‚  Last Scrape: Today at 3:15 AM PKT  |  Next: Tomorrow 3:00 AM   â”‚
â”‚                                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ“Š SCRAPER STATUS                                                â”‚
â”‚                                                                   â”‚
â”‚  Site          | Status  | Jobs Found | Duration | Last Run      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  Rozee.pk      | âœ… OK   |   423      | 2m 15s   | 3:05 AM      â”‚
â”‚  Mustakbil     | âœ… OK   |   287      | 1m 45s   | 3:08 AM      â”‚
â”‚  Indeed.pk     | âœ… OK   |   356      | 3m 20s   | 3:11 AM      â”‚
â”‚  BrightSpyre   | âš ï¸ Warn |   12       | 45s      | 3:15 AM      â”‚
â”‚  Bayt.com      | âœ… OK   |   198      | 1m 30s   | 3:17 AM      â”‚
â”‚  Jobz.pk       | âœ… OK   |   145      | 1m 10s   | 3:18 AM      â”‚
â”‚  Bayrozgar     | âœ… OK   |   89       | 55s      | 3:20 AM      â”‚
â”‚  JobsAlert     | âœ… OK   |   134      | 1m 05s   | 3:21 AM      â”‚
â”‚  PakPositions  | âœ… OK   |   67       | 50s      | 3:22 AM      â”‚
â”‚                                                                   â”‚
â”‚  [â–¶ï¸ Run All Scrapers Now]  [ðŸ“¥ Export All Jobs]                 â”‚
â”‚                                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ”¥ TOP JOBS TODAY                                                â”‚
â”‚                                                                   â”‚
â”‚  Senior Software Engineer - TechCorp (Karachi)                   â”‚
â”‚  PKR 150,000-200,000 | Posted 2 hours ago | Rozee.pk            â”‚
â”‚  Skills: Python, Django, AWS                       [View â†’]      â”‚
â”‚                                                                   â”‚
â”‚  Product Manager - StartupXYZ (Remote)                           â”‚
â”‚  PKR 180,000-250,000 | Posted 4 hours ago | Indeed.pk           â”‚
â”‚  Skills: Agile, Product Strategy                   [View â†’]      â”‚
â”‚                                                                   â”‚
â”‚  [See All 1,243 New Jobs â†’]                                      â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **13.3 Job Search Page**

### **Search Interface Features**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸ” JOB SEARCH                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Search: [python developer________________] ðŸ” [Search]    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€ FILTERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  ðŸ“ Location                                                â”‚ â”‚
â”‚  â”‚  â˜ Karachi (12,543)  â˜ Lahore (8,921)  â˜ Islamabad (6,234)â”‚ â”‚
â”‚  â”‚  â˜ Remote (2,109)    â˜ Other Cities                        â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  ðŸ’° Salary Range                                            â”‚ â”‚
â”‚  â”‚  [Min: 50,000] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [Max: 200,000] PKR            â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  ðŸŽ“ Experience Level                                        â”‚ â”‚
â”‚  â”‚  â˜ Entry (0-2 yrs)  â˜ Mid (3-5 yrs)  â˜ Senior (6-10 yrs) â”‚ â”‚
â”‚  â”‚  â˜ Executive (10+ yrs)                                     â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  ðŸ“… Posted Date                                             â”‚ â”‚
â”‚  â”‚  â— Last 24 hours  â—‹ Last 7 days  â—‹ Last 30 days  â—‹ All   â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  ðŸ¢ Job Type                                                â”‚ â”‚
â”‚  â”‚  â˜ Full-time  â˜ Part-time  â˜ Contract  â˜ Internship      â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  ðŸŒ Source Site                                             â”‚ â”‚
â”‚  â”‚  â˜‘ï¸ All Sites  â˜ Rozee  â˜ Indeed  â˜ Mustakbil            â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  [Clear Filters]  [Apply Filters]                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â”‚  Showing 342 results | Sort by: [Most Recent â–¼]                  â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Python Developer                               ðŸ’š Rozee.pk   â”‚â”‚
â”‚  â”‚ ABC Technologies - Karachi                                   â”‚â”‚
â”‚  â”‚ PKR 80,000-120,000 | Full-time | Mid Level                  â”‚â”‚
â”‚  â”‚ Skills: Python, Django, PostgreSQL, Docker                   â”‚â”‚
â”‚  â”‚ Posted: 2 days ago                            [View Details] â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Senior Python Engineer                         ðŸ”µ Indeed.pk  â”‚â”‚
â”‚  â”‚ XYZ Corp - Remote                                            â”‚â”‚
â”‚  â”‚ PKR 150,000-200,000 | Full-time | Senior Level              â”‚â”‚
â”‚  â”‚ Skills: Python, FastAPI, AWS, Kubernetes                     â”‚â”‚
â”‚  â”‚ Posted: 1 day ago                             [View Details] â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                   â”‚
â”‚  [Previous] [1] [2] [3] ... [12] [Next]                          â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **13.4 Flask Application Structure**

```python
# app.py - Main Flask application

from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
import psycopg2
from datetime import datetime

app = Flask(__name__)
CORS(app)  # Enable CORS for API endpoints

# Database connection
def get_db():
    return psycopg2.connect(os.environ['DATABASE_URL'])

@app.route('/')
def home():
    """Home page with overview statistics."""
    db = get_db()
    cursor = db.cursor()
    
    # Get statistics
    cursor.execute("SELECT COUNT(*) FROM jobs WHERE is_active = TRUE")
    total_jobs = cursor.fetchone()[0]
    
    cursor.execute("""
        SELECT COUNT(*) FROM jobs 
        WHERE scraped_at >= CURRENT_DATE
    """)
    jobs_today = cursor.fetchone()[0]
    
    cursor.execute("""
        SELECT COUNT(DISTINCT site_source) FROM scrape_logs
        WHERE start_time >= CURRENT_DATE AND status = 'success'
    """)
    active_sites = cursor.fetchone()[0]
    
    # Get recent scraper runs
    cursor.execute("""
        SELECT 
            site_name, status, jobs_found, 
            duration_seconds, start_time
        FROM scrape_logs
        WHERE start_time >= CURRENT_DATE
        ORDER BY start_time DESC
        LIMIT 9
    """)
    scraper_status = cursor.fetchall()
    
    # Get top jobs today
    cursor.execute("""
        SELECT title, company, location, salary_min, salary_max,
               skills, source_url, site_source
        FROM jobs
        WHERE scraped_at >= CURRENT_DATE
        ORDER BY scraped_at DESC
        LIMIT 10
    """)
    top_jobs = cursor.fetchall()
    
    db.close()
    
    return render_template('home.html',
        total_jobs=total_jobs,
        jobs_today=jobs_today,
        active_sites=active_sites,
        scraper_status=scraper_status,
        top_jobs=top_jobs
    )

@app.route('/jobs')
def job_search():
    """Job search page with filters."""
    # Get query parameters
    query = request.args.get('q', '')
    location = request.args.get('location', '')
    salary_min = request.args.get('salary_min', type=int)
    salary_max = request.args.get('salary_max', type=int)
    experience = request.args.get('experience', '')
    days = request.args.get('days', 30, type=int)
    page = request.args.get('page', 1, type=int)
    
    per_page = 20
    offset = (page - 1) * per_page
    
    # Build dynamic SQL query
    db = get_db()
    cursor = db.cursor()
    
    sql = """
        SELECT id, title, company, location, salary_min, salary_max,
               experience_level, job_type, skills, posted_date, 
               source_url, site_source
        FROM jobs
        WHERE is_active = TRUE
    """
    
    params = []
    
    if query:
        sql += " AND to_tsvector('english', title || ' ' || description) @@ plainto_tsquery(%s)"
        params.append(query)
    
    if location:
        sql += " AND location ILIKE %s"
        params.append(f'%{location}%')
    
    if salary_min:
        sql += " AND salary_min >= %s"
        params.append(salary_min)
    
    if salary_max:
        sql += " AND salary_max <= %s"
        params.append(salary_max)
    
    if experience:
        sql += " AND experience_level = %s"
        params.append(experience)
    
    if days:
        sql += " AND posted_date >= CURRENT_DATE - INTERVAL '%s days'"
        params.append(days)
    
    sql += " ORDER BY posted_date DESC LIMIT %s OFFSET %s"
    params.extend([per_page, offset])
    
    cursor.execute(sql, params)
    jobs = cursor.fetchall()
    
    # Get total count for pagination
    count_sql = sql.replace('SELECT id, title...', 'SELECT COUNT(*)')
    count_sql = count_sql.split('ORDER BY')[0]  # Remove ORDER/LIMIT
    cursor.execute(count_sql, params[:-2])  # Exclude LIMIT/OFFSET
    total = cursor.fetchone()[0]
    
    db.close()
    
    total_pages = (total + per_page - 1) // per_page
    
    return render_template('jobs.html',
        jobs=jobs,
        page=page,
        total_pages=total_pages,
        total=total,
        query=query
    )

@app.route('/scrapers')
def scrapers():
    """Scraper management page."""
    db = get_db()
    cursor = db.cursor()
    
    # Get latest status for each site
    cursor.execute("""
        SELECT DISTINCT ON (site_name)
            site_name, status, jobs_found, duration_seconds,
            start_time, error_message
        FROM scrape_logs
        ORDER BY site_name, start_time DESC
    """)
    scraper_status = cursor.fetchall()
    
    db.close()
    
    return render_template('scrapers.html', scrapers=scraper_status)

@app.route('/analytics')
def analytics():
    """Analytics dashboard."""
    db = get_db()
    cursor = db.cursor()
    
    # Top skills
    cursor.execute("""
        SELECT skill, COUNT(*) as count
        FROM jobs, jsonb_array_elements_text(skills) skill
        WHERE posted_date >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY skill
        ORDER BY count DESC
        LIMIT 20
    """)
    top_skills = cursor.fetchall()
    
    # Salary trends by experience
    cursor.execute("""
        SELECT 
            experience_level,
            AVG(salary_min) as avg_min,
            AVG(salary_max) as avg_max,
            COUNT(*) as job_count
        FROM jobs
        WHERE salary_min IS NOT NULL
        AND posted_date >= CURRENT_DATE - INTERVAL '90 days'
        GROUP BY experience_level
        ORDER BY avg_min DESC
    """)
    salary_trends = cursor.fetchall()
    
    # Jobs by city
    cursor.execute("""
        SELECT city, COUNT(*) as count
        FROM jobs
        WHERE city IS NOT NULL
        AND posted_date >= CURRENT_DATE - INTERVAL '30 days'
        GROUP BY city
        ORDER BY count DESC
        LIMIT 10
    """)
    jobs_by_city = cursor.fetchall()
    
    db.close()
    
    return render_template('analytics.html',
        top_skills=top_skills,
        salary_trends=salary_trends,
        jobs_by_city=jobs_by_city
    )

@app.route('/api/trigger-scrape/<site>')
def trigger_scrape(site):
    """Manually trigger scraper for a specific site (AJAX endpoint)."""
    # Import scraping orchestrator
    from scraping.orchestrator import ScrapingOrchestrator
    
    try:
        orchestrator = ScrapingOrchestrator()
        result = orchestrator.run_single_scraper(site)
        
        return jsonify({
            'success': True,
            'site': site,
            'jobs_found': result['jobs_found'],
            'duration': result['duration']
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/export')
def export_jobs():
    """Export jobs to CSV (download)."""
    format = request.args.get('format', 'csv')
    
    # Get jobs from database
    db = get_db()
    cursor = db.cursor()
    cursor.execute("SELECT * FROM jobs WHERE is_active = TRUE LIMIT 10000")
    jobs = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    db.close()
    
    # Convert to list of dicts
    jobs_dicts = [dict(zip(columns, job)) for job in jobs]
    
    # Export
    from data_pipeline import DataExporter
    
    if format == 'csv':
        filename = DataExporter.export_to_csv(jobs_dicts)
    elif format == 'json':
        filename = DataExporter.export_to_json(jobs_dicts)
    elif format == 'excel':
        filename = DataExporter.export_to_excel(jobs_dicts)
    
    return send_file(filename, as_attachment=True)

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
```

***

# **PRODUCT REQUIREMENTS DOCUMENT (PRD) - PART 4**

---

# **14. API SPECIFICATIONS**

## **14.1 REST API Overview**

### **API Design Principles**[1]

| Principle | Implementation |
|-----------|----------------|
| **RESTful** | HTTP methods (GET, POST, PUT, DELETE) |
| **Versioned** | `/api/v1/` prefix for backward compatibility |
| **JSON** | All responses in JSON format |
| **Stateless** | No session management (Phase 1) |
| **Paginated** | Large datasets use cursor/offset pagination |
| **Rate Limited** | 1000 requests/hour per IP |
| **CORS Enabled** | Allow cross-origin requests |
| **Documented** | OpenAPI/Swagger documentation |

***

## **14.2 API Endpoints Specification**

### **Endpoint 1: List Jobs**

```
GET /api/v1/jobs
```

**Description**: Retrieve paginated list of job postings with filtering.

**Query Parameters**:
```
?q=python developer        # Full-text search query
&location=karachi          # Location filter
&salary_min=50000          # Minimum salary
&salary_max=200000         # Maximum salary
&experience=Mid Level      # Experience level
&job_type=Full-time        # Employment type
&site=rozee                # Source site filter
&days=7                    # Posted within N days
&page=1                    # Page number (default: 1)
&per_page=50               # Results per page (max: 100, default: 20)
&sort=posted_date          # Sort field (posted_date, salary_min)
&order=desc                # Sort order (asc, desc)
```

**Response** (200 OK):
```json
{
  "success": true,
  "data": [
    {
      "id": "550e8400-e29b-41d4-a716-446655440000",
      "title": "Senior Python Developer",
      "company": "TechCorp Pakistan",
      "location": "Karachi, Sindh",
      "city": "Karachi",
      "salary": {
        "min": 150000,
        "max": 200000,
        "currency": "PKR",
        "text": "PKR 150,000-200,000 per month"
      },
      "description": "We are seeking an experienced Python developer...",
      "skills": ["Python", "Django", "PostgreSQL", "AWS"],
      "experience_level": "Senior Level",
      "job_type": "Full-time",
      "is_remote": false,
      "is_hybrid": false,
      "posted_date": "2025-10-12",
      "deadline_date": "2025-11-12",
      "source_url": "https://www.rozee.pk/job/12345",
      "site_source": "rozee",
      "application_method": "url",
      "application_url": "https://www.rozee.pk/apply/12345",
      "scraped_at": "2025-10-14T03:15:00Z"
    }
  ],
  "pagination": {
    "page": 1,
    "per_page": 20,
    "total": 342,
    "total_pages": 18,
    "has_next": true,
    "has_prev": false
  },
  "meta": {
    "query_time_ms": 45,
    "timestamp": "2025-10-14T07:18:00Z"
  }
}
```

**Error Response** (400 Bad Request):
```json
{
  "success": false,
  "error": {
    "code": "INVALID_PARAMETER",
    "message": "per_page must be between 1 and 100",
    "field": "per_page"
  }
}
```

***

### **Endpoint 2: Get Single Job**

```
GET /api/v1/jobs/{job_id}
```

**Description**: Retrieve detailed information for a specific job.

**Path Parameters**:
- `job_id` (UUID): Unique job identifier

**Response** (200 OK):
```json
{
  "success": true,
  "data": {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "title": "Senior Python Developer",
    "company": "TechCorp Pakistan",
    "location": "Karachi, Sindh",
    "city": "Karachi",
    "salary": {
      "min": 150000,
      "max": 200000,
      "currency": "PKR"
    },
    "description": "Full job description text...",
    "requirements": "Bachelor's degree in CS, 5+ years experience...",
    "benefits": "Health insurance, flexible hours, remote options...",
    "skills": ["Python", "Django", "PostgreSQL", "AWS", "Docker"],
    "experience_level": "Senior Level",
    "experience_years": "5-7 years",
    "education": "Bachelor's in Computer Science",
    "job_type": "Full-time",
    "is_remote": false,
    "is_hybrid": true,
    "posted_date": "2025-10-12",
    "deadline_date": "2025-11-12",
    "source_url": "https://www.rozee.pk/job/12345",
    "site_source": "rozee",
    "application_method": "url",
    "application_url": "https://www.rozee.pk/apply/12345",
    "scraped_at": "2025-10-14T03:15:00Z",
    "last_updated": "2025-10-14T03:15:00Z",
    "is_active": true
  }
}
```

**Error Response** (404 Not Found):
```json
{
  "success": false,
  "error": {
    "code": "JOB_NOT_FOUND",
    "message": "Job with ID 550e8400-e29b-41d4-a716-446655440000 not found"
  }
}
```

***

### **Endpoint 3: Search Jobs (Full-Text)**

```
GET /api/v1/jobs/search
```

**Description**: Full-text search across job titles and descriptions.[2]

**Query Parameters**:
```
?q=machine learning engineer   # Search query (required)
&location=lahore               # Optional location filter
&page=1
&per_page=20
```

**Response** (200 OK):
```json
{
  "success": true,
  "data": [
    {
      "id": "...",
      "title": "Machine Learning Engineer",
      "company": "AI Startup",
      "relevance_score": 0.95,
      "matched_terms": ["machine", "learning", "engineer"],
      "...": "... (full job fields)"
    }
  ],
  "pagination": {...}
}
```

***

### **Endpoint 4: Get Statistics**

```
GET /api/v1/stats
```

**Description**: Summary statistics and metrics.

**Response** (200 OK):
```json
{
  "success": true,
  "data": {
    "total_jobs": 52847,
    "active_jobs": 48320,
    "jobs_today": 1243,
    "jobs_this_week": 8567,
    "jobs_this_month": 34821,
    "sites_scraped": 9,
    "last_scrape_time": "2025-10-14T03:22:00Z",
    "next_scrape_time": "2025-10-15T03:00:00Z",
    "database_size_mb": 342,
    "top_cities": [
      {"city": "Karachi", "count": 12543},
      {"city": "Lahore", "count": 8921},
      {"city": "Islamabad", "count": 6234}
    ],
    "top_companies": [
      {"company": "Systems Limited", "count": 87},
      {"company": "TRG Pakistan", "count": 65},
      {"company": "Netsol Technologies", "count": 54}
    ]
  }
}
```

***

### **Endpoint 5: Get Site Status**

```
GET /api/v1/scrapers
```

**Description**: Health status of all scrapers.

**Response** (200 OK):
```json
{
  "success": true,
  "data": {
    "scrapers": [
      {
        "site_name": "rozee",
        "status": "success",
        "last_run": "2025-10-14T03:05:00Z",
        "jobs_found": 423,
        "jobs_new": 387,
        "duration_seconds": 135,
        "error_count": 0,
        "uptime_24h": 100.0,
        "next_run": "2025-10-15T03:00:00Z"
      },
      {
        "site_name": "indeed",
        "status": "success",
        "last_run": "2025-10-14T03:11:00Z",
        "jobs_found": 356,
        "duration_seconds": 200,
        "uptime_24h": 100.0
      }
    ],
    "overall_health": "healthy",
    "total_scrapers": 9,
    "active_scrapers": 9,
    "failed_scrapers": 0
  }
}
```

***

### **Endpoint 6: Get Analytics**

```
GET /api/v1/analytics/skills
```

**Description**: Top skills in demand (last 30 days).

**Response** (200 OK):
```json
{
  "success": true,
  "data": {
    "skills": [
      {"skill": "Python", "count": 1247, "growth_pct": 12.5},
      {"skill": "JavaScript", "count": 983, "growth_pct": 8.3},
      {"skill": "SQL", "count": 876, "growth_pct": -2.1},
      {"skill": "React", "count": 654, "growth_pct": 15.7},
      {"skill": "AWS", "count": 543, "growth_pct": 22.4}
    ],
    "period": "last_30_days",
    "total_jobs_analyzed": 8567
  }
}
```

```
GET /api/v1/analytics/salaries
```

**Description**: Salary trends by role and experience.

**Response** (200 OK):
```json
{
  "success": true,
  "data": {
    "by_experience": [
      {
        "level": "Entry Level",
        "avg_min": 45000,
        "avg_max": 65000,
        "median": 50000,
        "job_count": 1234
      },
      {
        "level": "Mid Level",
        "avg_min": 75000,
        "avg_max": 110000,
        "median": 85000,
        "job_count": 2456
      },
      {
        "level": "Senior Level",
        "avg_min": 130000,
        "avg_max": 180000,
        "median": 150000,
        "job_count": 987
      }
    ],
    "currency": "PKR",
    "period": "last_90_days"
  }
}
```

***

### **Endpoint 7: Trigger Scraper (Admin)**

```
POST /api/v1/scrapers/{site_name}/trigger
```

**Description**: Manually trigger scraper for specific site.

**Path Parameters**:
- `site_name`: rozee, mustakbil, indeed, etc.

**Request Body**:
```json
{
  "mode": "incremental"  # or "full_refresh"
}
```

**Response** (202 Accepted):
```json
{
  "success": true,
  "message": "Scraper for rozee triggered successfully",
  "job_id": "scrape_task_12345",
  "estimated_duration": "2-3 minutes",
  "status_url": "/api/v1/scrapers/rozee/status"
}
```

***

## **14.3 Rate Limiting**

### **Implementation**

```python
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["1000 per hour", "50 per minute"],
    storage_uri="redis://localhost:6379"  # Or in-memory for free tier
)

@app.route('/api/v1/jobs')
@limiter.limit("100 per minute")
def api_jobs():
    # ... implementation
    pass
```

**Rate Limit Headers** (included in responses):
```
X-RateLimit-Limit: 1000
X-RateLimit-Remaining: 847
X-RateLimit-Reset: 1697268000
```

**Rate Limit Exceeded Response** (429 Too Many Requests):
```json
{
  "success": false,
  "error": {
    "code": "RATE_LIMIT_EXCEEDED",
    "message": "API rate limit exceeded. Try again in 15 minutes.",
    "retry_after": 900
  }
}
```

***

## **14.4 API Documentation (Swagger)**

### **OpenAPI Specification**

```yaml
openapi: 3.0.0
info:
  title: PakJobs Aggregator API
  version: 1.0.0
  description: |
    REST API for accessing aggregated job listings from 9 Pakistani job portals.
    
    ## Features
    - Search and filter 50,000+ job listings
    - Full-text search across titles and descriptions
    - Analytics on salaries, skills, and trends
    - Real-time scraper status
    
    ## Rate Limits
    - 1000 requests per hour per IP
    - 100 requests per minute per endpoint
    
  contact:
    email: api@pakjobs.example.com
  license:
    name: MIT

servers:
  - url: https://pakjobs-api.onrender.com/api/v1
    description: Production server

paths:
  /jobs:
    get:
      summary: List jobs
      description: Retrieve paginated list of job postings with filters
      parameters:
        - name: q
          in: query
          description: Full-text search query
          schema:
            type: string
            example: python developer
        - name: location
          in: query
          description: Location filter
          schema:
            type: string
            example: karachi
        - name: salary_min
          in: query
          description: Minimum salary (PKR)
          schema:
            type: integer
            example: 50000
        - name: page
          in: query
          description: Page number
          schema:
            type: integer
            default: 1
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobListResponse'
        '400':
          description: Invalid parameters
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'

components:
  schemas:
    Job:
      type: object
      properties:
        id:
          type: string
          format: uuid
        title:
          type: string
        company:
          type: string
        location:
          type: string
        salary:
          type: object
          properties:
            min:
              type: integer
            max:
              type: integer
            currency:
              type: string
        skills:
          type: array
          items:
            type: string
        posted_date:
          type: string
          format: date
        source_url:
          type: string
          format: uri
    
    JobListResponse:
      type: object
      properties:
        success:
          type: boolean
        data:
          type: array
          items:
            $ref: '#/components/schemas/Job'
        pagination:
          type: object
          properties:
            page:
              type: integer
            total:
              type: integer
            total_pages:
              type: integer
```

**Access Swagger UI**: `https://pakjobs.onrender.com/api/docs`

***

# **15. SECURITY & PRIVACY**

## **15.1 Security Architecture**

### **Security Layers**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: TRANSPORT SECURITY                             â”‚
â”‚  â€¢ HTTPS only (TLS 1.3)                                  â”‚
â”‚  â€¢ SSL certificates (Render auto-managed)                â”‚
â”‚  â€¢ Secure headers (HSTS, CSP)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: APPLICATION SECURITY                           â”‚
â”‚  â€¢ Input validation (Pydantic schemas)                   â”‚
â”‚  â€¢ SQL injection prevention (parameterized queries)      â”‚
â”‚  â€¢ XSS protection (escape user inputs)                   â”‚
â”‚  â€¢ CSRF tokens (Flask-WTF)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: DATA SECURITY                                  â”‚
â”‚  â€¢ Encrypted credentials (Fernet)                        â”‚
â”‚  â€¢ Environment variables (no hardcoded secrets)          â”‚
â”‚  â€¢ Database encryption at rest (Render default)          â”‚
â”‚  â€¢ Secure password hashing (Argon2) [Phase 2]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 4: ACCESS CONTROL                                 â”‚
â”‚  â€¢ Rate limiting (prevent abuse)                         â”‚
â”‚  â€¢ API key authentication [Phase 2]                      â”‚
â”‚  â€¢ Role-based access control [Phase 2]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **15.2 Secure Configuration**

### **Environment Variables** (.env file)

```bash
# Database
DATABASE_URL=postgresql://user:password@host:5432/dbname

# Email (Gmail SMTP)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your-email@gmail.com
SMTP_PASSWORD=your-app-password  # Use App Password, not account password
SMTP_FROM_EMAIL=noreply@pakjobs.example.com

# External Database (Optional)
EXTERNAL_DB_ENABLED=false
EXTERNAL_DB_TYPE=postgresql
EXTERNAL_DB_CONNECTION=  # Encrypted in database

# Security
SECRET_KEY=your-random-secret-key-here-min-32-chars
ENCRYPTION_KEY=your-fernet-encryption-key-here

# API
API_RATE_LIMIT=1000
API_RATE_LIMIT_PERIOD=hour

# Scraping
USER_AGENT=Mozilla/5.0 (compatible; PakJobsBot/1.0; +https://pakjobs.example.com/bot)
SCRAPE_DELAY_SECONDS=3
MAX_SCRAPE_PAGES=50

# Monitoring
ENABLE_DEBUG=false  # Never true in production
LOG_LEVEL=INFO
```

**Security Notes**:
- Never commit `.env` to Git (add to `.gitignore`)
- Use Render environment variables for production
- Rotate credentials every 90 days

***

## **15.3 Credential Encryption**

### **Fernet Encryption for External DB Credentials**

```python
from cryptography.fernet import Fernet
import os

class CredentialManager:
    """
    Encrypt and decrypt sensitive credentials for storage.
    """
    
    def __init__(self):
        # Load encryption key from environment
        key = os.getenv('ENCRYPTION_KEY')
        if not key:
            # Generate new key if not exists
            key = Fernet.generate_key()
            print(f"Generated new encryption key: {key.decode()}")
            print("Add this to your .env file: ENCRYPTION_KEY={key}")
        
        self.cipher = Fernet(key.encode() if isinstance(key, str) else key)
    
    def encrypt(self, plaintext):
        """
        Encrypt plaintext string.
        
        Args:
            plaintext: String to encrypt
        
        Returns:
            Encrypted string (base64 encoded)
        """
        if not plaintext:
            return None
        
        encrypted = self.cipher.encrypt(plaintext.encode())
        return encrypted.decode()
    
    def decrypt(self, ciphertext):
        """
        Decrypt ciphertext string.
        
        Args:
            ciphertext: Encrypted string
        
        Returns:
            Decrypted plaintext
        """
        if not ciphertext:
            return None
        
        decrypted = self.cipher.decrypt(ciphertext.encode())
        return decrypted.decode()

# Usage example
cred_manager = CredentialManager()

# Encrypt database connection string before saving
connection_string = "postgresql://user:pass@host:5432/db"
encrypted = cred_manager.encrypt(connection_string)

# Store encrypted value in database
db.execute(
    "INSERT INTO user_config (config_key, config_value, is_encrypted) VALUES (%s, %s, %s)",
    ('external_db_connection', encrypted, True)
)

# Retrieve and decrypt when needed
result = db.execute("SELECT config_value FROM user_config WHERE config_key = 'external_db_connection'")
encrypted_value = result[0][0]
decrypted = cred_manager.decrypt(encrypted_value)
```

***

## **15.4 SQL Injection Prevention**[2]

### **Always Use Parameterized Queries**

**âŒ WRONG (Vulnerable to SQL Injection)**:
```python
# NEVER DO THIS
query = f"SELECT * FROM jobs WHERE title = '{user_input}'"
db.execute(query)
```

**âœ… CORRECT (Safe)**:
```python
# Use parameterized queries
query = "SELECT * FROM jobs WHERE title = %s"
db.execute(query, (user_input,))
```

### **SQLAlchemy ORM (Safer)**:
```python
from sqlalchemy import select
from models import Job

# SQLAlchemy automatically parameterizes
jobs = session.execute(
    select(Job).where(Job.title == user_input)
).all()
```

***

## **15.5 Secure Headers (Flask)**

```python
from flask import Flask
from flask_talisman import Talisman

app = Flask(__name__)

# Force HTTPS and set security headers
Talisman(app, 
    force_https=True,
    strict_transport_security=True,
    content_security_policy={
        'default-src': "'self'",
        'script-src': ["'self'", "'unsafe-inline'", "cdn.tailwindcss.com"],
        'style-src': ["'self'", "'unsafe-inline'"],
        'img-src': ["'self'", "data:", "https:"],
    }
)

@app.after_request
def add_security_headers(response):
    """Add additional security headers."""
    response.headers['X-Content-Type-Options'] = 'nosniff'
    response.headers['X-Frame-Options'] = 'DENY'
    response.headers['X-XSS-Protection'] = '1; mode=block'
    response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'
    return response
```

***

## **15.6 Privacy Compliance**

### **Personal Data Handling**[3]

**What We Scrape**:
- âœ… Job titles, descriptions, company names (public info)
- âœ… Salary ranges, locations, skills (public info)
- âœ… Application URLs (public info)

**What We DON'T Scrape**:
- âŒ Personal emails of individual applicants
- âŒ Phone numbers of individuals
- âŒ Names of hiring managers (unless publicly listed)
- âŒ Internal company data

**Data Retention Policy**:
- Jobs archived after 90 days (set `is_active=FALSE`)
- Full deletion after 1 year
- Users can request removal (email support)

### **robots.txt Compliance**[3]

```python
import requests
from urllib.robotparser import RobotFileParser

def check_robots_txt(site_url):
    """
    Check if scraping is allowed per site's robots.txt.
    """
    robots_url = f"{site_url}/robots.txt"
    
    parser = RobotFileParser()
    parser.set_url(robots_url)
    parser.read()
    
    # Check if our user agent is allowed
    user_agent = "PakJobsBot/1.0"
    job_search_url = f"{site_url}/jobs"
    
    if parser.can_fetch(user_agent, job_search_url):
        return True, "Scraping allowed"
    else:
        return False, "Scraping disallowed by robots.txt"

# Example usage
allowed, message = check_robots_txt("https://www.rozee.pk")
if not allowed:
    logger.warning(f"Rozee.pk: {message}")
    # Skip scraping or use official API
```

***

# **16. DEPLOYMENT STRATEGY**

## **16.1 Render.com Deployment**[4][5][6]

### **Why Render?**

| Feature | Render (Free Tier) | Alternatives |
|---------|-------------------|--------------|
| **Web Service** | 512MB RAM, 0.1 CPU | Heroku (removed free tier)[7] |
| **Database** | PostgreSQL 1GB, 90 days | PythonAnywhere (limited)[8] |
| **Auto-Deploy** | GitHub integration | Railway ($5 credit)[9] |
| **Custom Domain** | Free SSL | Most require paid plans |
| **Cron Jobs** | Built-in scheduler | Manual setup elsewhere |
| **Uptime** | 99.9% SLA | PythonAnywhere 99%[7] |
| **Zero Cost** | âœ… Forever free | Many sunset free tiers |

***

## **16.2 Step-by-Step Deployment**

### **Phase 1: GitHub Repository Setup**

```bash
# 1. Initialize Git repository
git init
git add .
git commit -m "Initial commit: PakJobs Aggregator"

# 2. Create GitHub repository (via GitHub website)
# 3. Push to GitHub
git remote add origin https://github.com/yourusername/pakjobs-aggregator.git
git branch -M main
git push -u origin main
```

**Repository Structure**:
```
pakjobs-aggregator/
â”œâ”€â”€ app.py                    # Main Flask application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ render.yaml              # Render deployment config
â”œâ”€â”€ runtime.txt              # Python version
â”œâ”€â”€ Procfile                 # Process definition
â”œâ”€â”€ .env.example             # Environment template
â”œâ”€â”€ .gitignore               # Git ignore rules
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Base scraper class
â”‚   â”œâ”€â”€ rozee.py             # Rozee.pk scraper
â”‚   â”œâ”€â”€ mustakbil.py         # Mustakbil.com scraper
â”‚   â”œâ”€â”€ indeed.py            # Indeed.pk scraper
â”‚   â””â”€â”€ ...                  # Other scrapers
â”œâ”€â”€ data_pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cleaner.py           # Data cleaning
â”‚   â”œâ”€â”€ validator.py         # Data validation
â”‚   â””â”€â”€ exporters.py         # CSV/JSON export
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ base.html
â”‚   â”œâ”€â”€ home.html
â”‚   â”œâ”€â”€ jobs.html
â”‚   â””â”€â”€ analytics.html
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ js/
â””â”€â”€ scripts/
    â”œâ”€â”€ init_db.py           # Database initialization
    â””â”€â”€ backup_db.sh         # Backup script
```

***

### **Phase 2: Render Configuration**

**render.yaml** (Infrastructure as Code):[5]
```yaml
services:
  # Web Service (Flask App)
  - type: web
    name: pakjobs-api
    env: python
    plan: free
    buildCommand: pip install -r requirements.txt
    startCommand: gunicorn app:app
    envVars:
      - key: PYTHON_VERSION
        value: 3.12.0
      - key: DATABASE_URL
        fromDatabase:
          name: pakjobs-db
          property: connectionString
      - key: SECRET_KEY
        generateValue: true
      - key: ENCRYPTION_KEY
        generateValue: true

  # Cron Job (Daily Scraping)
  - type: cron
    name: pakjobs-scraper
    env: python
    schedule: "0 3 * * *"  # 3 AM PKT daily
    buildCommand: pip install -r requirements.txt
    startCommand: python scripts/run_scrapers.py
    envVars:
      - key: DATABASE_URL
        fromDatabase:
          name: pakjobs-db
          property: connectionString

databases:
  # PostgreSQL Database
  - name: pakjobs-db
    plan: free
    databaseName: pakjobs_prod
    user: pakjobs_user
```

***

### **Phase 3: Render Dashboard Setup**[6][5]

**Steps** (via Render Web UI):

1. **Sign Up**: Go to https://render.com and create account
2. **New Web Service**:
   - Click "New +" â†’ "Web Service"
   - Connect GitHub account
   - Select `pakjobs-aggregator` repository
   - Name: `pakjobs-api`
   - Environment: Python 3
   - Build Command: `pip install -r requirements.txt`
   - Start Command: `gunicorn app:app --bind 0.0.0.0:$PORT`
   - Plan: Free
3. **New PostgreSQL Database**:
   - Click "New +" â†’ "PostgreSQL"
   - Name: `pakjobs-db`
   - Plan: Free
   - Note connection string
4. **Environment Variables** (in Web Service settings):
   ```
   DATABASE_URL = [auto-linked from pakjobs-db]
   SECRET_KEY = [generate random 32-char string]
   ENCRYPTION_KEY = [generate Fernet key]
   SMTP_USERNAME = your-email@gmail.com
   SMTP_PASSWORD = your-app-password
   ```
5. **Add Cron Job**:
   - Click "New +" â†’ "Cron Job"
   - Name: `pakjobs-scraper`
   - Schedule: `0 3 * * *` (3 AM daily)
   - Command: `python scripts/run_scrapers.py`
   - Link to same database

***

### **Phase 4: Database Migration**

**scripts/init_db.py** (Run once after deployment):
```python
import psycopg2
import os

def init_database():
    """
    Initialize database schema on first deployment.
    """
    conn = psycopg2.connect(os.getenv('DATABASE_URL'))
    cursor = conn.cursor()
    
    # Read SQL schema file
    with open('schema.sql', 'r') as f:
        schema_sql = f.read()
    
    # Execute schema creation
    cursor.execute(schema_sql)
    conn.commit()
    
    print("âœ… Database schema created successfully")
    
    # Insert default configuration
    cursor.execute("""
        INSERT INTO user_config (config_key, config_value, description) VALUES
        ('scrape_schedule', '0 3 * * *', 'Daily scraping at 3 AM PKT'),
        ('email_recipients', '', 'Comma-separated email list'),
        ('external_db_enabled', 'false', 'Enable external DB push'),
        ('max_scrape_pages', '50', 'Max pages per site')
        ON CONFLICT (config_key) DO NOTHING
    """)
    conn.commit()
    
    print("âœ… Default configuration inserted")
    
    conn.close()

if __name__ == '__main__':
    init_database()
```

**Run via Render Shell**:
```bash
# Access Render Shell (via dashboard)
python scripts/init_db.py
```

***

### **Phase 5: Auto-Deployment with GitHub Actions**[4]

**.github/workflows/deploy.yml**:
```yaml
name: Deploy to Render

on:
  push:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run tests
        run: pytest tests/ --cov=scrapers --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Trigger Render Deploy
        run: |
          curl -X POST ${{ secrets.RENDER_DEPLOY_HOOK }}
```

**Setup**:
1. Get Render deploy hook URL from dashboard
2. Add to GitHub Secrets: `RENDER_DEPLOY_HOOK`
3. Every push to `main` â†’ Auto-deploy

***

## **16.3 Local Development Setup**

### **Run on Your Laptop** (Windows/Mac/Linux)

**Installation**:
```bash
# Clone repository
git clone https://github.com/yourusername/pakjobs-aggregator.git
cd pakjobs-aggregator

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install PostgreSQL locally (or use Docker)
# Windows: Download from postgresql.org
# Mac: brew install postgresql
# Linux: sudo apt install postgresql

# Create local database
createdb pakjobs_dev

# Set environment variables
cp .env.example .env
# Edit .env with your local settings

# Initialize database
python scripts/init_db.py

# Run Flask development server
flask run --debug

# Open browser: http://localhost:5000
```

**Run Scrapers Locally**:
```bash
# Run all scrapers
python scripts/run_scrapers.py

# Run single scraper
python -c "from scrapers.rozee import RozeeScraper; RozeeScraper().scrape()"
```

**Estimated Time for Full Local Scrape** (Your Question):
- **Sequential scraping** (9 sites): 80-110 minutes
- **RAM usage**: 200-400 MB
- **Disk usage**: ~500 MB (database + exports)
- **CPU usage**: 10-20% (single core)

***

## **16.4 Docker Deployment** (Optional, Advanced)

**Dockerfile**:
```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    chromium \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install Playwright browsers
RUN playwright install chromium

# Copy application code
COPY . .

# Expose port
EXPOSE 5000

# Run application
CMD ["gunicorn", "app:app", "--bind", "0.0.0.0:5000"]
```

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  web:
    build: .
    ports:
      - "5000:5000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/pakjobs
    depends_on:
      - db
  
  db:
    image: postgres:16
    environment:
      - POSTGRES_DB=pakjobs
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

**Run with Docker**:
```bash
docker-compose up -d
```

***

# **17. MONITORING & OBSERVABILITY**

## **17.1 Monitoring Stack**

### **Render Built-in Monitoring**[5]

**Metrics Available** (Free Tier):
- CPU usage
- Memory usage
- Response times
- Error rates
- Log streaming

**Access**: Render Dashboard â†’ Service â†’ Metrics tab

---

## **17.2 Application Logging**

### **Structured Logging with Structlog**

```python
import structlog
import logging

# Configure structlog
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Usage examples
logger.info("scraper_started", site="rozee", mode="incremental")
logger.warning("low_jobs_found", site="brightspyre", count=12, expected_min=50)
logger.error("scraper_failed", site="indeed", error=str(e), retry_count=3)
```

**Log Output** (JSON format for easy parsing):
```json
{
  "event": "scraper_started",
  "site": "rozee",
  "mode": "incremental",
  "timestamp": "2025-10-14T03:05:00Z",
  "level": "info"
}
```

***

## **17.3 Email Alerts**[10][11]

### **Alert Conditions**

| Condition | Trigger | Recipient | Priority |
|-----------|---------|-----------|----------|
| Scraper failure (3 retries) | After 3rd failure | Admin | High |
| Low jobs found (<10) | Any site gets <10 jobs | Admin | Medium |
| Database full | >900MB used (90% of 1GB) | Admin | High |
| Scrape completion | Every successful run | Admin | Low |
| Daily digest | 4 AM PKT daily | Admin | Low |

### **Email Templates**

**Scraper Failure Alert**:
```python
def send_failure_alert(site_name, error_message):
    """Send email alert when scraper fails."""
    subject = f"ðŸš¨ ALERT: {site_name} scraper failed"
    
    body = f"""
    <html>
    <body>
        <h2 style="color: red;">Scraper Failure Alert</h2>
        <p><strong>Site:</strong> {site_name}</p>
        <p><strong>Error:</strong> {error_message}</p>
        <p><strong>Time:</strong> {datetime.now().isoformat()}</p>
        <p><strong>Action Required:</strong> Check logs and update scraper if site layout changed.</p>
        <hr>
        <p><small>PakJobs Aggregator Monitoring System</small></p>
    </body>
    </html>
    """
    
    send_email(subject, body, priority='high')
```

**Daily Digest Email**:
```python
def send_daily_digest():
    """Send daily summary email at 4 AM."""
    # Get statistics from last 24 hours
    stats = get_daily_stats()
    
    subject = f"ðŸ“Š Daily Digest: {stats['total_jobs']} jobs scraped"
    
    body = f"""
    <html>
    <body>
        <h2>PakJobs Aggregator - Daily Report</h2>
        <h3>ðŸ“ˆ Statistics (Last 24 Hours)</h3>
        <ul>
            <li><strong>Total Jobs Scraped:</strong> {stats['total_jobs']}</li>
            <li><strong>New Jobs:</strong> {stats['new_jobs']}</li>
            <li><strong>Updated Jobs:</strong> {stats['updated_jobs']}</li>
            <li><strong>Active Sites:</strong> {stats['active_sites']}/9</li>
            <li><strong>Average Salary:</strong> PKR {stats['avg_salary']:,}</li>
        </ul>
        
        <h3>ðŸ” Top Performing Sites</h3>
        <table border="1" cellpadding="5">
            <tr>
                <th>Site</th>
                <th>Jobs Found</th>
                <th>Duration</th>
                <th>Status</th>
            </tr>
            {"".join(f"<tr><td>{s['site']}</td><td>{s['jobs']}</td><td>{s['duration']}s</td><td>{s['status']}</td></tr>" for s in stats['top_sites'])}
        </table>
        
        <h3>âš ï¸ Issues (if any)</h3>
        <ul>
            {"".join(f"<li>{issue}</li>" for issue in stats['issues']) if stats['issues'] else "<li>No issues detected</li>"}
        </ul>
        
        <p><a href="https://pakjobs.onrender.com/scrapers">View Full Dashboard â†’</a></p>
    </body>
    </html>
    """
    
    send_email(subject, body)
```

***

## **17.4 Health Check Endpoints**

```python
@app.route('/health')
def health_check():
    """
    Health check endpoint for monitoring.
    Returns 200 if service is healthy.
    """
    try:
        # Check database connection
        db = get_db()
        cursor = db.cursor()
        cursor.execute("SELECT 1")
        db_healthy = cursor.fetchone()[0] == 1
        db.close()
        
        # Check last scrape time (should be within 25 hours)
        last_scrape = get_last_scrape_time()
        scraping_healthy = (datetime.now() - last_scrape).total_seconds() < 90000
        
        if db_healthy and scraping_healthy:
            return jsonify({
                'status': 'healthy',
                'database': 'ok',
                'scraping': 'ok',
                'last_scrape': last_scrape.isoformat()
            }), 200
        else:
            return jsonify({
                'status': 'degraded',
                'database': 'ok' if db_healthy else 'error',
                'scraping': 'ok' if scraping_healthy else 'stale'
            }), 503
    
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e)
        }), 503
```

**Use with Uptime Monitors**:
- UptimeRobot (free 50 monitors): Check `/health` every 5 minutes
- Pingdom (free tier): Monitor `https://pakjobs.onrender.com/health`

***

# **18. ERROR HANDLING & RECOVERY**

## **18.1 Retry Mechanism with Exponential Backoff**

```python
import time
from functools import wraps

def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):
    """
    Decorator for retrying functions with exponential backoff.
    
    Args:
        max_retries: Maximum number of retry attempts
        base_delay: Initial delay in seconds
        max_delay: Maximum delay between retries
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            delay = base_delay
            
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    retries += 1
                    
                    if retries >= max_retries:
                        logger.error(
                            "max_retries_exceeded",
                            function=func.__name__,
                            error=str(e),
                            retries=retries
                        )
                        raise
                    
                    # Calculate exponential backoff delay
                    wait_time = min(delay * (2 ** (retries - 1)), max_delay)
                    
                    logger.warning(
                        "retry_attempt",
                        function=func.__name__,
                        retry=retries,
                        max_retries=max_retries,
                        wait_time=wait_time,
                        error=str(e)
                    )
                    
                    time.sleep(wait_time)
            
        return wrapper
    return decorator

# Usage
@retry_with_backoff(max_retries=3, base_delay=2)
def scrape_with_retry(url):
    """Scrape URL with automatic retry on failure."""
    response = requests.get(url)
    response.raise_for_status()
    return response.text
```

***

## **18.2 Graceful Degradation**

```python
class ScraperOrchestrator:
    """
    Orchestrator with graceful degradation:
    - If one scraper fails, continue with others
    - Never crash entire system due to single failure
    """
    
    def run_all_scrapers_safe(self):
        """
        Run all scrapers with fault tolerance.
        Failed scrapers don't stop execution.
        """
        results = {}
        
        for site_name, scraper_class in self.scrapers.items():
            try:
                logger.info("starting_scraper", site=site_name)
                
                scraper = scraper_class()
                jobs = scraper.scrape()
                
                results[site_name] = {
                    'status': 'success',
                    'jobs_found': len(jobs)
                }
                
                logger.info("scraper_completed", site=site_name, jobs=len(jobs))
                
            except Exception as e:
                # Log error but continue with next scraper
                logger.error("scraper_failed", site=site_name, error=str(e))
                
                results[site_name] = {
                    'status': 'failed',
                    'error': str(e)
                }
                
                # Send alert but don't crash
                send_failure_alert(site_name, str(e))
                
                # Continue to next scraper
                continue
        
        # Return results even if some failed
        successful = sum(1 for r in results.values() if r['status'] == 'success')
        logger.info(
            "scraping_completed",
            successful=successful,
            total=len(results),
            success_rate=f"{successful/len(results)*100:.1f}%"
        )
        
        return results
```

***

## **18.3 Database Transaction Rollback**

```python
def save_jobs_transactional(jobs):
    """
    Save jobs with transaction support.
    If any error occurs, rollback all changes.
    """
    conn = get_db()
    cursor = conn.cursor()
    
    try:
        # Begin transaction
        cursor.execute("BEGIN")
        
        for job in jobs:
            cursor.execute("""
                INSERT INTO jobs (id, title, company, ...)
                VALUES (%s, %s, %s, ...)
                ON CONFLICT (source_url) DO UPDATE
                SET title = EXCLUDED.title, ...
            """, (job['id'], job['title'], job['company'], ...))
        
        # Commit if all inserts successful
        conn.commit()
        logger.info("jobs_saved", count=len(jobs))
        
    except Exception as e:
        # Rollback on any error
        conn.rollback()
        logger.error("save_failed_rollback", error=str(e), jobs_count=len(jobs))
        raise
    
    finally:
        cursor.close()
        conn.close()
```

***

# **19. MAINTENANCE & UPDATES**

## **19.1 Scraper Maintenance Process**

### **When Site Layout Changes** (Common Issue)[12][3]

**Detection**:
```python
def detect_layout_change(scraper_name, jobs_found):
    """
    Detect if site layout changed based on abnormal results.
    """
    # Get average jobs found for this site (last 7 days)
    avg_jobs = get_average_jobs(scraper_name, days=7)
    
    # Alert if <20% of expected jobs found
    if jobs_found < avg_jobs * 0.2:
        logger.warning(
            "possible_layout_change",
            site=scraper_name,
            jobs_found=jobs_found,
            expected=avg_jobs,
            threshold=avg_jobs * 0.2
        )
        
        send_alert(
            subject=f"âš ï¸ Possible Layout Change: {scraper_name}",
            message=f"Only {jobs_found} jobs found (expected ~{avg_jobs}). Site may have changed layout."
        )
        
        return True
    
    return False
```

**Update Process**:
1. **Identify broken scraper** (via email alert or monitoring)
2. **Inspect site** (visit site, check HTML structure)
3. **Update CSS selectors** (in scraper file)
4. **Test locally**:
   ```bash
   python -c "from scrapers.rozee import RozeeScraper; RozeeScraper().scrape()[:5]"
   ```
5. **Commit and push**:
   ```bash
   git add scrapers/rozee.py
   git commit -m "fix: Update Rozee.pk selectors after layout change"
   git push origin main
   ```
6. **Auto-deploy** (GitHub Actions triggers Render deployment)

***

## **19.2 Version Control Strategy**

### **Git Branching Model**

```
main (production)
  â””â”€â”€ develop (staging)
       â”œâ”€â”€ feature/new-scraper-bayt
       â”œâ”€â”€ fix/rozee-pagination
       â””â”€â”€ enhance/analytics-dashboard
```

**Workflow**:
1. Create feature branch: `git checkout -b fix/rozee-selectors`
2. Make changes, test locally
3. Commit: `git commit -m "fix: Update Rozee CSS selectors"`
4. Push: `git push origin fix/rozee-selectors`
5. Create Pull Request on GitHub
6. Merge to `main` â†’ Auto-deploy

***

## **19.3 Database Maintenance**

### **Weekly Cleanup Job**

```python
# scripts/weekly_cleanup.py

def weekly_maintenance():
    """
    Run weekly database maintenance tasks.
    """
    conn = get_db()
    cursor = conn.cursor()
    
    # 1. Archive old jobs (90+ days)
    cursor.execute("""
        UPDATE jobs
        SET is_active = FALSE
        WHERE posted_date < CURRENT_DATE - INTERVAL '90 days'
        AND is_active = TRUE
    """)
    archived_count = cursor.rowcount
    logger.info("archived_old_jobs", count=archived_count)
    
    # 2. Delete old scrape logs (6+ months)
    cursor.execute("""
        DELETE FROM scrape_logs
        WHERE created_at < NOW() - INTERVAL '6 months'
    """)
    deleted_logs = cursor.rowcount
    logger.info("deleted_old_logs", count=deleted_logs)
    
    # 3. Vacuum database (reclaim space)
    cursor.execute("VACUUM ANALYZE")
    logger.info("vacuum_completed")
    
    # 4. Check database size
    cursor.execute("""
        SELECT pg_size_pretty(pg_database_size(current_database()))
    """)
    db_size = cursor.fetchone()[0]
    logger.info("database_size", size=db_size)
    
    # Alert if approaching 1GB limit
    cursor.execute("SELECT pg_database_size(current_database()) / 1024 / 1024 / 1024.0")
    size_gb = cursor.fetchone()[0]
    if size_gb > 0.9:  # 90% of 1GB
        send_alert(
            subject="ðŸš¨ Database Almost Full",
            message=f"Database size: {db_size} (90% of 1GB free tier limit)"
        )
    
    conn.commit()
    conn.close()
```

**Schedule**: Run every Sunday at 2 AM via Render Cron Job

***

## **19.4 Dependency Updates**

### **Monthly Security Updates**

```bash
# Check for outdated packages
pip list --outdated

# Update all packages
pip install --upgrade -r requirements.txt

# Run tests to ensure nothing broke
pytest tests/

# Update requirements.txt
pip freeze > requirements.txt

# Commit and deploy
git add requirements.txt
git commit -m "chore: Update dependencies (security patches)"
git push
```

**Automated with Dependabot** (GitHub):
```yaml
# .github/dependabot.yml
version: 2
updates:
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "monthly"
    open-pull-requests-limit: 10
```

***

# **20. LEGAL & ETHICAL COMPLIANCE**

## **20.1 Web Scraping Legality**[12][3]

### **Legal Framework (Pakistan Context)**

| Aspect | Status | Notes |
|--------|--------|-------|
| **Scraping Public Data** | âœ… Generally Legal | Job postings are public information |
| **Copyright** | âš ï¸ Varies | Facts (job titles, salaries) not copyrightable; descriptions may be |
| **Terms of Service** | âš ï¸ Check Each Site | Some sites prohibit scraping in ToS |
| **Personal Data** | âŒ Restricted | Avoid scraping personal info (PDPA 2023) |
| **Commercial Use** | âš ï¸ Site-Specific | Some sites ban commercial scraping |

### **Our Compliance Approach**[3]

1. **Respect robots.txt** for all sites[13][3]
2. **Polite scraping** (3s delays, off-peak hours)[14][13]
3. **Attribution links** to original job postings
4. **No personal data** (skip emails, phones unless public)
5. **Transformative use** (add analytics, insights)
6. **Terms of Service monitoring** (quarterly review)

***

## **20.2 robots.txt Compliance**[3]

### **Check Before Scraping**

**Example robots.txt inspection**:
```
https://www.rozee.pk/robots.txt

User-agent: *
Disallow: /admin/
Disallow: /user/private/
Allow: /job/
Crawl-delay: 2
```

**Interpretation**:
- âœ… `/job/` pages allowed
- âŒ `/admin/` pages disallowed
- âœ… Crawl delay: 2 seconds (we use 3s, safer)

### **Implementation**

```python
from urllib.robotparser import RobotFileParser

class RobotsTxtChecker:
    """Check robots.txt compliance before scraping."""
    
    def __init__(self, site_url):
        self.robots_url = f"{site_url}/robots.txt"
        self.parser = RobotFileParser()
        self.parser.set_url(self.robots_url)
        try:
            self.parser.read()
            self.available = True
        except:
            self.available = False
            logger.warning("robots_txt_unavailable", site=site_url)
    
    def can_fetch(self, url, user_agent="PakJobsBot/1.0"):
        """Check if URL can be scraped."""
        if not self.available:
            return True  # Assume allowed if robots.txt unavailable
        
        return self.parser.can_fetch(user_agent, url)
    
    def get_crawl_delay(self, user_agent="PakJobsBot/1.0"):
        """Get recommended crawl delay in seconds."""
        if not self.available:
            return 3  # Default safe delay
        
        delay = self.parser.crawl_delay(user_agent)
        return max(delay or 3, 3)  # Minimum 3 seconds
```

***

## **20.3 Attribution & Source Links**

### **Always Link to Original Source**

**In Database**:
```sql
SELECT title, company, source_url FROM jobs LIMIT 1;

-- Result:
-- title: "Python Developer"
-- company: "TechCorp"
-- source_url: "https://www.rozee.pk/job/12345"  â† Always included
```

**In UI**:
```html
<!-- Job listing card -->
<div class="job-card">
    <h3>Python Developer</h3>
    <p>TechCorp - Karachi</p>
    <a href="https://www.rozee.pk/job/12345" target="_blank">
        Apply on Rozee.pk â†’
    </a>
    <small>Source: Rozee.pk</small>
</div>
```

**In API**:
```json
{
  "title": "Python Developer",
  "source_url": "https://www.rozee.pk/job/12345",
  "site_source": "rozee",
  "attribution": "Job posted on Rozee.pk"
}
```

***

## **20.4 Privacy Policy Template**

**To be displayed on website** (required for transparency):

```markdown
# Privacy Policy - PakJobs Aggregator

## What Data We Collect
- **Public Job Listings**: Job titles, descriptions, company names, locations, salaries (all publicly available on source sites)
- **Usage Data**: Anonymous analytics (page views, search queries)
- **No Personal Data**: We do NOT collect user emails, phone numbers, or personal information

## How We Use Data
- Aggregate job listings from 9 Pakistani job portals
- Provide salary trends and market analytics
- Improve search and filtering functionality

## Data Sources
All job listings are scraped from publicly accessible websites:
- Rozee.pk, Mustakbil.com, Indeed.pk, BrightSpyre.com, Bayt.com, Jobz.pk, Bayrozgar.com, JobsAlert.pk, PakPositions.com

## Your Rights
- Request job listing removal: Email support@pakjobs.example.com
- Data retention: Jobs archived after 90 days, deleted after 1 year
- No user accounts (Phase 1): No personal data stored

## Contact
For privacy concerns or removal requests: privacy@pakjobs.example.com

Last Updated: October 14, 2025
```

***

## **20.5 Ethical Scraping Checklist**

**Before Deploying**:
- [ ] Verified robots.txt compliance for all 9 sites
- [ ] Implemented 3-second delays between requests
- [ ] Scheduled scraping during off-peak hours (3 AM PKT)
- [ ] Added polite user-agent identification
- [ ] Included attribution links to original sources
- [ ] Excluded personal data from scraping
- [ ] Set up monitoring for abnormal request patterns
- [ ] Created privacy policy page
- [ ] Added "About" page explaining data sources
- [ ] Implemented rate limiting on API (prevent abuse)

***

# **21. RISK ANALYSIS & MITIGATION**

## **21.1 Technical Risks**

### **Risk 1: Site Layout Changes (HIGH PROBABILITY)**[12][3]

**Impact**: Scraper breaks, no new jobs collected  
**Probability**: High (every 3-6 months per site)  
**Mitigation**:
- Automated anomaly detection (alert if <20% expected jobs)[15]
- Email alerts on failures[11][10]
- Modular scraper design (update one without affecting others)[16]
- Weekly manual spot checks
- Version control (easy rollback if update breaks)[4]

**Response Time Target**: Fix within 24 hours

---

### **Risk 2: IP Blocking/Rate Limiting (MEDIUM)**[13][14]

**Impact**: Scraper can't access site, temporary outage  
**Probability**: Medium (if not polite)  
**Mitigation**:
- 3-second delays between requests[14][13]
- Rotate user-agents (mimic different browsers)[13]
- Scrape during off-peak hours (3 AM PKT)[3]
- Monitor for CAPTCHA challenges
- Respect rate limits from sites

**If Blocked**:
1. Wait 24 hours, retry
2. Contact site webmaster (explain educational use)
3. Consider official API if available
4. Skip site temporarily, notify users

***

### **Risk 3: Free Tier Limits Exceeded (MEDIUM)**[5][4]

**Impact**: Service degradation or shutdown  
**Probability**: Medium (as database grows)  
**Mitigation**:
- Automated cleanup (archive jobs >90 days)[17][15]
- Incremental scraping (reduce bandwidth 90%)[15]
- Monitor database size (alert at 900MB/1GB)[2]
- Optimize indexes for query performance[2]
- Compress old data before archiving

**Contingency Plan**:
- Reduce scraping frequency (daily â†’ every 2 days)
- Archive more aggressively (60 days instead of 90)
- Upgrade to paid plan if revenue available ($7/month for Render)

***

### **Risk 4: Scraper Crashes (LOW)**

**Impact**: Partial data loss for one scraping cycle  
**Probability**: Low (with retry logic)  
**Mitigation**:
- 3-retry mechanism with exponential backoff
- Transaction rollbacks (no partial data)[2]
- Graceful degradation (failed scraper doesn't stop others)[16]
- Comprehensive error logging
- Health check endpoints

***

## **21.2 Legal Risks**

### **Risk 5: Terms of Service Violations (MEDIUM)**[12][3]

**Impact**: Cease-and-desist letter, potential lawsuit  
**Probability**: Low-Medium (depends on site)  
**Mitigation**:
- Quarterly ToS review for all sites
- Legal disclaimer on website
- Transformative use (add analytics)[3]
- Attribution links to sources
- Comply with removal requests (within 48 hours)

**If ToS Change**:
1. Review new terms immediately
2. Adjust scraping (or stop if prohibited)
3. Seek legal advice if ambiguous
4. Consider official API as alternative

***

### **Risk 6: Copyright Infringement (LOW)**[3]

**Impact**: DMCA takedown, legal action  
**Probability**: Low (facts not copyrightable)  
**Mitigation**:
- Don't reproduce full job descriptions (excerpts only)
- Link to original source (not hosting content)
- Transformative use (aggregate + analytics)
- Respond quickly to DMCA notices
- Disclaimer: "Job postings Â© respective companies"

***

## **21.3 Operational Risks**

### **Risk 7: Data Inaccuracy (MEDIUM)**

**Impact**: Users apply to expired/fake jobs  
**Probability**: Medium (sites have stale postings)  
**Mitigation**:
- Daily scraping (keep data fresh)[15]
- Mark jobs as "Last updated X hours ago"
- Archive jobs >90 days old
- Disclaimer: "Verify with source site before applying"
- Duplicate detection (mark cross-posted jobs)[17]

---

### **Risk 8: Maintenance Burden (HIGH)**[12][3]

**Impact**: Developer burnout, abandoned project  
**Probability**: High (if not automated)  
**Mitigation**:
- Comprehensive documentation (easy for others to contribute)
- Automated testing (pytest suite)
- Modular code (easy to understand/modify)[16]
- Version control (track all changes)[4]
- CI/CD (automated deployment)[4]
- Community contributions (open-source Phase 2)

***

## **21.4 Risk Priority Matrix**

```
         HIGH IMPACT
             â”‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
             â”‚
Risk 1       â”‚  Risk 5
(Layout)     â”‚  (ToS)
             â”‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
             â”‚
Risk 8       â”‚  Risk 2
(Maintenance)â”‚  (IP Block)
             â”‚
         LOW IMPACT

    LOW PROB â† â†’ HIGH PROB
```

**Priority Order** (tackle first):
1. **Risk 1** (Layout Changes): High probability, high impact â†’ Robust monitoring
2. **Risk 8** (Maintenance): High probability, high impact â†’ Automation
3. **Risk 5** (ToS): Medium probability, high impact â†’ Legal review
4. **Risk 2** (IP Block): Medium probability, medium impact â†’ Polite scraping

***

# **22. PERFORMANCE REQUIREMENTS**

## **22.1 Performance Targets**

| Metric | Target | Measurement |
|--------|--------|-------------|
| **API Response Time** | <500ms (P95) | Render metrics[5] |
| **Database Query Time** | <100ms (avg) | PostgreSQL EXPLAIN ANALYZE[2] |
| **Full-Text Search** | <300ms | GIN index performance[2] |
| **Scraping Duration** | <2 hours (all sites) | Scrape logs |
| **Page Load Time** | <2s (dashboard) | Browser DevTools |
| **Uptime** | 99.5%+ | UptimeRobot monitoring |
| **Memory Usage** | <400MB (web service) | Render dashboard[5] |

***

## **22.2 Database Optimization**[2]

### **Index Strategy**

```sql
-- Essential indexes for performance
CREATE INDEX idx_jobs_posted_date ON jobs(posted_date DESC);
CREATE INDEX idx_jobs_city ON jobs(city) WHERE city IS NOT NULL;
CREATE INDEX idx_jobs_salary_range ON jobs(salary_min, salary_max) WHERE salary_min IS NOT NULL;
CREATE INDEX idx_jobs_site_source ON jobs(site_source);

-- Full-text search index (critical for search performance)
CREATE INDEX idx_jobs_fts ON jobs USING GIN(
    to_tsvector('english', COALESCE(title, '') || ' ' || COALESCE(description, ''))
);

-- JSONB index for skills array
CREATE INDEX idx_jobs_skills_gin ON jobs USING GIN(skills jsonb_path_ops);

-- Composite index for common query pattern
CREATE INDEX idx_jobs_active_posted ON jobs(is_active, posted_date DESC) 
WHERE is_active = TRUE;
```

**Query Performance Example**:
```sql
-- WITHOUT index: 850ms (full table scan)
-- WITH index: 35ms (index scan)
EXPLAIN ANALYZE
SELECT * FROM jobs 
WHERE posted_date > CURRENT_DATE - INTERVAL '7 days'
ORDER BY posted_date DESC
LIMIT 100;
```

***

## **22.3 Caching Strategy**

### **Analytics Caching**

```python
from functools import lru_cache
from datetime import datetime, timedelta

@lru_cache(maxsize=128)
def get_salary_trends(cache_key):
    """
    Cache salary trends for 6 hours.
    cache_key format: "salary_trends_2025-10-14"
    """
    # Expensive query
    results = db.execute("""
        SELECT experience_level, AVG(salary_min), AVG(salary_max)
        FROM jobs
        WHERE posted_date >= CURRENT_DATE - INTERVAL '90 days'
        GROUP BY experience_level
    """)
    return results

# Usage with cache invalidation
def get_cached_salary_trends():
    # Cache key changes daily (auto-refresh)
    cache_key = f"salary_trends_{datetime.now().strftime('%Y-%m-%d')}"
    return get_salary_trends(cache_key)
```

***

## **22.4 Scraping Performance Optimization**[17][15]

### **Incremental Scraping Gains**

| Metric | Full Scrape | Incremental | Improvement |
|--------|-------------|-------------|-------------|
| **Duration** | 90 minutes | 15 minutes | **6x faster** |
| **Bandwidth** | 4 GB | 400 MB | **10x less** |
| **Database Writes** | 50,000 | 2,000 | **25x fewer** |
| **CPU Usage** | 15% (90 min) | 15% (15 min) | **6x less time** |

**Key Optimization**: Early termination when hitting previously-scraped jobs[15]

***

# **23. TESTING STRATEGY**

## **23.1 Testing Pyramid**

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  E2E Tests   â”‚  (10 tests - browser automation)
         â”‚   (5%)       â”‚
       â”Œâ”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”
       â”‚ Integration Testsâ”‚  (30 tests - API, database)
       â”‚      (25%)       â”‚
     â”Œâ”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”
     â”‚   Unit Tests          â”‚  (100 tests - functions, classes)
     â”‚       (70%)           â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **23.2 Unit Tests (Pytest)**

```python
# tests/test_data_cleaner.py

import pytest
from data_pipeline.cleaner import DataCleaner

class TestDataCleaner:
    """Test data cleaning functions."""
    
    def test_strip_html(self):
        """Test HTML tag removal."""
        input_html = "<p>This is a <strong>job</strong> description.</p>"
        expected = "This is a job description."
        
        result = DataCleaner.strip_html(input_html)
        
        assert result == expected
    
    def test_parse_salary_range_valid(self):
        """Test salary parsing with valid input."""
        input_text = "PKR 50,000 - 80,000 per month"
        expected = (50000, 80000)
        
        result = DataCleaner.parse_salary_range(input_text)
        
        assert result == expected
    
    def test_parse_salary_range_k_suffix(self):
        """Test salary parsing with K suffix."""
        input_text = "50K-80K"
        expected = (50000, 80000)
        
        result = DataCleaner.parse_salary_range(input_text)
        
        assert result == expected
    
    def test_parse_salary_range_single_value(self):
        """Test salary parsing with single value."""
        input_text = "Salary: 60,000"
        expected = (60000, 60000)
        
        result = DataCleaner.parse_salary_range(input_text)
        
        assert result == expected
    
    def test_parse_salary_range_no_value(self):
        """Test salary parsing with no numeric value."""
        input_text = "Market Competitive"
        expected = (None, None)
        
        result = DataCleaner.parse_salary_range(input_text)
        
        assert result == expected
    
    def test_extract_city_karachi(self):
        """Test city extraction from location string."""
        input_location = "Karachi, Sindh, Pakistan"
        expected = "Karachi"
        
        result = DataCleaner.extract_city(input_location)
        
        assert result == expected
    
    @pytest.mark.parametrize("description,expected_skills", [
        ("We need a Python developer with Django experience", ["Python", "Django"]),
        ("Requires JavaScript, React, and Node.js skills", ["JavaScript", "React", "Node.Js"]),
        ("No technical skills mentioned here", [])
    ])
    def test_extract_skills(self, description, expected_skills):
        """Test skill extraction from descriptions."""
        result = DataCleaner.extract_skills(description)
        
        assert set(result) == set(expected_skills)

# Run tests
# pytest tests/test_data_cleaner.py -v --cov
```

***

## **23.3 Integration Tests**

```python
# tests/test_api_endpoints.py

import pytest
from app import app
import json

@pytest.fixture
def client():
    """Flask test client."""
    app.config['TESTING'] = True
    with app.test_client() as client:
        yield client

class TestAPIEndpoints:
    """Test REST API endpoints."""
    
    def test_health_check(self, client):
        """Test /health endpoint."""
        response = client.get('/health')
        
        assert response.status_code == 200
        data = json.loads(response.data)
        assert data['status'] in ['healthy', 'degraded']
    
    def test_list_jobs(self, client):
        """Test GET /api/v1/jobs."""
        response = client.get('/api/v1/jobs?page=1&per_page=20')
        
        assert response.status_code == 200
        data = json.loads(response.data)
        assert 'data' in data
        assert 'pagination' in data
        assert len(data['data']) <= 20
    
    def test_search_jobs(self, client):
        """Test full-text search."""
        response = client.get('/api/v1/jobs/search?q=python developer')
        
        assert response.status_code == 200
        data = json.loads(response.data)
        assert 'data' in data
        # Check if results contain search term
        if data['data']:
            first_job = data['data'][0]
            assert 'python' in first_job['title'].lower() or 'python' in first_job['description'].lower()
    
    def test_get_single_job(self, client):
        """Test GET /api/v1/jobs/{id}."""
        # Get any job ID first
        response = client.get('/api/v1/jobs?per_page=1')
        data = json.loads(response.data)
        
        if data['data']:
            job_id = data['data'][0]['id']
            
            # Test single job retrieval
            response = client.get(f'/api/v1/jobs/{job_id}')
            assert response.status_code == 200
            data = json.loads(response.data)
            assert data['data']['id'] == job_id
    
    def test_rate_limiting(self, client):
        """Test API rate limiting."""
        # Make 110 requests (exceeds 100/minute limit)
        for i in range(110):
            response = client.get('/api/v1/jobs')
        
        # Last request should be rate limited
        assert response.status_code == 429
        data = json.loads(response.data)
        assert 'RATE_LIMIT_EXCEEDED' in data['error']['code']

# Run: pytest tests/test_api_endpoints.py -v
```

***

## **23.4 End-to-End Tests (Playwright)**

```python
# tests/test_e2e.py

from playwright.sync_api import sync_playwright

def test_home_page_loads():
    """Test home page loads successfully."""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        # Navigate to home page
        page.goto('http://localhost:5000')
        
        # Check title
        assert 'PakJobs' in page.title()
        
        # Check key elements
        assert page.query_selector('h1') is not None
        assert page.query_selector('.scraper-status') is not None
        
        browser.close()

def test_job_search_flow():
    """Test complete job search flow."""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        
        # Go to jobs page
        page.goto('http://localhost:5000/jobs')
        
        # Enter search query
        page.fill('input[name="q"]', 'python developer')
        page.click('button[type="submit"]')
        
        # Wait for results
        page.wait_for_selector('.job-card')
        
        # Check results loaded
        job_cards = page.query_selector_all('.job-card')
        assert len(job_cards) > 0
        
        # Click first job
        first_job = job_cards[0]
        first_job.click()
        
        # Check job detail page loaded
        assert page.query_selector('.job-detail') is not None
        
        browser.close()

# Run: pytest tests/test_e2e.py
```

***

## **23.5 Test Coverage Goals**

| Component | Target Coverage | Current |
|-----------|----------------|---------|
| **Data Cleaners** | 90%+ | TBD |
| **Validators** | 95%+ | TBD |
| **API Endpoints** | 80%+ | TBD |
| **Scrapers** | 70%+ | TBD |
| **Overall** | 80%+ | TBD |

**Run Coverage Report**:
```bash
pytest --cov=scrapers --cov=data_pipeline --cov=app --cov-report=html
open htmlcov/index.html
```

***

# **24. TIMELINE & MILESTONES**

## **24.1 Development Phases**

### **Phase 1: Foundation (Weeks 1-2)**

**Milestone 1.1: Project Setup** (Days 1-3)
- [ ] Create GitHub repository
- [ ] Set up local development environment
- [ ] Initialize Flask application structure
- [ ] Create PostgreSQL database schema
- [ ] Write README and contribution guidelines

**Milestone 1.2: Base Scraper** (Days 4-7)
- [ ] Implement BaseScraper abstract class
- [ ] Build data validation module (Pydantic)
- [ ] Create data cleaning utilities
- [ ] Write unit tests for core functions
- [ ] Set up logging infrastructure

**Milestone 1.3: First Scraper** (Days 8-14)
- [ ] Implement Rozee.pk scraper (Scrapy)
- [ ] Test scraping 100 jobs
- [ ] Implement incremental scraping logic
- [ ] Save to local database
- [ ] Verify data quality (manual checks)

***

### **Phase 2: Core Scrapers (Weeks 3-4)**

**Milestone 2.1: Static Site Scrapers** (Days 15-21)
- [ ] Mustakbil.com scraper
- [ ] Jobz.pk scraper
- [ ] Bayrozgar.com scraper
- [ ] JobsAlert.pk scraper
- [ ] PakPositions.com scraper
- [ ] Unit tests for each scraper

**Milestone 2.2: Dynamic Site Scrapers** (Days 22-28)
- [ ] Indeed.pk scraper (Playwright)
- [ ] BrightSpyre.com scraper
- [ ] Bayt.com scraper
- [ ] Handle JavaScript rendering
- [ ] Test each scraper individually

***

### **Phase 3: Data Pipeline (Weeks 5-6)**

**Milestone 3.1: Data Processing** (Days 29-35)
- [ ] Implement deduplication logic
- [ ] Build external database connectors
- [ ] Create CSV/JSON exporters
- [ ] Write

# **PRODUCT REQUIREMENTS DOCUMENT (PRD) - FINAL PART**

***

# **24. TIMELINE & MILESTONES (CONTINUED)**

### **Phase 3: Data Pipeline (Weeks 5-6) - CONTINUED**

**Milestone 3.1: Data Processing** (Days 29-35)
- [ ] Implement deduplication logic (exact + fuzzy matching)
- [ ] Build external database connectors (PostgreSQL, MySQL, MongoDB)
- [ ] Create CSV/JSON/Excel exporters
- [ ] Write integration tests for data pipeline
- [ ] Optimize database queries with indexes

**Milestone 3.2: Orchestration** (Days 36-42)
- [ ] Build scraping orchestrator (sequential execution)
- [ ] Implement retry logic with exponential backoff
- [ ] Set up email notification system (Gmail SMTP)
- [ ] Create automated scheduling (APScheduler)
- [ ] Test full scraping workflow end-to-end

---

### **Phase 4: Web Dashboard (Weeks 7-8)**

**Milestone 4.1: Basic Dashboard** (Days 43-49)
- [ ] Set up Flask application structure
- [ ] Create base templates (Jinja2)
- [ ] Implement home page with statistics
- [ ] Build scraper status monitoring page
- [ ] Add Tailwind CSS styling
- [ ] Make responsive for mobile devices

**Milestone 4.2: Job Search Interface** (Days 50-56)
- [ ] Implement full-text search functionality
- [ ] Build advanced filter system (location, salary, experience)
- [ ] Create pagination for results
- [ ] Add job detail page
- [ ] Implement search result highlighting
- [ ] Test search performance (<500ms)

***

### **Phase 5: Analytics & API (Weeks 9-10)**

**Milestone 5.1: Analytics Dashboard** (Days 57-63)
- [ ] Build salary trends visualization
- [ ] Create top skills analysis
- [ ] Implement location heatmap (ASCII)
- [ ] Add company insights (top employers)
- [ ] Cache analytics queries for performance
- [ ] Update analytics daily via cron job

**Milestone 5.2: REST API** (Days 64-70)
- [ ] Implement core API endpoints (jobs, search, stats)
- [ ] Add rate limiting (1000 requests/hour)
- [ ] Enable CORS for cross-origin requests
- [ ] Create OpenAPI/Swagger documentation
- [ ] Write API integration tests
- [ ] Test API performance benchmarks

***

### **Phase 6: Testing & Quality Assurance (Week 11)**

**Milestone 6.1: Comprehensive Testing** (Days 71-77)
- [ ] Write 100+ unit tests (pytest)
- [ ] Create 30+ integration tests
- [ ] Implement 10+ end-to-end tests (Playwright)
- [ ] Achieve 80%+ code coverage
- [ ] Fix all critical bugs
- [ ] Perform load testing (simulate 1000 users)
- [ ] Security audit (SQL injection, XSS tests)

***

### **Phase 7: Deployment & Launch (Week 12)**

**Milestone 7.1: Production Deployment** (Days 78-84)
- [ ] Set up Render.com account
- [ ] Configure PostgreSQL database (1GB free tier)
- [ ] Deploy Flask application to Render
- [ ] Set up environment variables
- [ ] Configure cron job (daily 3 AM scraping)
- [ ] Initialize production database
- [ ] Run first full production scrape
- [ ] Verify all 9 scrapers working
- [ ] Set up monitoring and alerts
- [ ] Create backup procedures
- [ ] Write operational documentation
- [ ] **LAUNCH DAY**: Make platform publicly accessible

***

## **24.2 Gantt Chart (Visual Timeline)**

```
Week â†’  1    2    3    4    5    6    7    8    9    10   11   12
        â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚
Phase 1 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
        â”‚Setup   â”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚
Phase 2         â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
                â”‚  Core Scrapers  â”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚
Phase 3                 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
                        â”‚Pipelineâ”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚
Phase 4                         â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
                                â”‚Dashboardâ”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚
Phase 5                                 â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
                                        â”‚Analyticsâ”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚
Phase 6                                         â”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚
                                                â”‚Testâ”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”‚
Phase 7                                             â”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚
                                                    â”‚Deploy
                                                    
ðŸš€ LAUNCH: Day 84 (Week 12, End of Day 7)
```

***

## **24.3 Critical Path**

**Critical Dependencies** (delays in these affect launch):
1. **Base Scraper Framework** â†’ All other scrapers depend on this
2. **PostgreSQL Database** â†’ Everything depends on data storage
3. **Rozee.pk Scraper** â†’ Largest data source, must work first
4. **Data Pipeline** â†’ Required before dashboard can display data
5. **Render Deployment** â†’ Final blocker before launch

**Parallel Workstreams** (can be done simultaneously):
- Analytics development + API development (Week 9-10)
- Dashboard UI + Backend API (Week 7-9)
- Unit tests + Integration tests (ongoing throughout)

***

## **24.4 Risk Buffer**

**Contingency Time**: 2 weeks built into 12-week plan
- **Best Case**: Launch in 10 weeks (if no major issues)
- **Realistic**: Launch in 12 weeks (expected timeline)
- **Worst Case**: Launch in 14 weeks (if scrapers break frequently)

**High-Risk Milestones** (likely to take longer):
- âš ï¸ **Indeed.pk Scraper** (Playwright complexity): +2-3 days buffer
- âš ï¸ **Deduplication Logic** (fuzzy matching): +2 days buffer
- âš ï¸ **Production Deployment** (first-time Render setup): +2 days buffer

***

# **25. SUCCESS METRICS & KPIs**

## **25.1 Technical Performance Metrics**

### **Scraping Success Metrics**

| KPI | Target | Measurement Method | Review Frequency |
|-----|--------|-------------------|------------------|
| **Scraper Uptime** | 95%+ | `COUNT(status='success')/COUNT(*)` from scrape_logs | Daily |
| **Jobs Scraped Daily** | 5,000+ new jobs | Daily count of new records | Daily |
| **Scraping Duration** | <120 minutes | Log analysis (all 9 sites) | Per run |
| **Error Rate** | <2% | Failed requests / Total requests | Daily |
| **Data Quality** | 98%+ valid | Validation pass rate | Weekly |
| **Incremental Efficiency** | 10x faster than full | Compare incremental vs full scrape times | Monthly |

**Measurement Queries**:
```sql
-- Scraper uptime (last 30 days)
SELECT 
    site_name,
    COUNT(CASE WHEN status = 'success' THEN 1 END) * 100.0 / COUNT(*) AS uptime_pct
FROM scrape_logs
WHERE start_time >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY site_name;

-- Daily jobs scraped
SELECT 
    DATE(scraped_at) AS date,
    COUNT(*) AS new_jobs
FROM jobs
WHERE scraped_at >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY DATE(scraped_at)
ORDER BY date DESC;
```

***

### **Application Performance Metrics**

| KPI | Target | Measurement | Tool |
|-----|--------|-------------|------|
| **API Response Time (P95)** | <500ms | 95th percentile latency | Render metrics |
| **Database Query Time** | <100ms avg | PostgreSQL slow query log | pg_stat_statements |
| **Page Load Time** | <2 seconds | Time to interactive | Browser DevTools |
| **Uptime SLA** | 99.5%+ | Service availability | UptimeRobot |
| **Memory Usage** | <400MB | RSS memory | Render dashboard |
| **Database Size** | <900MB (90% of limit) | pg_database_size() | Daily check |

***

## **25.2 Product Usage Metrics**

### **User Engagement KPIs** (Phase 2 - requires analytics implementation)

| Metric | Target (Month 3) | Target (Month 6) | Definition |
|--------|------------------|------------------|------------|
| **Monthly Active Users** | 500+ | 2,000+ | Unique visitors/month |
| **Average Session Duration** | 3+ minutes | 5+ minutes | Time spent per visit |
| **Jobs Viewed per Session** | 5+ | 8+ | Job detail page views |
| **Search Conversion Rate** | 60%+ | 70%+ | Searches resulting in job views |
| **Return User Rate** | 30%+ | 40%+ | Users returning within 7 days |
| **API Usage** | 1,000+ calls/day | 5,000+ calls/day | External API requests |

***

## **25.3 Data Quality Metrics**

### **Content Accuracy KPIs**

| Metric | Target | How to Measure |
|--------|--------|---------------|
| **Job Duplication Rate** | <5% | Duplicate detection algorithm accuracy |
| **Data Completeness** | 90%+ | Percentage of jobs with all required fields |
| **Salary Data Coverage** | 40%+ | Jobs with salary information |
| **Fresh Data** | 95%+ within 24 hours | Jobs scraped within last 24 hours |
| **Dead Link Rate** | <10% | Quarterly manual verification of 100 random jobs |

**Quality Check Script**:
```python
def calculate_data_quality_score():
    """Calculate overall data quality score (0-100)."""
    
    # Completeness (40 points)
    complete_jobs = db.execute("""
        SELECT COUNT(*) FROM jobs 
        WHERE title IS NOT NULL 
        AND company IS NOT NULL 
        AND location IS NOT NULL 
        AND description IS NOT NULL
    """)[0][0]
    total_jobs = db.execute("SELECT COUNT(*) FROM jobs")[0][0]
    completeness_score = (complete_jobs / total_jobs) * 40
    
    # Freshness (30 points)
    fresh_jobs = db.execute("""
        SELECT COUNT(*) FROM jobs 
        WHERE scraped_at >= CURRENT_TIMESTAMP - INTERVAL '24 hours'
    """)[0][0]
    freshness_score = (fresh_jobs / total_jobs) * 30
    
    # Low duplication (20 points)
    duplicates = db.execute("SELECT COUNT(*) FROM jobs WHERE is_duplicate = TRUE")[0][0]
    duplication_score = max(0, 20 - (duplicates / total_jobs) * 100)
    
    # Salary coverage (10 points)
    with_salary = db.execute("SELECT COUNT(*) FROM jobs WHERE salary_min IS NOT NULL")[0][0]
    salary_score = (with_salary / total_jobs) * 10
    
    total_score = completeness_score + freshness_score + duplication_score + salary_score
    
    return {
        'overall_score': round(total_score, 2),
        'completeness': round(completeness_score, 2),
        'freshness': round(freshness_score, 2),
        'duplication': round(duplication_score, 2),
        'salary_coverage': round(salary_score, 2)
    }
```

***

## **25.4 Business Impact Metrics**

### **Value Delivery KPIs**

| Metric | Target (6 months) | Business Impact |
|--------|------------------|-----------------|
| **Total Jobs in Database** | 50,000+ | Market coverage |
| **Sites Successfully Scraped** | 9/9 (100%) | Data diversity |
| **Zero Infrastructure Cost** | PKR 0/month | Budget compliance |
| **External DB Integrations** | 10+ | Platform utility |
| **Job Seeker Time Saved** | 2 hours â†’ 15 min | User value (87% reduction) |
| **Unique Insights Provided** | 5+ analytics | Competitive advantage |

***

## **25.5 Dashboard for Success Tracking**

### **Weekly KPI Dashboard** (to be built)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸ“Š PAKJOBS AGGREGATOR - WEEKLY KPI REPORT                   â”‚
â”‚  Week of: October 7-13, 2025                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸŽ¯ CRITICAL METRICS                                          â”‚
â”‚                                                              â”‚
â”‚  Scraper Uptime:        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 97.2% âœ…        â”‚
â”‚  Target: 95%                                                 â”‚
â”‚                                                              â”‚
â”‚  Jobs Scraped (Week):   6,847 jobs                          â”‚
â”‚  Target: 35,000/week (5,000/day Ã— 7)                        â”‚
â”‚  Status: âš ï¸ BELOW TARGET (-28,153)                           â”‚
â”‚                                                              â”‚
â”‚  API Response Time:     342ms (P95) âœ…                        â”‚
â”‚  Target: <500ms                                              â”‚
â”‚                                                              â”‚
â”‚  Database Size:         687 MB / 1 GB âœ…                      â”‚
â”‚  Target: <900MB                                              â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ðŸ“ˆ TRENDS (vs Last Week)                                     â”‚
â”‚                                                              â”‚
â”‚  â€¢ Uptime:              +2.1% â†‘                              â”‚
â”‚  â€¢ Jobs Scraped:        -12% â†“ (investigate)                â”‚
â”‚  â€¢ API Calls:           +34% â†‘                               â”‚
â”‚  â€¢ Page Load Time:      -15% â†‘ (improved)                   â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš ï¸  ALERTS & ACTION ITEMS                                    â”‚
â”‚                                                              â”‚
â”‚  1. Mustakbil scraper: Only 45 jobs found (expected 300+)   â”‚
â”‚     â†’ ACTION: Check for layout change                        â”‚
â”‚                                                              â”‚
â”‚  2. Database growth rate: +85MB/week                         â”‚
â”‚     â†’ ACTION: Implement more aggressive archiving            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## **25.6 Success Criteria by Phase**

### **Phase 1 Success (Month 3)**
- âœ… All 9 scrapers operational with 95%+ uptime
- âœ… 40,000+ jobs in database
- âœ… API responding in <500ms
- âœ… Zero infrastructure costs maintained
- âœ… Dashboard functional and accessible
- âœ… Automated daily scraping at 3 AM PKT

**Evaluation**: If 5/6 criteria met â†’ Phase 1 SUCCESS

***

### **Phase 2 Success (Month 6)**
- âœ… 50,000+ jobs in database
- âœ… 1,000+ monthly active users
- âœ… 5+ external database integrations
- âœ… Analytics dashboard with salary/skill trends
- âœ… 99%+ uptime SLA
- âœ… Developer API usage: 1,000+ calls/day

**Evaluation**: If 4/6 criteria met â†’ Phase 2 SUCCESS

***

# **26. FUTURE ENHANCEMENTS (PHASE 2)**

## **26.1 User Authentication & Personalization**

### **User Accounts System**
**Timeline**: Months 4-5  
**Effort**: 3 weeks

**Features**:
- Email/password registration
- OAuth login (Google, LinkedIn)
- User profile management
- Saved job searches
- Job application tracking
- Resume upload and storage

**Technical Requirements**:
- Flask-Login for session management
- Argon2 password hashing
- JWT tokens for API authentication
- PostgreSQL user tables
- Email verification (SendGrid free tier: 100 emails/day)

**Benefits**:
- Personalized job recommendations
- Email job alerts (daily digest)
- Application history tracking
- Better user engagement metrics

***

## **26.2 Advanced Analytics**

### **Machine Learning Insights**
**Timeline**: Months 5-6  
**Effort**: 4 weeks

**Features**:
1. **Job Recommendation Engine**
   - Collaborative filtering based on user behavior
   - Skill-based matching
   - Location preference learning
   
2. **Salary Prediction Model**
   - Predict salary ranges for jobs without posted salaries
   - ML model: Random Forest Regressor
   - Features: title, location, experience, skills
   
3. **Skill Gap Analysis**
   - Compare user skills vs. market demand
   - Suggest learning paths
   - Partner with online course providers

4. **Career Path Visualization**
   - Show progression: Junior â†’ Mid â†’ Senior roles
   - Salary growth trajectories
   - Common skill acquisition patterns

**Technical Stack**:
- Scikit-learn for ML models
- Pandas for data preprocessing
- Matplotlib/Plotly for visualizations (charts)
- Scheduled model retraining (weekly)

***

## **26.3 Mobile Applications**

### **Native Mobile Apps**
**Timeline**: Months 6-8  
**Effort**: 8 weeks

**Platforms**:
- Android (Play Store)
- iOS (App Store)

**Development Approach**:
- React Native (cross-platform)
- Shared codebase (90%+ code reuse)
- Native performance for search

**Core Features**:
- Job search and filtering
- Push notifications (new matching jobs)
- Save jobs for later
- Apply via mobile
- Offline mode (cached searches)

**Monetization** (Future):
- Free tier: Basic search
- Premium ($4.99/month): 
  - Unlimited saved searches
  - Priority job alerts
  - Salary insights
  - No ads

***

## **26.4 Employer Features**

### **Job Posting Portal**
**Timeline**: Months 7-9  
**Effort**: 6 weeks

**Features**:
- Employer registration (company verification)
- Direct job posting to PakJobs
- Applicant tracking system (ATS)
- Analytics on job post performance
- Featured job listings (paid promotion)

**Revenue Model**:
- Free: 3 job posts/month
- Basic Plan: PKR 5,000/month (10 posts)
- Premium Plan: PKR 15,000/month (unlimited posts + featured)

**Competitive Advantage**:
- Undercut Rozee.pk pricing by 30%
- Provide better analytics
- Faster candidate matching (ML-powered)

***

## **26.5 Enhanced Data Sources**

### **Expand Beyond 9 Sites**
**Timeline**: Ongoing (Months 4-12)

**Additional Sources**:
1. **Social Media**:
   - LinkedIn job posts (via official API if budget allows)
   - Facebook job groups (with permission)
   - Twitter job hashtags (#JobsPK, #KarachiJobs)

2. **Company Career Pages**:
   - Scrape top 100 Pakistani companies directly
   - Systems Limited, TRG, Netsol, etc.
   - More up-to-date than aggregators

3. **Government Portals**:
   - FPSC (Federal Public Service Commission)
   - Provincial job portals
   - Public sector opportunities

4. **International Remote Jobs**:
   - Remote.co
   - We Work Remotely
   - Filter for Pakistan-eligible positions

**Target**: 100,000+ jobs in database by Month 12

***

## **26.6 Real-Time Scraping**

### **Hourly Updates**
**Timeline**: Month 10  
**Effort**: 2 weeks

**Current**: Daily scraping at 3 AM  
**Future**: Hourly incremental scraping (24 runs/day)

**Implementation**:
- Upgrade to Render paid plan ($7/month for hobby tier)
- More CPU/RAM for concurrent scraping
- Cron job every hour: `0 * * * *`
- Scrape only last 1 hour of jobs (ultra-incremental)

**Benefits**:
- Jobs appear on PakJobs within 1 hour of posting
- Competitive advantage over daily aggregators
- Better for time-sensitive opportunities

**Tradeoff**:
- Higher server load
- Requires paid hosting ($84/year)
- More complex error handling

***

## **26.7 Browser Extension**

### **Chrome/Firefox Extension**
**Timeline**: Month 11  
**Effort**: 3 weeks

**Features**:
- One-click save jobs from any site
- Salary comparison (show PakJobs average)
- Company reviews integration
- Auto-fill application forms

**Technical**:
- JavaScript extension (Manifest V3)
- Communicate with PakJobs API
- Local storage for offline use

**Distribution**:
- Chrome Web Store (free)
- Firefox Add-ons (free)
- 10,000+ users target in 6 months

---

## **26.8 Community Features**

### **User-Generated Content**
**Timeline**: Month 12+

**Features**:
1. **Company Reviews** (Glassdoor-style)
   - Work culture ratings
   - Salary transparency
   - Interview experiences
   
2. **Job Discussion Forums**
   - Ask questions about companies
   - Salary negotiation tips
   - Career advice

3. **Referral Network**
   - Connect job seekers with employees
   - Referral tracking
   - Incentive system (Phase 3)

***

## **26.9 Technology Upgrades**

### **Infrastructure Improvements**

**Month 6+**:
- Migrate to Render paid plan if revenue allows
- Redis caching for faster API responses
- CDN for static assets (Cloudflare free tier)
- Full-text search upgrade (Elasticsearch if budget permits)

**Month 12+**:
- Kubernetes deployment (scalability)
- Multi-region hosting
- GraphQL API (in addition to REST)
- WebSocket for real-time updates

---

## **26.10 Phase 2 Roadmap Summary**

```
Months 4-5:  User Authentication âœ…
Months 5-6:  ML Analytics âœ…
Months 6-8:  Mobile Apps âœ…
Months 7-9:  Employer Portal âœ…
Month 10:    Real-Time Scraping âœ…
Month 11:    Browser Extension âœ…
Month 12+:   Community Features âœ…

GOAL: 100,000+ jobs, 10,000+ users, PKR 50,000/month revenue by Month 12
```

***

# **27. APPENDICES**

## **27.1 Glossary of Terms**

| Term | Definition |
|------|------------|
| **API (Application Programming Interface)** | Set of protocols allowing external applications to access PakJobs data |
| **APScheduler** | Python library for scheduling background tasks (e.g., daily scraping) |
| **CAPTCHA** | Challenge-response test to distinguish humans from bots; can block scrapers |
| **CI/CD (Continuous Integration/Deployment)** | Automated testing and deployment pipeline via GitHub Actions |
| **CORS (Cross-Origin Resource Sharing)** | Security feature allowing API access from different domains |
| **Cron Job** | Scheduled task that runs automatically (e.g., daily at 3 AM) |
| **Deduplication** | Process of identifying and removing duplicate job listings |
| **Fernet Encryption** | Symmetric encryption algorithm for securing database credentials |
| **Full-Text Search** | Search functionality that indexes entire job descriptions, not just titles |
| **GIN Index** | PostgreSQL index type optimized for full-text and JSONB searches |
| **Graceful Degradation** | System continues functioning even if individual components fail |
| **Incremental Scraping** | Only scraping new jobs since last run (vs. scraping all jobs) |
| **Jinja2** | Python templating engine for generating HTML in Flask |
| **JWT (JSON Web Token)** | Secure method for API authentication (Phase 2) |
| **KPI (Key Performance Indicator)** | Measurable value demonstrating project success |
| **ORM (Object-Relational Mapping)** | SQLAlchemy library for database interactions using Python objects |
| **Playwright** | Browser automation tool for scraping JavaScript-heavy sites |
| **PostgreSQL** | Open-source relational database (1GB free on Render) |
| **PRD (Product Requirements Document)** | This document; comprehensive project specification |
| **Pydantic** | Python library for data validation using type hints |
| **Rate Limiting** | Restricting API usage to prevent abuse (1000 requests/hour) |
| **Render.com** | Cloud hosting platform with generous free tier for web apps |
| **robots.txt** | File specifying which parts of a website can be scraped |
| **Scrapy** | Python framework for fast, efficient web scraping |
| **SQLAlchemy** | Python SQL toolkit and ORM for database operations |
| **User-Agent** | String identifying scraper/browser to websites |
| **UUID (Universally Unique Identifier)** | 128-bit ID for uniquely identifying jobs |

---

## **27.2 Technology Stack Reference**

### **Complete Dependency List**

**Core Libraries**:
```
Python 3.12.0
Flask 3.0.0
Scrapy 2.11.0
Playwright 1.40.0
BeautifulSoup4 4.12.2
psycopg2-binary 2.9.9
SQLAlchemy 2.0.23
Pandas 2.2.0
APScheduler 3.10.4
```

**Hosting & Infrastructure**:
```
Render.com (Free Tier)
PostgreSQL 16 (1GB)
GitHub (Version Control)
GitHub Actions (CI/CD)
```

**Estimated Total Size**: ~250MB installed

***

## **27.3 Database Schema Quick Reference**

### **Primary Tables**

**jobs** (Main table):
```sql
- id (UUID, primary key)
- title (VARCHAR 255)
- company (VARCHAR 255)
- location (VARCHAR 255)
- salary_min, salary_max (INTEGER)
- description (TEXT)
- skills (JSONB array)
- source_url (TEXT, unique)
- site_source (VARCHAR 50)
- posted_date (DATE)
- scraped_at (TIMESTAMP)
- is_active (BOOLEAN)
```

**scrape_logs** (Monitoring):
```sql
- id (SERIAL)
- site_name (VARCHAR 50)
- status (success/failed)
- jobs_found (INTEGER)
- start_time, end_time (TIMESTAMP)
- error_message (TEXT)
```

**user_config** (Settings):
```sql
- config_key (VARCHAR 100, unique)
- config_value (TEXT)
- is_encrypted (BOOLEAN)
```

***

## **27.4 Sample Code Snippets**

### **Quick Start: Run First Scraper**

```python
# Run Rozee.pk scraper (test)
from scrapers.rozee import RozeeScraper

scraper = RozeeScraper(mode='incremental')
jobs = scraper.scrape()

print(f"Found {len(jobs)} jobs")
print(f"First job: {jobs[0]['title']} at {jobs[0]['company']}")
```

### **Query Jobs by Skill**

```sql
-- Find all Python jobs in Karachi
SELECT title, company, salary_min, salary_max, source_url
FROM jobs
WHERE skills @> '["Python"]'
AND city = 'Karachi'
AND posted_date >= CURRENT_DATE - INTERVAL '7 days'
ORDER BY posted_date DESC
LIMIT 20;
```

### **Calculate Average Salary by City**

```sql
-- Salary averages by city
SELECT 
    city,
    COUNT(*) AS job_count,
    AVG(salary_min) AS avg_min_salary,
    AVG(salary_max) AS avg_max_salary
FROM jobs
WHERE salary_min IS NOT NULL
AND posted_date >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY city
ORDER BY job_count DESC;
```

***

## **27.5 API Endpoint Quick Reference**

### **Essential Endpoints**

| Endpoint | Method | Description | Example |
|----------|--------|-------------|---------|
| `/api/v1/jobs` | GET | List jobs with filters | `?q=python&location=karachi&page=1` |
| `/api/v1/jobs/{id}` | GET | Single job details | `/api/v1/jobs/550e8400-e29b-41d4-a716-446655440000` |
| `/api/v1/jobs/search` | GET | Full-text search | `?q=machine learning engineer` |
| `/api/v1/stats` | GET | Platform statistics | N/A |
| `/api/v1/scrapers` | GET | Scraper status | N/A |
| `/api/v1/analytics/skills` | GET | Top skills in demand | N/A |
| `/api/v1/analytics/salaries` | GET | Salary trends | N/A |

**Rate Limit**: 1000 requests/hour per IP

***

## **27.6 Common Troubleshooting**

### **Issue 1: Scraper Returns 0 Jobs**

**Possible Causes**:
- Site layout changed (selectors outdated)
- IP temporarily blocked
- Site is down
- robots.txt disallows scraping

**Solution**:
1. Check scrape_logs for error messages
2. Visit site manually to verify accessibility
3. Inspect HTML to update CSS selectors
4. Wait 24 hours if IP blocked

---

### **Issue 2: Database Full (>1GB)**

**Immediate Action**:
```sql
-- Archive old jobs
UPDATE jobs SET is_active = FALSE 
WHERE posted_date < CURRENT_DATE - INTERVAL '60 days';

-- Delete very old jobs
DELETE FROM jobs 
WHERE posted_date < CURRENT_DATE - INTERVAL '180 days';

-- Vacuum database
VACUUM FULL;
```

**Prevention**: Run weekly cleanup job

***

### **Issue 3: Slow API Responses**

**Diagnosis**:
```sql
-- Find slow queries
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
WHERE mean_exec_time > 1000  -- >1 second
ORDER BY mean_exec_time DESC
LIMIT 10;
```

**Solutions**:
- Add missing indexes
- Enable query result caching
- Paginate large result sets
- Optimize JSONB queries

***

## **27.7 Deployment Checklist**

### **Pre-Launch Verification**

**Technical Checks**:
- [ ] All 9 scrapers tested individually
- [ ] Database schema created and indexed
- [ ] Environment variables configured on Render
- [ ] Cron job scheduled (3 AM PKT daily)
- [ ] Email notifications tested (Gmail SMTP working)
- [ ] API endpoints return valid responses
- [ ] Dashboard loads in <2 seconds
- [ ] Health check endpoint returns 200 OK
- [ ] SSL certificate active (HTTPS)
- [ ] Backup script tested

**Legal/Compliance Checks**:
- [ ] Privacy policy published on website
- [ ] robots.txt compliance verified for all sites
- [ ] Attribution links to source sites present
- [ ] Terms of service drafted
- [ ] DMCA agent contact information listed

**Documentation Checks**:
- [ ] README.md complete with setup instructions
- [ ] API documentation (Swagger) accessible
- [ ] Code comments added to critical functions
- [ ] Deployment guide written
- [ ] Troubleshooting FAQ created

***

## **27.8 Useful Resources**

### **Official Documentation**

| Technology | Documentation URL |
|-----------|------------------|
| Flask | https://flask.palletsprojects.com/ |
| Scrapy | https://docs.scrapy.org/ |
| Playwright | https://playwright.dev/python/ |
| PostgreSQL | https://www.postgresql.org/docs/ |
| Render | https://render.com/docs |
| SQLAlchemy | https://docs.sqlalchemy.org/ |

### **Learning Resources**

**Web Scraping**:
- Scrapy Tutorial: https://docs.scrapy.org/en/latest/intro/tutorial.html
- BeautifulSoup Guide: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- Ethical Scraping: https://www.scrapehero.com/avoid-anti-scraping-measures/

**Flask Development**:
- Flask Mega-Tutorial: https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world
- REST API Best Practices: https://restfulapi.net/

**PostgreSQL**:
- Full-Text Search Guide: https://www.postgresql.org/docs/current/textsearch.html
- Performance Tuning: https://wiki.postgresql.org/wiki/Performance_Optimization

***

## **27.9 Contact & Support**

### **Project Information**

**Project Name**: PakJobs Aggregator  
**Version**: 1.0.0  
**Status**: Development Phase  
**License**: MIT (Open Source in Phase 2)

**Maintainer**: [Your Name]  
**Email**: [your-email@example.com]  
**GitHub**: https://github.com/yourusername/pakjobs-aggregator

### **Getting Help**

**For Technical Issues**:
- Check troubleshooting section (27.6)
- Search GitHub issues
- Create new issue with error logs

**For Feature Requests**:
- Open GitHub discussion
- Describe use case clearly
- Explain expected behavior

**For Legal Questions**:
- Review Section 20 (Legal & Ethical Compliance)
- Consult local attorney if needed
- Email privacy concerns to privacy@pakjobs.example.com

***

## **27.10 Acknowledgments**

### **Data Sources**

This project aggregates publicly available job listings from:
- Rozee.pk
- Mustakbil.com
- Indeed.pk
- BrightSpyre.com
- Bayt.com
- Jobz.pk
- Bayrozgar.com
- JobsAlert.pk
- PakPositions.com

**We acknowledge** the hard work of these platforms in connecting employers with job seekers. PakJobs Aggregator is a complementary service providing enhanced search and analytics.

### **Open Source Dependencies**

Special thanks to the maintainers of:
- Flask (Pallets Projects)
- Scrapy (Scrapy developers)
- Playwright (Microsoft)
- PostgreSQL (PostgreSQL Global Development Group)
- All Python libraries listed in requirements.txt

### **Inspiration**

This project was inspired by:
- Indeed's job aggregation model
- Glassdoor's salary transparency
- LinkedIn's career insights

***

## **27.11 Document Revision History**

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 0.1 | Oct 14, 2025 | AI Assistant | Initial draft (sections 1-12) |
| 0.5 | Oct 14, 2025 | AI Assistant | Expanded (sections 13-24) |
| 1.0 | Oct 15, 2025 | AI Assistant | Complete PRD (all 27 sections) |

***

## **27.12 Next Steps After Reading This PRD**

### **For Developers**

1. **Read sections 1-6** (Overview & Architecture)
2. **Study section 8** (System Components Design)
3. **Review section 10** (Scraper Specifications)
4. **Follow section 16** (Deployment Strategy)
5. **Start with TASKS.md** (detailed task breakdown)

### **For Stakeholders**

1. **Read section 1** (Executive Summary)
2. **Review section 5** (Product Scope & Features)
3. **Study section 24** (Timeline & Milestones)
4. **Check section 25** (Success Metrics)
5. **Approve budget**: PKR 0 (confirm zero-cost requirement)

### **For Project Managers**

1. **Import section 24** (Timeline) into project management tool
2. **Set up tracking** for KPIs in section 25
3. **Schedule weekly reviews** of scraper status
4. **Plan for Phase 2** features (section 26)
5. **Assign tasks** from TASKS.md

***

# **CONCLUSION**

This Product Requirements Document provides a **comprehensive, detailed blueprint** for building the **PakJobs Aggregator** - Pakistan's first multi-site job aggregation platform with advanced analytics, built entirely on free infrastructure.

## **Key Highlights**

âœ… **9 job sites** scraped daily (30,000-50,000 jobs)  
âœ… **Zero infrastructure cost** (Render.com + PostgreSQL free tier)  
âœ… **12-week development timeline** (realistic with contingencies)  
âœ… **Incremental scraping** (10x faster, 90% less bandwidth)  
âœ… **REST API** for external integrations  
âœ… **Advanced analytics** (salary trends, skill demand)  
âœ… **Legal compliance** (robots.txt, ethical scraping)  
âœ… **Scalable architecture** (ready for Phase 2 growth)

## **Success Criteria**

**Phase 1 (3 months)**: 40,000+ jobs, 95% uptime, zero cost âœ…  
**Phase 2 (6 months)**: 50,000+ jobs, 1,000+ users, monetization path âœ…

## **What Makes This Different**

Unlike existing Pakistani job sites, PakJobs Aggregator offers:
- **Multi-source search** (9 sites in one)
- **Salary transparency** (trends & benchmarks)
- **Market intelligence** (skills in demand, hiring trends)
- **Free API** (developers can build on our data)
- **Zero cost to operate** (sustainable forever)

***

**TOTAL PRD LENGTH**: ~50,000+ words | 27 comprehensive sections | 100% complete

**READY FOR**: Development team handoff, stakeholder approval, implementation

**NEXT DOCUMENT**: TASKS.md (150+ atomic tasks with time estimates, dependencies, and AI agent execution instructions)

***

**Document Status**: âœ… **COMPLETE**  
**Approval Required**: Yes (from stakeholders)  
**Ready for Development**: Yes  

**Questions?** Review section 27.9 (Contact & Support)

**Let's build the future of job search in Pakistan! ðŸš€**

---

*End of Product Requirements Document*